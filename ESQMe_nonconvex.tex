%Version 2.1 April 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove “Numbered?in the optional parenthesis.
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst, sn-mathphys.bst. %

%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-mathphys,Numbered]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[default]{sn-jnl}% Default
%%\documentclass[default,iicol]{sn-jnl}% Default with double column layout

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{multirow,diagbox,multicol,booktabs}
\usepackage{graphicx}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
%\usepackage{refcheck}%
\usepackage{algorithm,algorithmic}%

\usepackage{latexsym,enumerate,verbatim,amsfonts,microtype}
\usepackage{color} % added by TK
\usepackage{graphicx}
\usepackage[active]{srcltx}
\usepackage{cases}
\usepackage[colorinlistoftodos,prependcaption,textsize=footnotesize]{todonotes}
\usepackage[framemethod=tikz]{mdframed}% added by TK. Create nice boxes
\mdfsetup{%
	skipbelow=4pt,
	skipabove=8pt,
	linewidth=1.25pt,
	backgroundcolor=gray!10,
	userdefinedwidth=\textwidth,
	roundcorner=10pt,
}

\allowdisplaybreaks[2]
\numberwithin{equation}{section}

% def by Ting Kei Pong
\def\cS{{\mathcal{S}}}
\def\cU{{\mathcal{U}}}
\def\cF{{\mathcal{F}}}
\def\cD{{\mathcal{D}}}
\def\cL{{\mathcal{L}}}
\def\R{{\mathbb{R}}}
\def\tr{{\rm tr}}
\def\argmin{\mathop{\rm arg\,min}}
\def\Argmin{\mathop{\rm Arg\,min}}
\def\argmax{\mathop{\rm arg\,max}}
\def\Diag{{\rm Diag}}
\def\tx{{\widetilde x}}
\def\hx{{\widehat x}}
\def\conv{{\rm conv}\,}

\def\prox{{\rm Prox}}
\def\xfeas{x^\odot}
\def\betamin{{\rm \beta_{min}}}
\def\sigmamin{{\rm\tilde{\sigma}_{min}}}
\def\diag{{\rm diag}}
\def\d{{\rm dist}}
\def\dom{{\rm dom}\,}
\def\xorig{{x_{\rm orig}}}
%\def\xfeasss{x^\circledcirc}
%\def\xfeasss{x^\circledast}

\newcommand{\ling}[3]{{\rm lin}_{g_{#1}}(#2,#3)}

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published
%%%%  by Springer Nature. The guidance has been prepared in partnership with
%%%%  production teams to conform to Springer Nature technical requirements.
%%%%  Editorial and presentation requirements differ among journal portfolios and
%%%%  research disciplines. You may find sections in this template are irrelevant
%%%%  to your work and are empowered to omit any such section if allowed by the
%%%%  journal you intend to submit to. The submission guidelines and policies
%%%%  of the journal take precedence. A detailed User Manual is available in the
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%\jyear{2021}%

%% as per the requirement new theorem styles can be included as shown below
%\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}[section]%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
%\newtheorem{proposition}[theorem]{Proposition}%
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.
\newtheorem{definition}{Definition}[section]%
\newtheorem{lemma}{Lemma}[section]
%\newtheorem{definition}{Definition}[section]
\newtheorem{fact}{Fact}[section]
%\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
%\newtheorem{remark}{Remark}[section]
%\newtheorem{example}{Example}[section]
\newtheorem{assumption}{Assumption}[section]

%\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}[section]%
\newtheorem{remark}{Remark}[section]%

%\theoremstyle{thmstylethree}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[ESQM]{An extended sequential quadratic method with extrapolation}

%%=============================================================%%
%% Prefix	-> \pfx{Dr}
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% NatureName	-> \tanm{Poet Laureate} -> Title after name
%% Degrees	-> \dgr{MSc, PhD}
%% \author*[1,2]{\pfx{Dr} \fnm{Joergen W.} \spfx{van der} \sur{Ploeg} \sfx{IV} \tanm{Poet Laureate}
%%                 \dgr{MSc, PhD}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1]{\fnm{Yongle} \sur{Zhang}}\email{yongle-zhang@163.com}

\author[2]{\fnm{Ting Kei} \sur{Pong}}%\email{tk.pong@polyu.edu.hk}
%\equalcont{These authors contributed equally to this work.}

\author[3]{\fnm{Siqi} \sur{Xu}}%\email{xxx.com}
%\equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{Department of Mathematics}, \orgname{Visual Computing and Virtual Reality Key Laboratory of Sichuan Province, Sichuan Normal University}, \orgaddress{\city{Chengdu}, \country{People's Republic of China}}}

\affil[2]{\orgdiv{Department of Applied Mathematics}, \orgname{the Hong Kong Polytechnic University}, \orgaddress{\city{Hong Kong}, \country{People's Republic of China}}}

\affil[3]{\orgdiv{Department of Mathematics}, \orgname{Sichuan Normal University}, \orgaddress{\city{Chengdu}, \country{People's Republic of China}}}

%%==================================%%
%% sample for unstructured abstract %%
%%==================================%%

\abstract{We revisit and adapt the extended sequential quadratic method (ESQM) in \cite{Ausleder13} for solving a class of difference-of-convex optimization problems whose constraints are defined as the intersection of level sets of smooth functions and a simple closed convex set. Particularly, for this class of problems, we develop a variant of ESQM, called ESQM with extrapolation (ESQM$_e$), which incorporates Nesterov's extrapolation techniques for empirical acceleration. Under standard constraint qualifications, we show that the sequence generated by ESQM$_e$ clusters at a critical point if the extrapolation parameters are uniformly bounded above by a certain threshold. Convergence of the whole sequence and the convergence rate are established by assuming Kurdyka-{\L}ojasiewicz (KL) property of a suitable potential function and imposing additional differentiability assumptions on the objective and constraint functions. In addition, when the objective and constraint functions are all convex, we show that linear convergence can be established if a certain exact penalty function is known to be a KL function with exponent $\frac12$; we also show that the KL exponent of such an exact penalty function can be deduced from that of the \emph{original} extended objective (i.e., sum of the objective and the indicator function of the constraint set). Finally, we perform numerical experiments to demonstrate the empirical acceleration of ESQM$_e$ over ESQM, and illustrate its effectiveness by comparing with the natural competing algorithm SCP$_{\rm ls}$ from \cite{yu21}.}

\keywords{ESQM, Extrapolation, KL exponent, Linear convergence}
%\keywords{Extended sequential quadratic method, Extrapolation, Kurdyka-{\L}ojasiewicz exponent, Linear convergence}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle



\section{Introduction}

Extrapolation techniques, due to their simplicity and easy adaptability, have been widely studied in recent years to empirically accelerate first-order methods; see, for example, \cite{Polyak64,SOR00,nesterov83,Brezinski00,SmithFordSidi87} and references therein. Among them, Nesterov's extrapolation techniques \cite{nesterov83,Nesterov2004,Nesterov2005,Nesterov2013} have been successfully applied to accelerate the proximal gradient algorithm \cite{LionsMercier79} for minimizing $f + h$, with $f$ being a convex {\em loss function} with Lipschitz continuous gradient, and $h$ being a proper closed convex and possibly nonsmooth {\em regularizer} with \emph{easy-to-compute} proximal operator. These studies led to the developments of various algorithms and softwares including the well-known algorithm FISTA \cite{beck09} for linear inverse problems and the software TFOCS \cite{BeckerCandesGrant11} for solving a large class of convex cone problems. Nesterov's extrapolation techniques have also been suitably adapted in subsequent works such as \cite{wen17,wen18} in some nonconvex settings, and most of these works also require the proximal operator of (part of) the regularizer to be easy to compute: in the case when $h$ is the indicator function of some closed set $D$, this requirement amounts to saying that the projection onto $D$ can be computed efficiently. In this paper, we consider a class of constrained optimization problems whose constraint sets \emph{do not admit easy projections}, and investigate the adaptation of extrapolation techniques on empirically accelerating a classical algorithm for these problems.

Specifically, we consider the following difference-of-convex (DC) optimization problem with smooth inequality and simple geometric constraints:
\begin{align}\label{eq1}
\min_{x\in \mathbb {R}^n }\quad& P(x):= P_1(x) - P_2(x) \notag \\
\text{s.t.}\quad & g_i(x)\leq 0,~~ i = 1,\ldots, m,\\
& x\in C,  \notag
\end{align}
where $P_1:\mathbb{R}^n \to \R$ and $P_2:\mathbb{R}^n\to \R$ are convex, each $g_i:\mathbb{R}^n\to \R$ is smooth and $\nabla g_i$ is Lipschitz continuous, $C\subseteq \mathbb {R}^n$ is a nonempty compact convex set, and the feasible set $\mathcal{G} := C\cap \mathscr{F}$ is nonempty with $\mathscr{F}:=\{x\in \mathbb{R}^n : g_i(x)\leq 0, ~i = 1,\dots,m\}$. This class of problems arises naturally in many applications. For example, in compressed sensing, the $P$ can be a sparsity inducing regularizer such as the difference of $\ell_1$ and $\ell_2$ norms \cite{YinLouHeXin15}, the $g_i$, $i = 1,\ldots,m$ can be loss functions based on the noise models in the $i$th transmission channel, and the set $C$ can be used to model some priors such as nonnegativity or boundedness.

Since projections onto the feasible set of \eqref{eq1} are not easy to compute, existing algorithms for \eqref{eq1} usually leverage the Lipschitz continuity of $\nabla g_i$ to build approximations for the feasible sets, leading to relatively easier subproblems. One natural approach for building approximations is to replace $g_i$ by its quadratic majorants at the current iterate. Specific algorithms based on quadratically approximating $g_i$ in \eqref{eq1} include the moving balls approximation algorithm \cite{AusSheTeb10} and its variants (e.g., \cite{bolte16,yu21}), which were proposed and studied for instances of \eqref{eq1} {\em in the absence of the requirement $x\in C$}. Another natural approach for building approximations is to make use of \emph{affine approximations} to $g_i$ at the current iterate, leading to subproblems with even simpler structures. This approach has its roots in
the literature of sequential quadratic programming (SQP) method, and we refer the readers to \cite{GillWong12} and references therein for more discussions of SQP. Here, we are interested in the framework described in \cite{Ausleder13,Auslender13}, which focused on solving \eqref{eq1} when $P$ and each $g_i$ are twice continuously differentiable. We adapt the algorithmic framework described there to solve \eqref{eq1}, and incorporate extrapolation techniques to empirically accelerate the algorithm. We call the resulting algorithm extended sequential quadratic method with extrapolation (ESQM$_e$), following the use of the name ESQM in \cite{Auslender13}. The algorithmic details will be presented in Section~\ref{sec3} below; in particular, in each iteration of ESQM$_e$, the $g_i$ in \eqref{eq1} is replaced by its affine approximation at a point \emph{extrapolated from} the current iterate.

In this paper, we study the convergence properties of ESQM$_e$ and perform numerical experiments to examine its computational efficiency. In particular, we show that the sequence generated by ESQM$_e$ clusters at a critical point if the extrapolation parameters are uniformly bounded above by a certain threshold, under a set of constraint qualifications similarly used in \cite{Auslender13}. We also construct a suitable potential function and establish the convergence of the whole sequence and its convergence rate by assuming Kurdyka-{\L}ojasiewicz (KL) property of the potential function and additional differentiability conditions on $P_2$ and each $g_i$ in \eqref{eq1}. Furthermore, when $P_2 \equiv 0$ and each $g_i$ is convex, we show that linear convergence can also be established if a certain exact penalty function of \eqref{eq1} is known to be a KL function with exponent $\frac12$. We also discuss how the KL exponent of such an exact penalty function can be derived from that of the function $P+\delta_{\cal G}$ from \eqref{eq1} (see Section~\ref{sec2} for notation). Finally, we perform numerical experiments on compressed sensing models with different types of measurement noises taking the form of \eqref{eq1}. Our experiments on random instances illustrate the empirical acceleration of ESQM$_e$ over ESQM, and also suggest that ESQM$_e$ outperforms the natural competing algorithm SCP$_{\rm ls}$ from \cite{yu21}.

The remainder of the paper is organized as follows. We present notation and preliminary materials in Section~\ref{sec2}. Our algorithm, ESQM$_e$ is presented in Section~\ref{sec3}, and its subsequential and sequential convergences are established in Section~\ref{sec41}. We discuss the convergence behavior in the convex setting (i.e., $P_2 \equiv 0$ and each $g_i$ is convex in \eqref{eq1}) in Section~\ref{sec42}, and the relationship between the KL exponent of the function $P+\delta_{\cal G}$ from \eqref{eq1} and that of the exact penalty function used in the analysis in Section~\ref{sec42} is studied in Section~\ref{sec5}. Numerical experiments are presented in Section~\ref{sec6}.

\section{Notation and preliminaries}\label{sec2}

In this paper, we let $\mathbb{R}$ and $\mathbb{R}_+$ denote the sets of real numbers and nonnegative real numbers respectively, and $\mathbb{N}$ is the set of positive integers. We also let $\mathbb{R}^n$ and $\mathbb{R}_+^n$ denote the $n$-dimensional Euclidean space and its nonnegative orthant respectively. For an $x\in \mathbb{R}^n$, we let $\|x\|$ denote its Euclidean norm; moreover, for $x$ and $y\in \mathbb{R}^n$, we let $\langle x,y \rangle$ denote their inner product.

For an extended-real-valued function $f:\mathbb{R}^n \to (-\infty,+\infty]$, we say that $f$ is proper if $\dom f := \{ x: f(x)<\infty \}\neq \emptyset$. A proper function $f$ is said to be closed if it is lower semicontinuous. We use $x^k\overset{f}{\to} x$ to denote $x^k\to x$ and $f(x^k)\to f(x)$. For a proper closed function $f$, the regular subdifferential of $f$ at $w\in \dom f$ is given by
$$
\widehat{\partial} f(w):= \left\{ \xi\in\mathbb{R}^n :\liminf_{v\to w,v\neq w} \frac{f(v) - f(w) - \langle \xi, v - w \rangle }{\| v - w\|}\geq 0 \right\}.
$$
The (limiting) subdifferential of $f$ at $w\in \dom f$ is given by
\begin{equation*}
\partial f(w):= \left\{ \xi\in\mathbb{R}^n:\exists w^{k}\overset{f}{\to} w, \xi^{k}\to\xi \text{ with } \xi^{k} \in \widehat{\partial} f(w^{k}) \text{ for each } k\right\},
\end{equation*}
and we set $\partial f(x)=\widehat{\partial}f(x) = \emptyset$ when $ x\notin \dom f$ by convention. We also define $\dom \partial f := \{x:\partial f(x)\neq \emptyset\}$. The above subdifferential of $f$ is consistent with the classical subdifferential of $f$ when $f$ is in addition convex; indeed, in this case, we have
$$
\partial f(w)=\left\{ \xi\in\mathbb{R}^n:\langle \xi, v - w \rangle\leq f(v) - f(w) ~~ \forall v\in \mathbb{R}^n \right\};
$$
see, for example, \cite[proposition 8.12]{rock97a}.
For a nonempty closed set $D\subseteq \mathbb{R}^n$, the indicator function $\delta_D$ is defined by
\begin{equation*}
\delta_D(x) = \left\{
\begin{array}{lr}
 0~~ &x\in D,
 \\  \infty ~~ &x\notin D.
 \end{array}
 \right.
\end{equation*}
The normal cone of $D$ at $x\in D$ is defined by
$\mathcal{N}_D(x) :=\partial \delta_D(x)$.
Finally, the distance from a point $x$ to $D$ is denoted by $\d(x, D)$, and the convex hull of $D$ is denoted by $\conv D$.

We next recall some important definitions that will be used in the sequel. We start by recalling the following constraint qualification for \eqref{eq1} (which was first introduced in \cite{Ausleder13}), and the (associated) first-order optimality conditions for \eqref{eq1}.
\begin{definition}[{{\bf RCQ}}]\label{RCQ}
We say that the Robinson constraint qualification holds at $x\in\R^n$ for \eqref{eq1} if the following statement holds:
$$ RCQ(x):~\exists y\in C \text{ such that } g_i(x) + \langle\nabla g_i(x), y-x\rangle < 0~~ \forall i = 1,2,\cdots m.$$
\end{definition}

\begin{definition}[{{\bf Critical point}}]\label{Stationary}
For \eqref{eq1}, we say that $x$ is a critical point of \eqref{eq1} if $x\in C$ and there exists $\lambda=(\lambda_1, \lambda_2, \dots, \lambda_m)\in \mathbb{R}_+^m$ such that $(x, \lambda)$ satisfies the following conditions:
\begin{enumerate}[{\rm (i)}]
    \item $ g_i(x)\leq 0 ~~\forall i=1,\dots,m,$
    \item $ \lambda_i g_i(x)=0 ~~\forall i=1,\dots,m,$
    \item $0\in\partial P_1(x) - \partial P_2(x) + \sum_{i=1}^{m}\lambda_i\nabla g_i(x) + \mathcal{N}_C(x).$
\end{enumerate}	
\end{definition}

One can show using similar arguments as in \cite[Section 2]{yu21} that if $RCQ(x)$ holds at every $x\in\mathcal{G}$, then any local minimizer of \eqref{eq1} is a critical point of \eqref{eq1}.

Next, we recall the definitions of Kurdyka-{\L}ojasiewicz (KL) property and Kurdyka-{\L}ojasiewicz (KL) exponent.
\begin{definition}[{{\bf Kurdyka-{\L}ojasiewicz (KL) property and exponent}}]\label{KLd}
A proper closed function $f$ is said to satisfy the KL property at $\bar{x}\in \dom\partial f$ if there exist $r\in (0,\infty]$, a neighborhood $U$ of $\bar{x}$, and a continuous concave function $\phi:[0,r)\to \mathbb{R}_+$ satisfying $\phi(0)=0$ such that:
\begin{enumerate}[{\rm (i)}]
    \item $\phi$ is continuously differentiable on $(0,r)$ with $\phi'>0$;
    \item for all $x\in U$, $f(\bar{x})< f(x) < f(\bar{x}) + r$, it holds that
        \begin{align}\label{eq21}
        \phi'(f(x) - f(\bar{x}))\d(0,\partial f(x))\geq 1.
        \end{align}
\end{enumerate}
If $f$ satisfies the KL property at $\bar{x}\in \dom\partial f$, and the $\phi$ in \eqref{eq21} can be chosen as $\phi(\varsigma)= \rho \varsigma^{1-\alpha}$ for some $\rho>0$ and $\alpha\in[0,1)$, then we say that $f$ satisfies the KL property with exponent $\alpha$ at $\bar{x}$.

A proper closed function $f$ satisfying the KL property at every point in $\dom\partial f$ is called a KL function. A proper closed function $f$ satisfying the KL property with exponent $\alpha\in[0,1)$ at every point in $\dom\partial f$ is called a KL function with exponent $\alpha$.
\end{definition}

Many functions are known to satisfy the KL property. For instance, proper closed semi-algebraic functions satisfy the KL property with some exponent $\alpha$; see \cite{bolte07_2}. The KL property plays an important role in the global convergence analysis of first order methods and the exponent is important in establishing convergence rates; see, for example, \cite{bolte14,attouch13,attouch10,li18}.

Finally, before ending this section, we recall two technical lemmas. The first lemma concerns the uniformized KL property (see \cite[Section~3.5]{bolte14}) and was established in \cite[Lemma~3.10]{yu21}. The second lemma is a special case of Robinson \cite{Rob75} concerning error bounds for convex functions, which will be used in Section~\ref{sec5} for studying the KL property of a penalty function associated with \eqref{eq1}.
\begin{lemma}\label{KLinequ}
Let $f:\R^n\rightarrow (-\infty,+\infty]$ be a level-bounded proper closed convex function with $\Lambda:= \Argmin f\not=\emptyset$. Let $\underline{f}:=\inf f$. Suppose that $f$ satisfies the KL property at each point in $\Lambda$ with exponent $\alpha\in[0,1)$. Then there exist $\epsilon >0$, $r_0>0$ and $c_0>0$ such that
\[
\d(x, \Lambda)\leq c_0(f(x) - \underline{f})^{1-\alpha}
\]
for any $x\in\dom \partial f$ satisfying $\d(x, \Lambda)\leq \epsilon$ and $\underline{f}\leq f(x) \leq \underline{f} + r_0$.
\end{lemma}

\begin{lemma}\label{RobEB}
Let $g:\R^n\to \R^m$ with each component function $g_i$ being convex. Let $\Omega := \{x\in \R^n:\; 0 \in g(x) + \R^m_+\}$ and suppose there exist $x^s\in \Omega$ and $\delta_0 > 0$ such that $B(0,\delta_0)\subseteq g(x^s) + \R^m_+$.
Then
\[
\d(x,\Omega)\leq \frac{\|x - x^s\|}{\delta_0}\d(0, g(x) + \R^m_+)~~~~~  \forall x\in \R^n.
\]
\end{lemma}

\section{Algorithmic framework}\label{sec3}

In this section, we present our algorithm for solving \eqref{eq1}. To describe our algorithm, following the discussion in \cite[Section~3]{wen17}, for each $i$, notice that we can rewrite $g_i$ (whose gradient is Lipschitz continuous) as $g_i = g_i^1 - g_i^2$, where $g_i^1$ and $g_i^2$ are two convex functions with Lipschitz continuous gradients. The next remark concerns the Lipschitz continuity moduli of $\nabla g_i$, $\nabla g_i^1$ and $\nabla g_i^2$.

\begin{remark}[Lipschitz continuity moduli]\label{Remarkg}
Here and throughout, we denote a Lipschitz continuity modulus of $\nabla g_i^1$ by $L_{g_i} > 0$ and a Lipschitz continuity modulus of $\nabla g_i^2$ by $\ell_{g_i} \ge 0$. In addition, by taking a larger $L_{g_i}$ if necessary, we will assume without loss of generality that $L_{g_i} \geq \ell_{g_i}$. Then one can show that $\nabla g_i$ is Lipschitz continuous with a modulus $L_{g_i}$. We also define $L_g := \max\{L_{g_i}: i=1,\dots,m\}$ and $\ell_g = \max\{\ell_{g_i}: i=1,\dots,m\}$.
\end{remark}

The algorithm we study in this paper is presented as Algorithm \ref{alg:Framwork} below; here and throughout, for notational simplicity, for each $u$, $w\in \R^n$, we define
\begin{equation}\label{ling}
\ling{0}{u}{w}\equiv 0 \ \ {\rm and}\ \ \ling{i}{u}{w} := g_i(w) + \langle\nabla g_i(w), u - w\rangle \ \ \ \forall i = 1,\ldots,m.
\end{equation}
We identify our algorithm as an extended sequential quadratic method with extrapolation (ESQM$_{\text{e}}$), where ``extrapolation" refers to the step \eqref{defyk}. This is because when $\beta_k \equiv 0$, our algorithm reduces to an instance of the ESQM proposed in \cite{Ausleder13}, whose convergence was established for solving \eqref{eq1} when the $P$ in \eqref{eq1} is in addition smooth with Lipschitz gradient.\footnote{More precisely, when the $P$ in \eqref{eq1} is smooth with Lipschitz gradient (say, with Lipschitz constant $L_P$) and $\beta_k \equiv 0$, our algorithm applied to \eqref{eq1} with $P_1(x) := \frac{L_P}2\|x\|^2$ and $P_2(x) := \frac{L_P}2\|x\|^2 - P(x)$ becomes an instance of the ESQM in \cite{Ausleder13}.} Notice that the subproblem in \eqref{eq2} has a unique solution as an optimization problem with a strongly convex objective and a nonempty feasible set. While it requires an iterative solver in general, we refer the readers to \cite[Appendix~A]{zhang23} for an efficient routine for solving the subproblem in \eqref{eq2} when $m = 1$.
\begin{algorithm}
\caption{ESQM$_{\text{e}}$ for solving \eqref{eq1}}\label{alg:Framwork}
\begin{algorithmic}
\STATE
\begin{description}
  \item[\bf Step 0.] Choose $x^{-1}=x^0\in C$, $\{\beta_k\}\subseteq\left[0,\sqrt{\frac{L_g}{L_g + \ell_g}}~\right)$ with $\bar\beta:= \sup_k\beta_k < \sqrt{\frac{L_g}{L_g + \ell_g}}$, $\theta_0>0$, $d>0$, where $L_g = \max\{L_{g_i}, ~ i=1,\dots,m\}$ and $\ell_g = \max\{\ell_{g_i}, ~ i=1,\dots,m\}$ as in Remark~\ref{Remarkg}.
  \item[\bf Step 1.] Set
    \begin{equation}\label{defyk}
      y^k = x^k + \beta_k(x^k - x^{k-1}).
    \end{equation}
  \item[\bf Step 2.] Take any $\xi^k\in \partial P_2(x^{k})$ and compute
    \begin{align}\label{eq2}
    (x^{k+1},s^{k+1})\in \Argmin\limits_{(x,s)\in \mathbb{R}^{n+1}}\quad &P_1(x) - \langle \xi^k, x \rangle
	+ \theta_k s + \frac{\theta_k L_g}{2}\| x - y^k \|^2   \notag
	\\ \text{s.t.} \quad &\ling{i}{x}{y^k} \leq s ~~\forall i=1,\dots,m,
	\\& (x,s)\in C\times \mathbb{R_+},  \notag
	\end{align}
where ${\rm lin}_{g_i}$ is defined in \eqref{ling}.
  \item[\bf Step 3.]  If $\ling{i}{x^{k+1}}{y^k}\leq 0$ for all $i$, then $\theta_{k+1}=\theta_k$; otherwise $\theta_{k+1}=\theta_k+d$. Update $k\leftarrow k+1$ and go to step 1.
\end{description}
\end{algorithmic}
\end{algorithm}

The convergence properties of our algorithm will be studied in Section~\ref{sec4}, and we end this section by presenting some useful facts concerning the subproblem \eqref{eq2}. The first two items are trivial observations, and they are stated here for easy reference later.
\begin{lemma}\label{subproremarks}
Suppose that $x^k\in C$ is generated at the beginning of the $k$-th iteration of Algorithm \ref{alg:Framwork} for some $k\geq 0$. Then the following statements hold:
\begin{enumerate}[{\rm (i)}]
    \item $s^{k+1} = \max_{i = 1,\cdots,m}[\ling{i}{x^{k+1}}{y^k}]_+$.
    \item Problem \eqref{eq2} has a unique solution.
    \item Let $g_0:= 0$. Then $x^{k+1}$ is a component of the minimizer of the subproblem in \eqref{eq2} if and only if there exist $\lambda_i^k \geq 0$ for all $i\in I_k(x^{k+1})$ such that $\sum_{i\in I_k(x^{k+1})}\lambda_i^k = 1$ and
        \begin{equation*}%\label{KKT2}
        0\in \partial P_1(x^{k+1}) - \xi^k + \theta_k\sum_{i\in I_k(x^{k+1})} \lambda_i^k\nabla g_i(y^k) + \theta_kL_g(x^{k+1} - y^k) + \mathcal{N}_C(x^{k+1}),
        \end{equation*}
        where
        \begin{equation}\label{defiIk}
        I_k(x): = \left\{s\in\{0,1,\cdots,m\}: \ling{s}{x}{y^k} = \max_{i = 0, 1,\cdots,m}\ling{i}{x}{y^k}\right\}.
        \end{equation}
\end{enumerate}
\end{lemma}

\begin{proof}
The proofs of (i) and (ii) are standard and we omit the proofs for brevity.

We now prove (iii). Combining $g_0\equiv 0$ with items (i) and (ii), we see that $(x^{k+1},s^{k+1})$ is the minimizer of the subproblem in \eqref{eq2} if and only if $s^{k+1} = \max_{i = 1,\cdots,m}[\ling{i}{x^{k+1}}{y^k}]_+$ and
\begin{equation}\label{subproblem2}
x^{k+1} = \argmin\limits_{x\in C}~ P_1(x) - \langle \xi^k, x \rangle + \theta_k \max_{i = 0, 1,\cdots,m}\{\ling{i}{x}{y^k}\} + \frac{\theta_k L_g}{2}\| x - y^k \|^2.
\end{equation}
Then, from \cite[Theorem 23.8]{Ro70}, we have that $x^{k+1}$ is a minimizer of the convex problem \eqref{subproblem2} if and only if
\begin{align*}
0&\!\in\! \partial P_1(x^{k+1}) \!-\! \xi^k \!+\! \theta_k\partial\!\left(\max_{i = 0, 1,\cdots,m}\{\ling{i}{\cdot}{y^k}\}\right)\!(x^{k+1}) \!+\! \theta_kL_g(x^{k+1} - y^k) \!+\! \mathcal{N}_C(x^{k+1})\\
& \!\!\overset{\rm(a)}=\! \partial P_1(x^{k+1}) - \xi^k + \theta_k\conv\{\nabla g_i(y^k): i\in I_k(x^{k+1})\} + \theta_kL_g(x^{k+1} - y^k) + \mathcal{N}_C(x^{k+1}),
\end{align*}
where (a) follows from \cite[Exercise~8.31]{rock97a} with $I_k(\cdot)$ defined in \eqref{defiIk}.
\end{proof}


\section{Convergence properties}\label{sec4}

\subsection{Convergence analysis for ESQM$_{\rm e}$}\label{sec41}
We first show that the successive changes of the $\{x^k\}$ generated by ESQM$_{\rm e}$ vanish.
\begin{theorem}[Vanishing successive changes]\label{suffdec}
Consider \eqref{eq1} and let $\{(x^k,y^k,s^k,\theta_k)\}$ be generated by Algorithm \ref{alg:Framwork}. Then the following statements hold:
\begin{enumerate}[{\rm (i)}]
    \item The sequence $\{x^k\}$ belongs to $C$ and is bounded.
    \item Let $\bar{m} = \inf\{P(x):x\in C\}$. Then $\bar m \in \R$ and for any $k\geq 1$,
        $$
        Q(x^{k+1},x^{k},y^{k},\theta_{k+1}) \leq Q(x^k,x^{k-1},y^{k-1},\theta_k) - \left(1 - \frac{L_g + \ell_g}{L_g}\beta_k^2\right)\frac{L_g}{2}\| x^{k} - x^{k-1}\|^2,
        $$
    where
    \[
    Q(x,y,z,\theta):=\frac{P(x) - \bar{m}}{\theta} + \max_{i = 1,\cdots,m}\left[\ling{i}{x}{z}\right]_++ \frac{L_g}{2}\| x - y \|^2 + \frac{L_g}{2}\| x - z \|^2.
    \]
    \item It holds that $\sum_{k = 1}^{\infty} \frac{L_g - (L_g + \ell_g)\beta_k^2}{2} \| x^k - x^{k-1}\|^2 < \infty$, and $\lim_{k \rightarrow\infty}\|x^k - x^{k-1}\| = 0$ and $\lim_{k \rightarrow\infty}\|x^k - y^k\| = 0$.
\end{enumerate}
\end{theorem}

\begin{proof}
(i): Note that $\{x^k\}\subseteq C$ according to \eqref{eq2}. Since $C$ is compact, $\{x^k\}$ is bounded.

(ii): Note that the objective in the subproblem \eqref{eq2} is strongly convex. Using this, and noting the definition of $(x^{k+1},s^{k+1})$ in \eqref{eq2} as a minimizer and the fact that $(x^k,\max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^k}\right]_+)$ is feasible for \eqref{eq2}, we have for any $k\ge 0$ that
\begin{equation}
\begin{split}\label{eq6}
& P_1(x^{k+1}) \!-\! \langle \xi^k,x^{k+1}-x^k\rangle + \theta_k s^{k+1} + \frac{\theta_k L_g}{2}\| x^{k+1} - y^k  \|^2 \\
&\leq P_1(x^{k}) \!+\! \theta_k \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^k}\right]_+ \!+\! \frac{\theta_k L_g}{2}\| x^{k}-y^k \|^2 \!-\! \frac{\theta_k L_g}{2}\| x^{k+1}-x^k \|^2.
\end{split}
\end{equation}
Meanwhile, from Remark~\ref{Remarkg} and the definition of ${\rm lin}_{g_i}$ in \eqref{ling}, we see that whenever $k \ge 1$,
\begin{align}\label{gnonconvex}
&\!\! \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^k}\right]_+ \notag\\
&\!\!= \max_{i = 1,\cdots,m}\left[g^1_i(y^k) + \langle\nabla g^1_i(y^k), x^k - y^k \rangle - g^2_i(y^k) - \langle\nabla g^2_i(y^k), x^k - y^k \rangle\right]_+ \notag\\
&\!\!\overset{\rm(a)}{\leq}\!\! \max_{i = 1,\cdots,m}\left[g^1_i(x^{k}) \!-\! g^2_i(x^{k}) \!+\! \frac{\ell_{g_i}}{2}\|x^k \!-\! y^{k}\|^2\right]_+ \!\!\!\!=\!\!\! \max_{i = 1,\cdots,m}\left[g_i(x^{k}) \!+\! \frac{\ell_{g_i}}{2}\|x^k \!-\! y^{k}\|^2\right]_+ \notag\\
&\!\! \overset{\rm(b)}{\leq} \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^{k-1}}+ \frac{L_{g_i}}{2}\|x^k - y^{k-1}\|^2 + \frac{\ell_{g_i}}{2}\|x^k - y^{k}\|^2\right]_+ \notag\\
&\!\!\overset{\rm(c)}{\leq} \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^{k-1}}\right]_+ + \frac{L_g}{2}\|x^k - y^{k-1}\|^2 + \frac{\ell_{g}}{2}\|x^k - y^{k}\|^2,
\end{align}
where (a) holds because of the convexity of $g^1_i$ and the Lipschitz continuity of $\nabla g^2_i$, (b) follows from the Lipschitz continuity of $\nabla g_i$, and (c) holds because $L_g=\max\{L_{g_i}:\;i=1,\dots,m\}$ and $\ell_g=\max\{\ell_{g_i}:\;i=1,\dots,m\}$.
Then, we obtain that when $k \ge 1$,
\begin{align*}
&P(x^{k+1}) = P_1(x^{k+1}) - P_2(x^{k+1})\overset{\rm(a)}{\leq} P_1(x^{k+1}) - \langle\xi^k, x^{k+1} - x^k\rangle - P_2(x^{k})\\
& = P_1(x^{k+1}) - \langle \xi^k, x^{k+1} - x^k\rangle + \frac{\theta_k L_g}{2}\| x^{k+1} - y^k \|^2 - \frac{\theta_k L_g}{2}\| x^{k+1} - y^k\|^2 - P_2(x^{k})\\
&\overset{\rm(b)}{\leq} P_1(x^{k}) + \frac{\theta_k L_g}{2}\| x^{k}-y^k \|^2 - \theta_k s^{k+1} + \theta_k \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^{k}}\right]_+ \\
&~~~~~~ - \frac{\theta_k L_g}{2}\| x^{k+1} - x^k\|^2 - \frac{\theta_k L_g}{2}\| x^{k+1} - y^k\|^2 - P_2(x^{k}) \\
&\overset{\rm(c)}{\leq} P(x^{k}) + \theta_k \left(\max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^{k-1}}\right]_+
 + \frac{L_g}{2}\| x^k - y^{k-1}\|^2 + \frac{\ell_{g}}{2}\|x^k - y^{k}\|^2\right)\\
  & ~~~~~  + \frac{\theta_k L_g}{2}\| x^{k} - y^k\|^2 - \theta_k s^{k+1} - \frac{\theta_k L_g}{2}\| x^{k+1} - x^k\|^2 - \frac{\theta_k L_g}{2}\| x^{k+1} - y^k\|^2,
\end{align*}
where (a) holds because $P_2$ is convex and $\xi^k\in \partial P_2(x^k)$, (b) holds thanks to \eqref{eq6}, and (c) holds because of \eqref{gnonconvex}.

Rearranging terms in the above display and noting that $y^k - x^k = \beta_k(x^k - x^{k - 1})$ for $k \ge 0$ (thanks to the definition of $y^k$ in \eqref{defyk}), we have that for $k\ge 1$,
\begin{align}\label{eq8}
&P(x^{k+1}) + \theta_k s^{k+1} + \frac{\theta_k L_g}{2}\| x^{k+1} - x^k\|^2 + \frac{\theta_k L_g}{2}\| x^{k+1} - y^k\|^2 \notag\\
&\leq P(x^{k}) + \theta_k\max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^{k-1}}\right]_+ + \frac{\theta_k L_g}{2}\| x^k - y^{k-1}\|^2 \notag\\
&~~~~ + \frac{\theta_k (L_g + \ell_g)}{2}\beta_k^2\| x^{k} - x^{k-1}\|^2 \notag\\
&= P(x^{k}) + \theta_k\max_{i = 1,\cdots,m}[\ling{i}{x^{k}}{y^{k-1}}]_+ + \frac{\theta_k L_g}{2}\| x^{k} - x^{k-1}\|^2 \notag\\
&~~~~~~ + \frac{\theta_k L_g}{2}\| x^k - y^{k-1}\|^2 - \left(1 - \frac{L_g + \ell_g}{L_g}\beta_k^2\right)\frac{\theta_k L_g}{2}\| x^{k} - x^{k-1}\|^2.
\end{align}

Since $P$ is continuous and $C$ is a nonempty compact set, we see that $\bar{m} = \inf\{P(x):x\in C\} \in \R$. Then we can deduce from the definition of $Q$ and the observation $s^{k+1} = \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k+1}}{y^{k}}\right]_+$ (thanks to Lemma~\ref{subproremarks}(i)) that whenever $k\ge 1$,
\begin{align}\label{eq81}
&Q(x^{k+1},x^{k},y^{k},\theta_{k+1}) \notag\\
&= \frac{P(x^{k+1}) - \bar{m}}{\theta_{k+1}} + s^{k+1} + \frac{L_g}{2}\| x^{k+1} - x^{k} \|^2 + \frac{L_g}{2}\| x^{k+1} - y^{k} \|^2 \notag\\
&\overset{\rm(a)}{\leq} \frac{P(x^{k+1}) - \bar{m}}{\theta_k} + s^{k+1} + \frac{L_g}{2}\| x^{k+1} - x^{k} \|^2 + \frac{L_g}{2}\| x^{k+1} - y^{k} \|^2 \notag\\
&\overset{\rm(b)}{\leq} \frac{1}{\theta_k}\bigg[ P(x^{k}) - \bar{m} + \theta_k \max_{i = 1,\cdots,m}[\ling{i}{x^{k}}{y^{k-1}}]_+ + \frac{\theta_k L_g}{2}\| x^{k} - x^{k-1}\|^2 \notag\\
&~~~~ + \frac{\theta_k L_g}{2}\|x^k - y^{k-1}\|^2- \left(1 - \frac{L_g + \ell_g}{L_g}\beta_k^2\right)\frac{\theta_k L_g}{2}\| x^{k} - x^{k-1}\|^2\bigg] \notag\\
& = Q(x^k,x^{k-1},y^{k-1},\theta_k) - \left(1 - \frac{L_g + \ell_g}{L_g}\beta_k^2\right)\frac{L_g}{2}\| x^{k} - x^{k-1}\|^2,
\end{align}
where (a) holds because of the definition of $\bar{m}$ and the facts that $x^{k+1}\in C$ and $\{\theta_k^{-1}\}$ is nonincreasing, and (b) follows from \eqref{eq8} and the fact that $\frac{1}{\theta_k} > 0$.

(iii): Observe that, for any $k\geq 0$,
\begin{equation*}
\begin{split}
&Q(x^{k+1},x^{k},y^k,\theta_{k+1})  \\
&= \frac{P(x^{k+1}) - \bar{m}}{\theta_{k+1}} \!+\! \max_{i = 1,\cdots,m}[\ling{i}{x^{k+1}}{y^k}]_+ \!+\! \frac{L_g}{2}\| x^{k+1} - x^{k} \|^2 \!+\! \frac{L_g}{2}\| x^{k+1}-y^{k} \|^2 \!\geq\! 0.
\end{split}
\end{equation*}
Combining the above display with \eqref{eq81}, we have
\begin{align*}
&\sum_{k=1}^{\infty}\left(1 - \frac{L_g + \ell_g}{L_g}\beta_k^2\right)\frac{L_g}{2}\| x^{k} - x^{k-1} \|^2 \\
&\leq Q(x^1,x^{0},y^{0},\theta_1) - \liminf_{k\to \infty} Q(x^{k+1},x^{k},y^k,\theta_{k+1})\leq Q(x^1,x^{0},y^{0},\theta_1) <\infty .
\end{align*}
Finally, since $\bar{\beta}: = \sup_k\beta_k < \sqrt{\frac{L_g}{L_g + \ell_g}}$, we can deduce from the above display that
$$\lim_{k\to \infty}\| x^k - x^{k-1}\|=0.$$
Combining this with the definition of $y^k$ in \eqref{defyk}, we can obtain further that
$
\lim_{k\to \infty}\| y^k - x^k\| = \lim_{k\to \infty}\beta_k\| x^k - x^{k-1}\| =0$.
\end{proof}

Next, we recall the following assumption concerning the RCQ in Definition~\ref{RCQ}. This assumption was first introduced in \cite[Assumption~(A1)]{Ausleder13} for studying ESQM.

\begin{assumption}\label{A1}
For \eqref{eq1}, the $RCQ(x)$ holds at every $x\in C\cap \mathscr{F}$, and for every $x\in C\setminus \mathscr{F}$, there cannot exist $u_i$, $i\in I(x)$, such that
\begin{equation}\label{A11}
u_i\geq 0 ~~ \forall i\in I(x), ~~ \sum_{i\in I(x)}u_i=1, ~~ \left\langle\sum_{i\in I(x)}u_i\nabla g_i(x), z - x\right\rangle\geq0 ~~\forall z\in C,
\end{equation}
where $I(x) := \Big\{ s\in\{1, 2, \dots, m\}: g_s(x) = \max_{i = 1, 2, \dots, m} \{ g_i(x), 0\} \Big\}$.\footnote{From the definition of $I_k(\cdot)$ in \eqref{defiIk}, one can see that the difference between $I_k(\cdot)$ and $I(\cdot)$ is that $I_k(\cdot)\subseteq\{0, 1,\cdots,m\}$, while $I(\cdot)\subseteq\{1,\cdots,m\}$. We would also like to point out that while our definition of $I(x)$ seems to look slightly different from the corresponding definition, namely $T(x)$, in \cite{Ausleder13} (see the discussions before \cite[Eq.~(6)]{Ausleder13}), one can check that Assumption~\ref{A1} above is equivalent to \cite[Assumption~(A1)]{Ausleder13}.}
\end{assumption}
\begin{remark}\label{remarkRCQ}
\begin{enumerate}[{\rm (i)}]
  \item Using \cite[Remark~2.1]{Ausleder13}, one can deduce that if Assumption~\ref{A1} holds, then for any $x\in C$, there cannot exist $u_i$, $i\in I(x)$, such that \eqref{A11} holds.
  \item From \cite[Remark~2.2]{Ausleder13}, we know that if the $RCQ(x)$ holds at every $x\in C$, then Assumption~\ref{A1} holds.
\end{enumerate}
\end{remark}
Using Assumption~\ref{A1} and Theorem~\ref{suffdec}, we will prove in the next theorem that the sequence $\{\theta_k\}$ in Algorithm~\ref{alg:Framwork} is bounded. The same conclusion was established for ESQM in \cite[Theorem~3.1(b)]{Ausleder13}.
\begin{theorem}[Boundedness of $\{\theta_k\}$]\label{alpha}
Consider \eqref{eq1} and suppose that Assumption~\ref{A1} holds. Let $\{(x^k,y^k,\theta_k)\}$ be generated by Algorithm~\ref{alg:Framwork}, $A := \{k\in \mathbb{N}:\theta_{k+1}>\theta_k\}$, and let $|A|$ denote the number of elements in A. Then $|A|$ is finite, i.e., there exists $N_0\in \mathbb{N}$ such that $\theta_k \equiv \theta_{N_0}$ whenever $k\geq N_0$. Moreover, $s^{k+1} = 0$ whenever $k\geq N_0$.
\end{theorem}

\begin{proof}
Suppose to the contrary that $|A| = \infty$. Then by the definition of $\theta_k$ in Step~3 of Algorithm~\ref{alg:Framwork}, we have $\lim_{k\to \infty}\theta_k = \infty$ and $\lim_{k\to \infty}\theta_k^{-1}=0$.
	
We first claim that for each $i$, there exists $n_i\in \mathbb{N}$ such that, for all $k\geq n_i$,
\begin{equation*}
 g_i(y^k) + \langle \nabla g_i(y^k),x^{k+1} - y^k \rangle\leq 0.
\end{equation*}
Suppose not. Then there exists $i_0\in\{1,\dots, m\}$ and (infinite) subsequences $\{x^{k_j}\}$ and $\{y^{k_j}\}$ such that
\begin{equation*}
 g_{i_0}(y^{k_j}) + \langle \nabla g_{i_0}(y^{k_j}),x^{{k_j}+1} - y^{k_j} \rangle > 0.
\end{equation*}
Using this and recalling the definition of $I_k(\cdot)$ in \eqref{defiIk}, we have that for all $i\in I_{k_j}(x^{k_j+1})$,
\begin{equation*}
 g_{i}(y^{k_j}) + \langle \nabla g_{i}(y^{k_j}),x^{{k_j}+1} - y^{k_j} \rangle > 0.
\end{equation*}
In particular, $0 \notin  I_{k_j}(x^{k_j+1})$ (recall that $g_0 \equiv 0$). Now, in view of the finiteness of $\left\{I_{k_j}(x^{k_j+1})\right\}$ (since $I_{k_j}(x^{k_j+1})\subseteq \{1, \dots, m\}$ for all $j$), by passing to a further subsequence if necessary, we deduce that there exists a nonempty subset $I_0\subseteq \{1,\dots,m\}$ such that
$I_{k_j}(x^{k_j+1})\equiv I_0$ for all $j$. That is, for all $i\in I_0$,
\begin{equation}\label{eq10}
\ling{i}{x^{k_j+1}}{y^{k_j}} = \max_{s = 0, 1, \dots, m} \left\{ \ling{s}{x^{k_j+1}}{y^{k_j}}\right\} > 0~~ \forall j.
\end{equation}
In addition, from Lemma~\ref{subproremarks}(iii), we have that for each $k_j$, there exist $\lambda_i^{k_j} \geq 0$ for each $i\in I_{k_j}(x^{k_j + 1}) \equiv I_0$, such that $\sum_{i\in I_0}\lambda_i^{k_j} = 1$ and
\begin{align}\label{eq11}
0\in \theta_{k_j}^{-1}(\partial P_1(x^{k_j+1}) - \xi^{k_j}) + L_g(x^{k_j+1} - y^{k_j}) \!+\! \sum_{i\in I_0} \lambda_i^{k_j} \nabla g_i(y^{k_j}) \!+\! \mathcal{N}_C(x^{k_j+1}).
\end{align}
Now, since the sequences $\{x^k\}\subseteq C$ and $\{\lambda_i^{k_j}: i \in I_0\}$ are bounded, by passing to a further subsequence if necessary, we assume that $\lim_{j \to \infty} x^{k_j} = x^{*}$ for some $x^*$ and that for each $i\in I_0$, $\lim_{j \to \infty}\lambda_i^{k_j}= \bar{\lambda}_i$ for some $\bar \lambda_i$. Then $x^*\in C$, $\bar{\lambda}_i \ge 0$ (for each $i\in I_0$), $\sum_{i\in I_0} \bar{\lambda}_i = 1$ and $I_0 \subseteq \left\{i \in\{0,1,\cdots,m\}: g_i(x^*) = \max_{i=0,1,\cdots,m} g_i(x^*)\right\}$ (thanks to \eqref{eq10}, \eqref{ling} and Theorem~\ref{suffdec}(iii)). Since $0\notin I_0$, we see that
\[
I_0 \subseteq I(x^*) =\left\{i \in\{1,2,\cdots,m\}: g_i(x^*) = \max_{i=1,\cdots,m} \{g_i(x^*), 0\}\right\},
\]
where $I(x)$ was defined in Assumption~\ref{A1}.
Passing to the limit in \eqref{eq11}, and noting that $\lim_{j \to \infty} \theta_{k_j}^{-1} = 0$, $\lim_{k \to \infty}\| x^{k+1} - y^{k} \|=\lim_{k \to \infty}\| x^{k+1} - x^{k} \|=0$ (thanks to Theorem~\ref{suffdec}(iii)) and the fact that $\{\partial P_1(x^{k_j+1})\}$ and $\{\xi^{k_j}\}$ are uniformly bounded (thanks to the real-valuedness and convexity of $P_1$, $P_2$, the compactness of $C$ and \cite[Theorem~24.7]{Ro70}), we have upon invoking the closedness of $x\rightrightarrows {\cal N}_C(x)$ that
$$0\in \sum_{i\in I_0}\bar{\lambda}_i \nabla g_i(x^*) + \mathcal{N}_C(x^*),$$
which implies that
\begin{align*}
\left\langle \sum_{i\in I_0}\bar{\lambda}_i \nabla g_i(x^*), x-x^* \right\rangle \geq 0 ~~~~ \forall x\in C.
\end{align*}
Since $I_0\subseteq I(x^*)$, this contradicts Assumption~\ref{A1} (see also Remark~\ref{remarkRCQ}(i)).

Therefore, if $|A| = \infty$, then it must hold that for each $i$, there exists $n_i\in \mathbb{N}$, such that for any $k\geq n_i$,
\[
g_i(y^k) + \langle \nabla g_i(y^k),x^{k+1} - y^k \rangle\leq 0.
\]
Let $N_0 =  \max_{i = 1, \dots, m} n_i$. Then for all $i\in\{1, 2, \dots, m\}$ and for any $k\ge N_0$, we have
\[
g_i(y^k) + \langle \nabla g_i(y^k),x^{k+1} - y^k \rangle\leq 0.
\]
In view of this and the definition of $\theta_k$ in Step 3 of Algorithm \ref{alg:Framwork}, we must have $\theta_k\equiv \theta_{N_0}$ for all $k\geq N_0$, which contradicts $\theta_k\rightarrow\infty$. Thus, it must hold that $|A|\not= \infty$.

Since $|A|$ is finite, there exists $N_0\in \mathbb{N}$, such that $\theta_k\equiv\theta_{N_0}$ whenever $k\geq N_0$. From Step 3 of Algorithm \ref{alg:Framwork}, we known that for each $i$, $ g_i(y^k) + \langle \nabla g_i(y^k), x^{k+1} - y^k \rangle\leq 0$, for all $k\geq N_0$. Then Lemma~\ref{subproremarks}(i) asserts that $s^{k+1}=0$ for any $k\geq N_0$.
\end{proof}

We are now ready to prove that any cluster point of the $\{x^k\}$ generated by Algorithm~\ref{alg:Framwork} is a critical point of \eqref{eq1}.
\begin{theorem}[Subsequential convergence]\label{subconver}
Consider \eqref{eq1} and suppose that Assumption \ref{A1} holds. Let $\{(x^k,y^k,\theta_k)\}$ be generated by Algorithm \ref{alg:Framwork}. Then for any accumulation point $\bar x$ of $\{x^k\}$, there exists $\bar{\lambda}_i\geq 0$ for each $i\in \tilde{I}(\bar{x})$ such that $\sum_{i\in \tilde{I}(\bar{x})} \bar{\lambda}_i = 1$ and
\begin{equation}\label{critical3333}
        0\in \partial P_1(\bar{x}) - \partial P_2(\bar x) + \theta_{N_0}\sum_{i\in \tilde{I}(\bar{x})} \bar{\lambda}_i\nabla g_i(\bar{x}) + \mathcal{N}_C(\bar{x}),
\end{equation}
where $\tilde{I}(\bar{x}):=\left\{s \in\{0,1,\cdots,m\}: g_s(\bar{x}) = \max_{i=0,1,\cdots,m} \{g_i(\cdot)\}\right\}$ and $\theta_{N_0}$ is defined in Theorem~\ref{alpha}; moreover, $\bar x$ is a critical point of \eqref{eq1}.
\end{theorem}
\begin{proof}
Suppose that $\bar{x}$ is an accumulation point of $\{x^k\}$ with $\lim_{j\to \infty} x^{k_j} = \bar{x}$ for some convergent subsequence $\{x^{k_j}\}$. Let $\{\xi^k\}$ be generated in Algorithm~\ref{alg:Framwork} and $\{\lambda^k_i\}$ with $i\in I_k(x^{k+1})$ be as in Lemma~\ref{subproremarks}(iii). Then, in view of the finiteness of $\{I_{k_j}(x^{k_j+1})\}$ (since $I_{k_j}(x^{k_j+1})\subseteq \{0, 1, 2, \dots, m\}$ for all $j$), by passing to a further subsequence if necessary, we see that there exists a nonempty subset $I_0\subseteq \{0, 1,\dots,m\}$ such that $I_{k_j}(x^{k_j+1})\equiv I_0$.
Moreover, $\{\lambda^{k_j}_i\}$ for each $i \in I_{k_j}(x^{k_j+1})\equiv I_0$ is bounded as sequences of nonnegative numbers at most $1$, and $\{\xi^k\}$ is bounded thanks to the real-valuedness and convexity of $P_2$ and \cite[Theorem~24.7]{Ro70}. Passing to a further subsequence if necessary, we assume without loss of generality that $\lim_{j\to \infty} \lambda_i^{k_j} = \bar{\lambda}_i\geq 0$ for each $i\in I_0$ and $\lim_{j\to \infty} \xi^{k_j} = \bar{\xi}$; moreover, the property of $\{\lambda^{k_j}_i\}$ with $i\in I_{k_j}(x^{k_j+1})\equiv I_0$ guaranteed by Lemma~\ref{subproremarks}(iii) asserts that for all $j$, it holds that
\begin{equation}\label{inclusion}
\begin{aligned}
&0\in \partial P_1(x^{k_j+1}) \!-\! \xi^{k_j} \!+\! \theta_{k_j}L_g(x^{{k_j}+1} - y^{k_j}) \!+\! \theta_{k_j}\! \sum_{i\in I_0} \lambda_i^{k_j}\nabla g_i(y^{k_j}) \!+\! \mathcal{N}_C(x^{{k_j}+1})\\
&{\rm and}\ \ \sum_{i\in I_0}\lambda^{k_j}_i = 1,\ \ \ \lambda^{k_j}_{i} \ge 0\ \ \forall i \in I_{k_j}(x^{k_j+1})\equiv I_0.
\end{aligned}
\end{equation}
In addition, in view of \eqref{eq2}, we obtain that for each $j$,
\begin{equation}\label{giineq}
g_i(y^{k_j}) + \langle\nabla g_i(y^{k_j}), x^{{k_j}+1} - y^{k_j} \rangle \leq s^{{k_j}+1} ~~~~ \forall i =1,\dots,m.
\end{equation}

Now, note that $\lim_{k\to \infty} \| x^{k} - x^{k-1} \|=\lim_{k\to \infty} \| x^{k+1} - y^{k} \|=0$ (thanks to Theorem~\ref{suffdec}(iii)), $s^{k_j+1} = 0$ and $\theta_{k_j} \equiv \theta_{N_0}$, for any $k_j\geq N_0$ (thanks to Theorem~\ref{alpha}). Passing to the limit in \eqref{giineq} and \eqref{inclusion}, we see that
\begin{equation}\label{critical1}
g_i(\bar{x})\le 0 ~~ \forall i=1,\dots,m,\ \ \sum_{i\in I_0}\bar \lambda_i = 1,\ \ \ \bar\lambda_i \ge 0 \ \ \forall i\in I_0,
\end{equation}
and
\begin{equation}\label{critical3}
0\in \partial P_1(\bar{x}) - \partial P_2(\bar x) + \theta_{N_0}\sum_{i\in I_0} \bar{\lambda}_i\nabla g_i(\bar{x}) + \mathcal{N}_C(\bar{x}).
\end{equation}
where we also invoked the closedness of $\partial P_1$, $\partial P_2$ and $\mathcal{N}_C$ to deduce \eqref{critical3}. Furthermore, we have from the definition of $I_{k_j}(x^{k_j + 1})$ in \eqref{defiIk} (recall that $I_{k_j}(x^{k_j + 1})\equiv I_0$) and Theorem~\ref{suffdec}(iii) that
\begin{equation}\label{I0}
 I_0\subseteq \tilde{I}(\bar{x}):=\left\{s \in\{0,1,\cdots,m\}: g_s(\bar{x}) = \max_{i=0,1,\cdots,m} \{g_i(\bar{x})\}\right\}.
\end{equation}
Then the inclusion \eqref{critical3333} follows from \eqref{critical3} and \eqref{critical1} upon noting $I_0\subseteq \tilde{I}(\bar{x})$ (see \eqref{I0}) and defining $\bar \lambda_i = 0$ for $i\in \tilde I(\bar x)\setminus I_0$.

Finally, let $\hat{\lambda}_i := \theta_{N_0}\bar{\lambda}_i\geq 0$ for all $i\in I_0\cap\{1,2,\cdots,m\}$, and $\hat{\lambda}_i = 0$ for all $i\in \{1,2,\cdots,m\}\setminus I_0$. Then by \eqref{critical1} and $I_0\subseteq \tilde{I}(\bar{x})$ (see \eqref{I0}), we have that
\begin{equation}\label{critical2}
\hat{\lambda}_i g_i(\bar{x}) = 0 ~~ \forall i=1,\dots,m.
\end{equation}
Indeed, for each $i\in I_0$, we have that $g_i(\bar{x}) = 0$, and for each $i\notin I_0$, we have that $\hat{\lambda}_i = 0$.

Notice that $\nabla g_0 (\bar{x}) = 0$ (thanks to $g_0 \equiv 0$), using the definition of $\hat{\lambda}_i$ and \eqref{critical3}, we have
\begin{equation}\label{critical33}
\begin{aligned}
0&\in \partial P_1(\bar{x}) - \partial P_2(\bar{x}) + \sum_{i=1}^m \hat{\lambda}_i\nabla g_i(\bar{x}) + \mathcal{N}_C(\bar{x}).
\end{aligned}
\end{equation}
Combining \eqref{critical1}, \eqref{critical2}, \eqref{critical33} and the above definition of $\hat\lambda$, we conclude that $\bar{x}$ is a critical point of \eqref{eq1}.
\end{proof}


%\subsection{Global convergence}

We next derive the global convergence property of the $\{x^k\}$ generated by Algorithm~\ref{alg:Framwork}. We will need to make use of the following function,
\begin{equation}\label{defH}
H(x,y,z) := \frac{P(x) - \bar{m}}{\hat{\theta}} + \delta_{C}(x) + \max_i[\ling{i}{x}{z}]_+ + \frac{L_g}{2}\| x-y \|^2 + \frac{L_g}{2}\| x - z \|^2,
\end{equation}
where $\bar{m}$ is defined in Theorem~\ref{suffdec}(ii), and $\hat{\theta}:= \theta_{N_0}$ with $N_0$ defined in Theorem~\ref{alpha}. Our analysis follows the nowadays standard convergence arguments based on Kurdyka-{\L}ojasiewicz property; see, for example, \cite{attouch10,attouch13,bolte14}. In essence, under Assumption~\ref{A1}, we will show that $H$ has sufficient descent along the sequence $\{(x^{k+1},x^{k},y^k)\}$ for all sufficiently large $k$, and $H$ is constant on the set of accumulation points of $\{(x^{k+1},x^{k},y^k)\}$. We will also show that $ \d(0,\partial H(x^{k+1},x^{k},y^k))$ is suitably bounded by successive changes of the iterates by imposing additional differentiability assumptions on $g$ and $P_2$. These together with the assumption that $H$ satisfies the KL property will be used to establish global convergence of the $\{x^k\}$ generated by Algorithm~\ref{alg:Framwork}.

We start with a remark concerning the sufficient descent property.
\begin{remark}[Sufficient descent]\label{rebarh}
Consider \eqref{eq1} and suppose that Assumption~\ref{A1} holds. Notice from the definition of $Q$ in Theorem~\ref{suffdec}{\rm (ii)} that $H(x,y,z) = Q(x,y,z,\hat\theta) + \delta_C(x)$. Now, according to Theorem~\ref{alpha}, we have $\theta_k \equiv \theta_{N_0} = \hat\theta$ for all $k\geq N_0$. Thus, we have $H(x^{k+1},x^{k},y^k) = Q(x^{k+1},x^{k},y^k,\hat{\theta})$ for all $k\geq N_0$. Then one can see that the sequence $\{H(x^{k+1},x^{k},y^k)\}_{k\ge N_0}$ is nonincreasing thanks to Theorem~\ref{suffdec}{\rm (ii)}, and it holds that
\[
H(x^{k+1},x^{k},y^{k}) \leq H(x^{k},x^{k-1},y^{k-1}) - \frac{L - (L + \ell)\bar{\beta}^2}{2}\| x^{k} - x^{k-1}\|^2\ \ \ \ \forall k\ge N_0,
\]
where $\bar{\beta} = \sup_k\beta_k$, and notice that $L > (L + \ell)\bar\beta^2$ thanks to the choice of $\{\beta_k\}$.
\end{remark}

\begin{lemma}\label{lemma1}
Consider \eqref{eq1} and suppose that Assumption~\ref{A1} holds. Let $\{(x^k,y^k)\}$ be generated by Algorithm \ref{alg:Framwork}, and $\Omega$ be the set of accumulation points of $\{(x^{k+1},x^{k},y^{k})\}$. Then $\Omega$ is a nonempty compact set, $\omega := \lim_{k\to \infty} H(x^{k+1},x^{k}, y^{k})$ exists, and $H \equiv \omega$ on $\Omega$.
\end{lemma}
\begin{proof}
From Theorem~\ref{suffdec}(i), we have that the set of accumulation points of $\{x^k\}$, denoted by $\varLambda$, is a nonempty compact set. Since $\lim_{k\to \infty} \| x^k - x^{k-1} \|=\lim_{k\to \infty} \| x^k - y^k\| = 0$ thanks to Theorem~\ref{suffdec}(iii), one can see that $\Omega= \{(\bar{x},\bar{x},\bar{x}): \bar{x}\in \varLambda\}$, which is a nonempty compact set.

Next, according to Remark~\ref{rebarh}, the sequence $\{H(x^{k+1},x^{k}, y^{k})\}_{k\ge N_0}$ is nonincreasing. Moreover, one can see from the definition of $H$ (see \eqref{defH}) that $\{H(x^{k+1},x^{k}, y^{k})\}$ is bounded from below (by zero). Thus, $\omega := \lim_{k\to \infty} H(x^{k+1},x^{k}, y^{k})$ exists.

For any $(\bar{x},\bar{x},\bar{x})\in \Omega$, let $\{x^{k_j}\}$ be a convergent subsequence with $\lim_{j\to \infty} x^{k_j} = \bar{x}$.
Since $P$ and each $g_i$ are continuous, and $\lim_{k\to \infty} \| x^k - x^{k-1}\| = \lim_{k\to \infty} \| x^k - y^{k}\| = 0$ (see Theorem~\ref{suffdec}(iii)), we obtain that
\begin{align*}
& H(\bar{x}, \bar{x}, \bar{x}) = \frac{P(\bar{x}) - \bar{m}}{\hat{\theta}} + \max_{i = 1,\cdots,m}\left[\ling{i}{\bar x}{\bar x}\right]_+\\
&\!\!=\!\lim_{j\to \infty}\! \frac{P(x^{k_j+1}) \!-\! \bar{m}}{\hat{\theta}} \!+\!\!\max_{i = 1,\cdots,m}\!\left[\ling{i }{x^{k_j+1}}{y^{k_j}}\right]_+ \!\!\!+\! \frac{L_g}{2}\| x^{k_j+1} \!\!-\! x^{k_j} \|^2 \!+\! \frac{L_g}{2}\| x^{k_j+1} \!\!-\! y^{k_j} \|^2 \\
&\!\!=\lim_{j\to \infty} H(x^{k_j+1},x^{k_j},y^{k_j}) = \lim_{k\to \infty} H(x^{k+1},x^{k}, y^{k}) = \omega.
\end{align*}
Since $(\bar{x}, \bar{x}, \bar{x})\in \Omega$ is arbitrary, we conclude that $H \equiv \omega$ on $\Omega$.
\end{proof}


Next, we introduce an assumption for deriving a bound on $\d(0,\partial H (x^{k+1}, x^k, y^k))$. This assumption was also used in \cite{wen18,yu21} and is satisfied in many applications; see \cite{wen18}.
\begin{assumption}\label{A2}
Each $g_i$ in \eqref{eq1} is twice continuously differentiable. The function $P_2$ is continuously differentiable on an open set $U_0$ containing $\mathcal{X}$, and $\nabla P_2$ is locally Lipschitz continuous on $U_0$, where $\mathcal{X}$ be the set of critical points of \eqref{eq1}.
\end{assumption}

Now, we present the following bound on $\d(0,\partial H(x^{k+1},x^{k},y^{k}))$.
\begin{lemma}\label{th2.1}
Consider \eqref{eq1} and suppose that Assumptions~\ref{A1} and \ref{A2} hold. Let $\{(x^k,y^k)\}$ be generated by Algorithm \ref{alg:Framwork}. Then there exist $\tau > 0$ and $N_1\in \mathbb{N}$ such that for all $k \ge N_1$, we have
\[
\d (0,\partial H(x^{k+1},x^{k},y^{k}))\le \tau (\|x^{k+1} - x^{k}\| + \|x^{k} - x^{k-1}\|).
\]
%\begin{enumerate}[{\rm (i)}]
%    \item
%    \item The sequence $\{x^k\}$ globally converges to a critical point of \eqref{eq1}. Moreover, $\sum_{k=1}^{\infty} \| x^{k+1} - x^{k}\|<\infty.$
%\end{enumerate}
\end{lemma}
\begin{proof}
Let $\varLambda$ be the set of accumulation points of $\{x^k\}$. Then $\varLambda$ is nonempty and compact in view of Theorem~\ref{suffdec}(i), and $\varLambda\subseteq {\cal X}$ thanks to Theorem~\ref{subconver}. Moreover, we have $\lim_{k\to \infty}\d(x^k,\varLambda)=0$. Since $\Lambda\subseteq {\cal X}\subset U_0$ and $\Lambda$ is compact, there exists $N_2\in \mathbb{N}$ such that $x^k\in U_0$ (where $U_0$ is defined as in Assumption \ref{A2}) for all $k\geq N_2$.

Next, let $N_0$ be defined as in Theorem \ref{alpha}. Since $P_2$ is continuously differentiable on $U_0$ and $x^k\in U_0$ for any $k\geq N_1 := \max\{N_0,N_2\}$, we obtain from \cite[Theorem~8.6]{rock97a} that for any $k \ge N_1$,
\begin{align}\label{eq16}
&\!\!\!\!\!\!\!\!\!\!~~~\partial H(x^{k+1},x^{k},y^{k})\supseteq \widehat{\partial} H(x^{k+1},x^{k},y^{k})\notag \\
&\!\!\!\!\!\!\!\!\!\!~~\overset{\rm(a)}\supseteq\!\!\!
\left[\!
\begin{array}{c}
\Xi_k    \\ [2 pt]
 - L_g(x^{k+1} - x^{k}) \\ [2 pt]
 \conv\!_{i\in I_k(x^{k+1})}\{\nabla^2 g_i(y^{k})(x^{k+1} - y^{k})\} - L_g(x^{k+1}-y^{k})
\end{array}\!
\right]\\
&\!\!\!\!\!\!\!\!\!\!~~\overset{\rm(b)}{\supseteq}\!\!\!
\left[
\begin{array}{c}
\!\frac{1}{\hat{\theta}}\partial P(x^{k+1}) \!+ \!\!\!\!\sum\limits_{i\in I_k(x^{k+1})}\!\!\! \lambda_i^k \nabla g_i(y^k) \!+\! \mathcal{N}_C(x^{k+1}) \!+\! L_g(x^{k+1} \!-\! x^k) \!+\! L_g(x^{k+1} \!-\! y^k)    \\
 - L_g(x^{k+1} - x^{k}) \\ [5 pt]
\sum_{i\in I_k(x^{k+1})} \lambda_i^k\nabla^2 g_i(y^{k})(x^{k+1} - y^{k}) - L_g(x^{k+1}-y^{k})
\end{array}\!
\right]\!\!,\!\!\!\!\notag
\end{align}
where $I_k(x^{k+1})$ and $\lambda_i^k$ are defined as in Lemma~\ref{subproremarks}(iii) and
\[
\Xi_k \!\!:=\! \frac{1}{\hat{\theta}}\partial P(x^{k+1}) + \conv\!_{i\in I_k(x^{k+1})}\{\nabla g_i(y^k)\} + \mathcal{N}_C(x^{k+1}) + L_g(x^{k+1} - x^k) + L_g(x^{k+1} - y^k),
\]
and (a) holds because of the subdifferential calculus rules in \cite[Proposition~10.5, Corollary~10.9, Exercise~8.31]{rock97a} and the regularity of the closed convex convex set $C$ and the convex function $P_1$ (thanks to \cite[Proposition~8.12]{rock97a}), and (b) holds as $\sum_{i\in I_k(x^{k+1})} \lambda_i^k = 1$ and $\lambda_i^k\ge 0$ for all $i\in I_k(x^{k+1})$.

On the other hand, according to Theorem \ref{alpha} and the definition of $\hat\theta$ in \eqref{defH}, we have that $\theta_k \equiv \theta_{N_0} = \hat \theta$ for any $k\ge N_0$. Using this together with the property of $\lambda^k_i$ from Lemma~\ref{subproremarks}(iii) and the differentiability assumption of $P_2$, we obtain that for all $k \ge N_0$,
\begin{align*}
0\in \partial P_1(x^{k+1}) - \nabla P_2(x^{k}) + \hat{\theta}\sum_{i\in I_k(x^{k+1})} \lambda_i^k \nabla g_i(y^k) + \hat{\theta}L_g(x^{k+1} - y^{k})+ \mathcal{N}_C(x^{k+1}).
\end{align*}
Rearranging terms in the above display, we see that
\begin{align}\label{eq23}
\nabla P_2(x^{k}) \!-\! \hat{\theta}\sum_{i\in I_k(x^{k+1})} \lambda_i^k \nabla g_i(y^k) - \hat{\theta}L_g(x^{k+1} - y^{k})\in \partial P_1(x^{k+1}) + \mathcal{N}_C(x^{k+1}).
\end{align}

%We claim that for any $k\geq\max\{N_0, N_1\}$,
%\begin{align}
%&\frac{1}{\hat{\theta}}\left(-\hat{\theta}L_g(x^{k} - y^{k}) + \nabla P_2(x^{k}) - \nabla P_2(x^{k+1})\right)  \nonumber\\
%& \in \frac{1}{\hat{\theta}}\partial P(x^{k+1}) + \sum_{i\in I_k(x^{k+1})} \lambda_i^k \nabla g_i(y^k) + \mathcal{N}_C(x^{k+1}) + L_g(x^{k+1} - x^k).
%\end{align}
Since $P_2$ is continuously differentiable in $U_0$ (and hence at $x^k$ and $x^{k+1}$ when $k\ge N_1$), we obtain for any $k\ge N_1$ that
\begin{align}\label{eq22}
&\frac{1}{\hat{\theta}}\left(-\hat{\theta}L_g(x^{k} - y^{k}) + \nabla P_2(x^{k}) - \nabla P_2(x^{k+1})\right) \notag\\
& = \frac{1}{\hat{\theta}}\left(\hat{\theta}L_g(x^{k+1} - x^{k}) - \nabla P_2(x^{k+1}) + \hat{\theta}\sum_{i\in I_k(x^{k+1})} \lambda_i^k\nabla g_i(y^k)\right) \notag\\
&~~~~ + \frac{1}{\hat{\theta}}\left(\nabla P_2(x^{k}) - \hat{\theta}\sum_{i\in I_k(x^{k+1})} \lambda_i^k\nabla g_i(y^k)
 - \hat{\theta}L_g(x^{k+1} - y^{k})\right) \notag\\
& \overset{\rm(a)}{\in}\! \frac{1}{\hat{\theta}}\!\left(\hat{\theta}L_g(x^{k+1} \!-\! x^{k}) \!-\! \nabla P_2(x^{k+1}) + \hat{\theta}\!\!\!\!\sum_{i\in I_k(x^{k+1})}\!\! \lambda_i^k\nabla g_i(y^k)\right) \!+\! \frac{1}{\hat{\theta}}\partial P_1(x^{k+1}) \!+\! \mathcal{N}_C(x^{k+1}) \notag\\
& = \frac{1}{\hat{\theta}}\partial P(x^{k+1}) + \sum_{i\in I_k(x^{k+1})} \lambda_i^k \nabla g_i(y^k) + \mathcal{N}_C(x^{k+1}) + L_g(x^{k+1} - x^k)
\end{align}
where (a) follows from \eqref{eq23}, and the last equality holds thanks to \cite[Exercise~8.8(c)]{rock97a} and the fact that $P = P_1 - P_2$.

Combining \eqref{eq16} and \eqref{eq22}, for any $k\geq N_1$, we have
\begin{equation}
\begin{aligned}
\nonumber
\left[
\begin{array}{c}
\frac{1}{\hat{\theta}}\left(-\hat{\theta}L_g(x^{k} - y^{k}) + \nabla P_2(x^{k}) - \nabla P_2(x^{k+1})\right) + L_g(x^{k+1} - y^{k})\\
-L_g(x^{k+1} - x^{k})\\
\sum_{i\in I_k(x^{k+1})}\lambda_i^k\nabla^2 g_i(y^{k})(x^{k+1} - y^{k}) - L_g(x^{k+1} - y^{k})
\end{array}
\right]
\!\!\in\! \partial H(x^{k+1},x^{k},y^{k}).
\end{aligned}
\end{equation}
Since $\nabla P_2$ is Lipschitz continuous on $U_0$ (say, with modulus $L_{P_2}$), we see for any $k\geq N_1$ that
\begin{align*}
& \d\left(0,\partial H(x^{k+1},x^{k},y^{k})\right)^2 \\
&\leq \left\| \frac{1}{\hat{\theta}}\!\left(\!-\hat{\theta}L_g(x^{k} - y^{k}) \!+\! \nabla P_2(x^{k}) - \nabla P_2(x^{k+1})\right) \!+\! L_g(x^{k+1} \!-\! y^{k})\right\|^2\!\!\!\! +\! \| L_g(x^{k+1} - x^{k})\|^2 \\
&~~~~~~ + \left\| \sum_{i\in I_k(x^{k+1})} \lambda_i^k\nabla^2 g_i(y^k)(x^{k+1} - y^{k}) - L_g(x^{k+1} - y^{k})\right\|^2 \\
&\leq 3L_g^2\| x^{k} - y^{k}\|^2 + \frac{3}{\hat{\theta}^2}L_{P_2}^2\| x^{k+1} - x^{k}\|^2 + 3L_g^2\| x^{k+1} - y^{k}\|^2 + L_g^2\| x^{k+1} - x^{k} \|^2 \\
&~~~~~~ + 2\left(\sum_{i\in I_k(x^{k+1})} \lambda_i^k\nabla^2 g_i(y^k)\right)^2 \| x^{k+1}-y^{k} \|^2 + 2L_g^2\| x^{k+1} - y^{k}\|^2.
\end{align*}
The desired conclusion now follows immediately from the above display, the definition and the boundedness of $\{y^k\}$ (thanks to Theorem~\ref{suffdec}(i) and \eqref{defyk}) and the continuity of $\nabla^2 g_i$ (thanks to Assumption~\ref{A2}).
\end{proof}

Now, we present the convergence rate of the $\{x^k\}$ generated by Algorithm~\ref{alg:Framwork} under suitable assumptions. The proof is routine and we refer the readers to, for example, the proofs of Theorems 4.2 and 4.3 of \cite{wen18}.
\begin{theorem}[Convergence rate of Algorithm \ref{alg:Framwork} in nonconvex setting]\label{th2.2}
Consider \eqref{eq1}. Suppose that Assumptions~\ref{A1} and \ref{A2} hold, and $H$ in \eqref{defH} is a KL function. Let $\{(x^k, y^k)\}$ be generated by Algorithm~\ref{alg:Framwork} and $\Omega$ be the set of accumulation points of $\left\{(x^{k+1},x^{k},y^{k})\right\}$. Then $\{x^k\}$ converges to a critical point $\bar{x}$ of \eqref{eq1}. Moreover, if $H$ satisfies the KL property with exponent $\alpha\in [0,1)$ at every point in $\Omega$, then there exists $\underline{N}\in \mathbb{N}$ such that the following statements hold.
\begin{enumerate}[{\rm (i)}]
    \item If $\alpha=0$, then $\{x^k\}$ converges finitely.
    \item If $\alpha\in (0,\frac{1}{2}]$, then there exist $a_0\in(0, 1)$ and $a_1>0$ such that \[
        \|x^k - \bar{x}\|\leq a_1a_0^k ~~\forall k > \underline{N}.
        \]
    \item If $\alpha\in (\frac{1}{2},1)$, then there exists $a_2>0$ such that
        \[
        \|x^k - \bar{x}\|\leq a_2k^{-\frac{1-\alpha}{2\alpha-1}} ~~\forall k > \underline{N}.
        \]
\end{enumerate}
\end{theorem}


\subsection{Convergence analysis in convex setting}\label{sec42}
We study the convergence properties of Algorithm \ref{alg:Framwork} under the following convex settings.

\begin{assumption}\label{B1}
Suppose that in \eqref{eq1}, $P_2 = 0$ and $g_1,\ldots, g_m$ are convex.
\end{assumption}

\begin{assumption}\label{B2}
The Slater condition holds for $\mathcal{G}$ in \eqref{eq1}, i.e., there exists $\hat{x}\in C$ with $g_i(\hat{x})<0$ for $i=1,\dots,m$.
\end{assumption}
\begin{remark}\label{rem43}
If each $g_i$ is convex and Assumption~\ref{B2} holds, then $RCQ(x)$ holds at every $x\in C$, which implies that Assumption~\ref{A1} holds thanks to Remark~\ref{remarkRCQ}(ii).
\end{remark}

Now, we present the convergence properties of Algorithm \ref{alg:Framwork} under Assumptions~\ref{B1} and \ref{B2}. Unlike our convergence rate result in Theorem~\ref{th2.2} which was based on the KL property of the function $H$ in \eqref{defH}, our analysis in this section is based on the KL property of the following function:
\begin{equation}\label{definfalpha}
F_{\eta}(x) := \frac{1}{\eta}(P_1(x) - \hat m) + \delta_C(x) + \max_{i = 1,\cdots,m}\left[g_i(x)\right]_+,
\end{equation}
where $\eta > 0$ and $\hat{m} = \inf\{P_1(x): x\in C\}\in \R$. Compared with $H$, the explicit KL exponent of $F_{\eta}$ is more readily obtainable, as we will discuss in Section~\ref{sec5}. Our $F_{\eta}$-based analysis, however, will require an additional assumption concerning the set of minimizers of $F_{\hat\theta}$, where $\hat\theta$ is defined in \eqref{defH}. The next lemma recalls a well-known fact and serves to motivate such an assumption.
\begin{lemma}\label{lem43}
  Consider \eqref{eq1}. Suppose that Assumption~\ref{B1} holds and let $F_\eta$ be defined in \eqref{definfalpha}. If
  $\bar\eta > 0$ is such that $\Argmin F_{\bar\eta}\cap \Argmin_{x\in\cal G}P_1(x)\neq \emptyset$, then $\Argmin F_{\eta} = \Argmin_{x\in {\cal G}}P_1(x)$ whenever $\eta > \bar\eta$.
%  \begin{equation}\label{set3}
%\emptyset\not=\Lambda\subseteq \Argmin_{x\in {\cal G}}P_1(x) = \Argmin F_{\hat\theta} \cap \mathscr{F} = \Argmin F_{{\hat\theta}+\iota},
%\end{equation}
%for any $\iota > 0$.
\end{lemma}
\begin{proof}
Fix any $\eta > \bar\eta$ and let $\hat x \in \Argmin F_{\bar\eta}\cap \Argmin_{x\in\cal G}P_1(x)$.
We first argue that $\Argmin_{x\in\cal G}P_1(x) = \Argmin F_{\eta}\cap \mathscr{F}$. Indeed, if $\tilde x\in \Argmin_{x\in {\cal G}}P_1(x)$, then $\tilde x \in {\cal G}\subseteq \mathscr{F}$ and hence $\max_{i=1,\ldots,m}[g_i(\tilde x)]_+ = 0$. Moreover, it holds that
\begin{align*}
\eta F_{\eta}(\tilde x) = P_1(\tilde x)- \hat m\overset{\rm (a)}= P_1(\hat x)- \hat m = \bar\eta F_{\bar\eta}(\hat x)\overset{\rm (b)}\le \bar\eta F_{\bar\eta}(x)\overset{\rm (c)}\le \eta F_{\eta}(x)
\end{align*}
for any $x\in C$, where (a) holds because both $\hat x$ and $\tilde x$ minimizes $P_1$ over ${\cal G}$, (b) holds because $\hat x$ also minimizes $\bar\eta F_{\bar\eta}$, and (c) holds because $\eta> \hat\eta$. As for the converse inclusion, let $\tilde x \in \Argmin F_{\eta}\cap \mathscr{F}$. Then for any $x\in {\cal G} = C\cap \mathscr{F}$, we have
\[
P_1(\tilde x)- \hat m = {\eta} F_{{\eta}}(\tilde x)\le {\eta} F_{{\eta}}(x) = P_1(x)- \hat m,
\]
where the equalities hold because $u\in \mathscr{F}$ implies $\max_{i=1,\ldots,m}[g_i(u)]_+=0$. The above arguments establish $\Argmin_{x\in\cal G}P_1(x) = \Argmin F_{\eta}\cap \mathscr{F}$.

To complete the proof, it now suffices to show that $\Argmin F_{\eta}\subseteq \mathscr{F}$. To this end, let $\tilde x \in \Argmin F_{\eta}$. Then we have
\begin{align*}
&P_1(\tilde x)- \hat m + \eta \max_{i=1,\ldots,m}[g_i(\tilde x)]_+=\eta F_{\eta}(\tilde x) \le \eta F_{\eta}(\hat x) = P_1(\hat x)- \hat m + \eta \max_{i=1,\ldots,m}[g_i(\hat x)]_+ \\
&\!\!\overset{\rm (a)}= P_1(\hat x)- \hat m + \bar\eta\! \max_{i=1,\ldots,m}[g_i(\hat x)]_+
\!=\! \bar\eta F_{\bar\eta}(\hat x) \overset{\rm (b)}\le \bar\eta F_{\bar\eta}(\tilde x) = P_1(\tilde x)- \hat m + \bar\eta\!\max_{i=1,\ldots,m}[g_i(\tilde x)]_+,
\end{align*}
where (a) holds because $\hat x\in {\cal G}$ and (b) holds because $\hat x$ also minimizes $\bar\eta F_{\bar\eta}$. Rearranging terms in the above inequality, we obtain $(\eta-\bar\eta)\max_{i=1,\ldots,m}[g_i(\tilde x)]_+ = 0$, which means $\tilde x\in \mathscr{F}$.
\end{proof}

\begin{theorem}[Convergence rate of Algorithm \ref{alg:Framwork} in convex setting]\label{Edecres}
Consider \eqref{eq1} and suppose that Assumptions~\ref{B1} and \ref{B2} hold. Let $\{(x^k,\theta_k)\}$ be generated by Algorithm \ref{alg:Framwork}. Then the following statements hold.
\begin{enumerate}[{\rm (i)}]
    \item For any $k\ge 1$,
          \[
          E(x^{k+1},x^{k},\theta_{k+1}) \leq E(x^k,x^{k-1},\theta_k) - \frac{(1 - \beta_k^2)L_g}{2}\|x^{k} - x^{k-1}\|^2,
          \]
          where $E(x,y,\theta):=\frac{1}{\theta}\big(P_1(x) - \hat{m} + \delta_C(x) + \theta\max_{i = 1,\cdots,m}[g_i(x)]_+ + \frac{\theta L_g}{2}\| x - y \|^2\big)$, $\hat m$ is defined as in \eqref{definfalpha} and $L_g$ is defined in Remark~\ref{Remarkg}.
    \item Let $\Omega$ be the set of accumulation points of $\left\{(x^{k+1},x^{k},\theta_k)\right\}$. Then $\Omega$ is a nonempty compact set, $\bar{\omega} := \lim_{k\to \infty} E(x^{k+1},x^{k}, \theta_k)$ exists, and $E \equiv \bar{\omega}$ on $\Omega$.
    \item If the function\footnote{i.e., the $F_\eta$ in \eqref{definfalpha} with $\eta = \hat \theta$, where $\hat\theta$ is given in \eqref{defH}.} $F_{\hat\theta}$ is a KL function with exponent $\frac{1}{2}$ and it holds that
    \begin{equation}\label{assumptionunhappy}
    \Argmin F_{\hat\theta} = \Argmin_{x\in {\cal G}}P_1(x),
    \end{equation}
     then $\{x^k\}$ converges to a minimizer $x^*$ of \eqref{eq1}, and there exist $c_0 > 0$, $s\in (0,1)$ and $k_0\in \mathbb{N}$ such that
        \[
        \|x^k - x^*\|\leq c_0 s^k ~~\forall k > k_0.
        \]
\end{enumerate}
\end{theorem}
\begin{remark}[Comments on \eqref{assumptionunhappy}]
Consider \eqref{eq1} and suppose that Assumptions~\ref{B1} and \ref{B2} hold. Let $\{(x^k,\theta_k)\}$ be generated by Algorithm \ref{alg:Framwork}. Using Theorem~\ref{subconver} (see also Remark~\ref{rem43}) and the formula for the subdifferential of $\max_{i=1,\ldots,m}[g_i(\cdot)]_+$, we deduce from the definition of $F_\eta$ in \eqref{definfalpha} that
\begin{equation}\label{conditionunhappy}
\emptyset\neq \Lambda\subseteq \Argmin F_{\hat\theta} \cap \Argmin_{x\in\cal G}P_1(x).
\end{equation}
where ${\cal G}$ is the feasible set of \eqref{eq1} and $\Lambda$ is the set of accumulation points of $\{x^k\}$. Consequently, Lemma~\ref{lem43} implies that $\Argmin F_{\eta} = \Argmin_{x\in {\cal G}}P_1(x)$ whenever $\eta > \hat\theta$. In this regard, the assumption \eqref{assumptionunhappy} can be seen as a slightly stronger requirement where we require the equality of the two sets to hold when $\eta = \hat\theta$ as well. See Remark~\ref{rem52} below for further comments on \eqref{assumptionunhappy} based on the notion of exact penalty parameters in Definition~\ref{Exact}.
\end{remark}
%
%\begin{proof}
%(i): Since $\{x^k\}\subseteq C$ and $C$ is a nonempty compact convex set, $\{x^k\}$ is bounded.
\begin{proof}
Recall that $s^{k+1} = \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k+1}}{y^k}\right]_+$ (see Lemma~\ref{subproremarks}(i)). Using the definition of $(x^{k+1},s^{k+1})$ in \eqref{eq2} as a minimizer, the feasibility of $(x,\max_{i = 1,\cdots,m}\left[\ling{i}{x}{y^k}\right]_+)$ for \eqref{eq2} when $x\in C$ and the strong convexity of the objective in \eqref{eq2}, we obtain that for any $x\in C$,
\begin{align}\label{stongconve}
&P_1(x^{k+1}) + \theta_k \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k+1}}{y^k}\right]_+ + \frac{\theta_k L_g}{2}\| x^{k+1} - y^k\|^2 \notag\\
&= P_1(x^{k+1}) + \theta_k s^{k+1} + \frac{\theta_k L_g}{2}\| x^{k+1} - y^k\|^2 \notag\\
&\leq P_1(x) + \theta_k \max_{i = 1,\cdots,m}\left[\ling{i}{x}{y^k}\right]_+ + \frac{\theta_k L_g}{2}\| x - y^k \|^2 - \frac{\theta_k L_g}{2}\| x - x^{k+1}\|^2.
\end{align}
Now we are ready to prove the three items one by one.

(i): For any $k \ge 1$, we see that
\begin{align*}
&\frac{1}{\theta_{k+1}}\!\left(P_1(x^{k+1}) \!-\! \hat{m}\right) \!+\!\! \max_{i = 1,\cdots,m}\left[g_i(x^{k+1})\right]_+\overset{\rm(a)}{\leq} \frac{1}{\theta_k}\!\left(P_1(x^{k+1}) \!-\! \hat{m}\right)\! +\!\! \max_{i = 1,\cdots,m}\left[g_i(x^{k+1})\right]_+\\
&\overset{\rm(b)}{\leq} \frac{P_1(x^{k+1}) - \hat{m}}{\theta_k} + \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k+1}}{y^k} + \frac{L_{g_i}}{2}\| x^{k+1} - y^k \|^2\right]_+\\
&\overset{\rm(c)}{\leq} \frac{P_1(x^{k+1}) - \hat{m}}{\theta_k} + \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k+1}}{y^k} \right]_+ + \frac{L_{g}}{2}\| x^{k+1} - y^k \|^2\\
&\overset{\rm(d)}{\leq}\frac{P_1(x^{k}) - \hat{m}}{\theta_k} + \max_{i = 1,\cdots,m}\left[\ling{i}{x^k}{y^k}\right]_+ + \frac{L_{g}}{2}\| x^{k} - y^k \|^2 - \frac{L_g}{2}\| x^{k+1} - x^k \|^2\\
&\overset{\rm(e)}{\leq}\frac{P_1(x^{k}) - \hat{m}}{\theta_k} +  \max_{i = 1,\cdots,m}\left[g_i(x^k)\right]_+ + \frac{L_{g}}{2}\| x^{k} - y^k \|^2 - \frac{L_g}{2}\| x^{k+1} - x^k \|^2\\
&= \frac{1}{\theta_k}\!\left(\!P_1(x^{k}) - \hat{m} + \theta_k\max_{i = 1,\cdots,m}\left[g_i(x^k)\right]_+\!\right)\! +\! \frac{\beta_k^2 L_g}{2}\|x^k - x^{k-1}\|^2 - \frac{L_g}{2}\| x^{k+1} - x^k \|^2\\
&= E(x^{k},x^{k-1},\theta_k) - \frac{(1 - \beta_k^2)L_g}{2}\|x^k - x^{k-1}\|^2 - \frac{L_g}{2}\| x^{k+1} - x^k \|^2,
\end{align*}
where (a) holds thanks to $\theta_k \leq\theta_{k+1}$ and $\hat{m} = \inf\{P_1(x): x\in C\}$, (b) holds because of the Lipschitz continuity of $\nabla g_i$, (c) follows from $L_g = \max\{L_{g_i}:\; i=1,\dots,m\}$, (d) holds thanks to \eqref{stongconve} with $x=x^k$ (as $x^k\in C$), (e) follows from the convexity of $g_i$, and the last equality follows from the definition of $E(x^{k},x^{k-1},\theta_{k})$. The desired inequality now follows immediately from the above display and the definition of $E(x^{k+1},x^{k},\theta_{k+1})$.

(ii): Using similar arguments as Lemma~\ref{lemma1} (but using item (i) in place of Remark~\ref{rebarh}, and noting that Assumption~\ref{A1} holds according to Remark~\ref{rem43}), one can show that (ii) holds. We omit its proof for brevity.

(iii): Let $\Lambda$ be the set of accumulation points of $\{x^k\}$ for notational simplicity. Using Theorem~\ref{subconver} and the assumption \eqref{assumptionunhappy}, we conclude that
\begin{equation}\label{set}
\emptyset\not=\Lambda\subseteq \Argmin_{x\in {\cal G}}P_1(x) = \Argmin F_{\hat{\theta}} =: S.
\end{equation}
Now, write $E_{\theta}(x, y) := E(x, y, \theta)$ for notational simplicity. By the definitions of $F_\eta$ in \eqref{definfalpha} and $E(x, y, \theta)$ in item (i), we see that $E_{\hat{\theta}}(x, y) = F_{\hat{\theta}}(x) + \frac{L_g}{2}\|x - y\|^2$, where $\hat\theta$ is as in \eqref{defH}. From Remark~\ref{rem43}, Theorem~\ref{alpha} and item (i), we have that for any $k\geq N_0$, it holds that $\theta_k = \hat\theta$ and
\begin{equation}\label{upperK}
E_{\hat{\theta}}(x^{k+1}, x^k)\leq E_{\hat{\theta}}(x^k, x^{k-1}) -\frac{L_g(1 - \bar{\beta}^2)}{2}\|x^k - x^{k-1}\|^2.
\end{equation}
Moreover, since $F_{\hat{\theta}}$ is a KL function with exponent $\frac{1}{2}$, we conclude from \cite[Theorem~3.6]{li18} that $E_{\hat{\theta}}$ is a KL function with exponent $\frac{1}{2}$.

Let $\bar{S} = \{(x^*, x^*): x^*\in S\}$ and $\bar{\Lambda} = \{(x^*, x^*): x^*\in \Lambda\}$.
In view of \eqref{set}, we have $F_{\hat\theta}(\bar x) = \inf F_{\hat\theta}$ for any $\bar x\in S$. Using this together with item (ii) and the definition of $E_{\hat\theta}$, one can show readily that whenever $\bar{x}\in S$
\begin{equation}\label{valueomega}
\bar{\omega} = E_{\hat{\theta}}(\bar{x}, \bar{x}) = F_{\hat{\theta}}(\bar{x}) = \inf_x F_{\hat{\theta}}(x) = \inf_{x,y} E_{\hat{\theta}}(x, y).
\end{equation}
Moreover, in view of \eqref{set} and the definition of $E_{\hat\theta}$, we have
\begin{equation}\label{set2}
\emptyset\not=\bar{\Lambda}\subseteq \bar{S}= \Argmin\limits_{x,y} E_{\hat{\theta}}(x, y).
\end{equation}
Using \eqref{valueomega}, \eqref{set2} and Lemma~\ref{KLinequ}, we deduce that there exist $\epsilon_0 >0$, $r_0>0$, and $c_0>0$ such that
\begin{equation}\label{erro}
\d((x, y), \bar{S})^2\leq c_0(E_{\hat{\theta}}(x, y) - \bar\omega),
\end{equation}
for any $(x, y)\in\dom \partial E_{\hat{\theta}}$ satisfying $\d((x, y), \bar{S})\leq \epsilon_0$ and $\bar\omega\leq E_{\hat{\theta}}(x, y) \leq \bar\omega + r_0$.

Next, notice that $\{(x^k, x^{k-1})\}\subseteq C\times C\subset \dom \partial E_{\hat{\theta}} = C\times \R^n$. From Remark~\ref{rem43}, Theorem~\ref{subconver} and \eqref{set2}, we deduce that there exists $k_1\in \mathbb{N}$ such that
\begin{equation}\label{erro1}
\d((x^k, x^{k-1}), \bar{S})\leq\d((x^k, x^{k-1}), \bar{\Lambda})\leq\epsilon_0~~~~ \forall k\geq k_1.
\end{equation}
On the other hand, from Remark~\ref{rem43}, Theorem~\ref{alpha} and item (ii), we deduce the existence of $k_2\in \mathbb{N}$ such that
\begin{equation}\label{Ferro1}
\bar\omega\leq E_{\hat{\theta}}(x^k, x^{k-1})\leq\bar\omega + r_0 ~~~~ \forall k\geq k_2.
\end{equation}
Combining \eqref{erro}, \eqref{erro1} and \eqref{Ferro1}, we conclude that for any $k\geq k_3:=\max\{k_1, k_2\}$,
\begin{equation}\label{Eerro}
\d(x^{k}, S)^2\leq\d((x^k, x^{k-1}), \bar{S})^2\leq c_0(E_{\hat{\theta}}(x^k, x^{k-1}) - \bar\omega).
\end{equation}

Next, let $\bar{x}^k\in S$ satisfy $\|x^k - \bar{x}^k\| = \d(x^k, S)$. Then for any $k\geq N_0$ (note that $N_0$ is defined in Theorem~\ref{alpha}) and $\gamma\in(\frac{L_g c_0}{1 + L_gc_0}, 1)$, we have
\begin{align}\label{upperF}
&F_{\hat{\theta}}(x^{k+1})= \frac{1}{\hat{\theta}}\left(P_1(x^{k+1}) - \hat{m} + \hat{\theta}\max_{i = 1,\cdots,m}\left[g_i(x^{k+1})\right]_+\right) \nonumber\\
&\overset{\rm (a)}\leq \frac{1}{\hat{\theta}}\left(P_1(x^{k+1}) - \hat{m} + \hat{\theta}\max_{i = 1,\cdots,m}\left[\ling{i}{x^{k+1}}{y^k} + \frac{L_{g_i}}{2}\|x^{k+1} - y^k\|^2\right]_+\right) \nonumber\\
&\overset{\rm (b)}\leq\frac{1}{\hat{\theta}}\left(P_1(x^{k+1}) - \hat{m} + \hat{\theta}\max_{i = 1,\cdots,m}\left[\ling{i}{x^{k+1}}{y^k}\right]_+ + \frac{\hat{\theta}L_{g}}{2}\|x^{k+1} - y^k\|^2\right)\nonumber\\
&\overset{\rm (c)}\leq\frac{1}{\hat{\theta}}\left(P_1(\bar{x}^k) - \hat{m}\right) + \frac{L_g}{2}\|\bar{x}^k - y^k\|^2 - \frac{L_g}{2}\|\bar{x}^k - x^{k+1}\|^2 \nonumber\\
&\overset{\rm (d)}\leq F_{\hat{\theta}}(\bar{x}^k) + \frac{L_g}{2}\left(\|\bar{x}^k - x^k\| + \|x^k - y^k\|\right)^2 - \frac{L_g}{2}\|\bar{x}^k - x^{k+1}\|^2, \nonumber\\
&\overset{\rm(e)}\leq F_{\hat{\theta}}(\bar{x}^k)  + \frac{L_g}{2\gamma}\|\bar{x}^k - x^k\|^2 + \frac{L_g}{2(1 - \gamma)}\|x^k - y^k\|^2 - \frac{L_g}{2}\|\bar{x}^k - x^{k+1}\|^2\nonumber\\
&\overset{\rm(f)}\leq \bar\omega + \frac{L_g}{2\gamma}\d(x^k, S)^2 + \frac{L_g}{2(1 - \gamma)}\|x^k - y^k\|^2 - \frac{L_g}{2}\d(x^{k+1}, S)^2,
\end{align}
where (a) holds because of the Lipschitz continuity of $\nabla g_i$, (b) holds because $L_g = \max_{i = 1,\cdots,m}\{L_{g_i}\}$, (c) follows from \eqref{stongconve} with $x = \bar{x}^k$ (thanks to $\bar{x}^k\in S\subseteq C$) and $\ling{i}{\bar{x}^k}{y^k}\leq g_i(\bar{x}^k) \leq 0$ for each $i$ (thanks to the convexity of $g_i$ and $\bar{x}^k\in S \subseteq\mathcal{G}$), (d) follows from the triangle inequality and the fact that $g_i(\bar{x}^k)\leq 0$ for each $i$ (thanks to $\bar{x}^k\in S \subseteq\mathcal{G}$) so that $\max_{i = 1,\ldots,m}[g_i(\bar x^k)]_+ = 0$, (e) follows from the fact that $(a + b)^2 = (\gamma\frac{a}{\gamma} + (1 - \gamma)\frac{b}{(1-\gamma)})^2\leq\frac{a^2}{\gamma} + \frac{b^2}{(1-\gamma)}$ as $\gamma\in(0,1)$, and (f) holds thanks to \eqref{valueomega} and the definition of $\bar{x}^k$.

Then, we have for any $k\geq k_4:= \max\{k_3, N_0\}$ that
\begin{align*}%\label{upperE}
&E_{\hat{\theta}}(x^{k+1},x^k) - \bar\omega
= F_{\hat{\theta}}(x^{k+1}) - \bar\omega + \frac{L_g}{2}\|x^{k+1} - x^k\|^2 \\
&\overset{\rm(a)}\leq \frac{L_g}{2\gamma}\d(x^k, S)^2 + \frac{L_g}{2(1 - \gamma)}\|x^k - y^k\|^2 - \frac{L_g}{2\gamma}\d(x^{k+1}, S)^2  \nonumber\\
&~~~~+ \frac{L_g}{2}\left(\frac{1}{\gamma} - 1\right)\d(x^{k+1}, S)^2 + \frac{L_g}{2}\|x^{k+1} - x^k\|^2 \\
&\overset{\rm(b)}\leq \left(\frac{L_g}{2\gamma}\d(x^k, S)^2 - \frac{L_g}{2}\|x^k - x^{k-1}\|^2\right) - \left(\frac{L_g}{2\gamma}\d(x^{k+1}, S)^2 - \frac{L_g}{2}\|x^{k+1} - x^k\|^2\right) \nonumber\\
&~~~~+ \frac{L_g\bar{\beta}^2}{2(1 - \gamma)}\|x^k - x^{k-1}\|^2 + \frac{L_g}{2}\|x^k - x^{k-1}\|^2 + \frac{L_g}{2}\left(\frac{1}{\gamma} - 1\right)c_0(E_{\hat{\theta}}(x^{k+1}, x^{k}) - \bar\omega),
\end{align*}
where (a) holds because of \eqref{upperF}, and (b) follows from \eqref{Eerro}, $y^k = x^k + \beta_k(x^k - x^{k-1})$ and $\bar{\beta} = \sup_k\beta_k$.

Now, notice that $\gamma\in(\frac{L_g c_0}{1 + L_gc_0}, 1)$ implies $\frac{L_g}{2}(\frac{1}{\gamma} - 1)c_0 < \frac{1}{2}$. Letting $\vartheta: = 1 - \frac{L_g}{2}(\frac{1}{\gamma} - 1)c_0$, then we known that $\vartheta > \frac{1}{2}$. Rearranging terms in the above display inequality, we have that for any $k\ge k_4$,
\begin{align*}\label{upperE1}
&\vartheta \left(E_{\hat{\theta}}(x^{k+1},x^k) - \bar\omega\right) \\
& \leq \frac{L_g}{2}\left(\frac{1}{\gamma}\d(x^k, S)^2 - \|x^k - x^{k-1}\|^2\right) - \frac{L_g}{2}\left(\frac{1}{\gamma}\d(x^{k+1}, S)^2 -\|x^{k+1} - x^k\|^2\right) \nonumber\\
&~~~~+ \frac{L_g(1 - \gamma + \bar{\beta}^2)}{2(1 - \gamma)}\|x^k - x^{k-1}\|^2 \nonumber\\
&\overset{\rm(a)}\leq \frac{L_g}{2}\left(\frac{1}{\gamma}\d(x^k, S)^2 - \|x^k - x^{k-1}\|^2\right) - \frac{L_g}{2}\left(\frac{1}{\gamma}\d(x^{k+1}, S)^2 -\|x^{k+1} - x^k\|^2\right) \nonumber\\
&~~~~+ \frac{L_g(1 - \gamma + \bar{\beta}^2)}{2(1 - \gamma)}\cdot\frac{2}{L_g(1 - \bar{\beta}^2)}\left(E_{\hat{\theta}}(x^k, x^{k-1}) - E_{\hat{\theta}}(x^{k+1}, x^k)\right), \nonumber
\end{align*}
where (a) follows from \eqref{upperK}.

Denote $\zeta: = \frac{1 + \bar{\beta}^2 - \gamma}{(1-\gamma)(1 - \bar{\beta}^2)} > 1$ and $A_k := \frac{L_g}{2}(\frac{1}{\gamma}\d(x^k, S)^2 - \|x^k - x^{k-1}\|^2)$. Rearranging terms in the above inequality, we obtain that for any $k\ge k_4$,
\begin{equation*}
(\vartheta + \zeta) \left(E_{\hat{\theta}}(x^{k+1},x^k) - \bar\omega\right)  \leq A_k - A_{k+1} + \zeta\left(E_{\hat{\theta}}(x^{k},x^{k-1}) - \bar\omega\right).
\end{equation*}
Dividing $\vartheta + \zeta$ on both sides in the above inequality, we see that for any $k\ge k_4$,
\begin{equation}\label{upperE2}
E_{\hat{\theta}}(x^{k+1},x^k) - \bar\omega \leq \frac{\zeta}{\vartheta + \zeta}\left(E_{\hat{\theta}}(x^{k},x^{k-1}) - \bar\omega\right) + \frac{1}{\vartheta + \zeta}A_k - \frac{1}{\vartheta + \zeta}A_{k+1}.
\end{equation}
Since $\vartheta > \frac{1}{2}$ and $\zeta > 1$, we have that for any $k\ge k_4$,
\begin{align}\label{upperA}
&\left|\frac{A_k}{\vartheta + \zeta}\right|\leq \left|A_k\right|\leq \frac{L_g}{2}\left(\frac{1}{\gamma}\d(x^k, S)^2 + \|x^k - x^{k-1}\|^2\right) \nonumber\\
&\overset{\rm(a)}\leq\frac{L_g c_0}{2\gamma}\left(E_{\hat{\theta}}(x^k, x^{k-1}) - \bar\omega\right) + \frac{L_g}{2}\|x^k - x^{k-1}\|^2\nonumber\\
&\overset{\rm(b)}\leq\frac{L_g c_0}{2\gamma}\left(E_{\hat{\theta}}(x^k, x^{k-1}) - \bar\omega\right) + \frac{1}{(1 - \bar{\beta}^2)}\left(E_{\hat{\theta}}(x^k, x^{k-1}) - E_{\hat{\theta}}(x^{k+1}, x^{k})\right)\nonumber\\
&\overset{\rm(c)} \leq c_1\left(E_{\hat{\theta}}(x^k, x^{k-1}) - \bar\omega\right),
\end{align}
where (a) holds thanks to \eqref{Eerro}, (b) holds because of \eqref{upperK}, and (c) follows from $E_{\hat{\theta}}(x^{k+1}, x^{k})\geq\bar\omega$ (see \eqref{valueomega}) with $c_1:= \frac{L_g c_0}{2\gamma} + \frac{1}{(1 - \bar{\beta}^2)}$.

Let $\varrho = \frac{c_1 + \frac{\zeta}{\vartheta+\zeta}}{c_1+1}\in(0,1)$. Then one can see that
\begin{equation}\label{divi}
\frac{\zeta}{\vartheta+\zeta} + (1 - \varrho)c_1 = \varrho.
\end{equation}
Then, from \eqref{upperE2}, we obtain that for any $k\ge k_4$,
\begin{align*}
&E_{\hat{\theta}}(x^{k+1},x^k) - \bar\omega + \frac{1}{\vartheta + \zeta}A_{k+1} \leq \frac{\zeta}{\vartheta + \zeta}\left(E_{\hat{\theta}}(x^{k},x^{k-1}) - \bar\omega\right) + \frac{1}{\vartheta + \zeta}A_k \nonumber\\
&\overset{\rm(a)} \leq \frac{\zeta}{\vartheta + \zeta}\left(E_{\hat{\theta}}(x^{k},x^{k-1}) - \bar\omega\right) + \frac{\varrho}{\vartheta + \zeta}A_k + (1 - \varrho)\left|\frac{A_k}{\vartheta + \zeta}\right|\nonumber\\
&\overset{\rm(b)} \leq \left(\frac{\zeta}{\vartheta + \zeta} + (1 - \varrho)c_1\right)\left(E_{\hat{\theta}}(x^{k},x^{k-1}) - \bar\omega\right) + \frac{\varrho}{\vartheta + \zeta}A_k\nonumber\\
&\overset{\rm(c)} = \varrho \left(E_{\hat{\theta}}(x^{k},x^{k-1}) - \bar\omega + \frac{1}{\vartheta + \zeta}A_k \right),
\end{align*}
where (a) holds as $\varrho\in(0, 1)$, (b) follows from \eqref{upperA}, and (c) holds because of \eqref{divi}.

Inductively, since $\varrho > 0$, we see that for any $k\geq k_4$,
\begin{equation*}
E_{\hat{\theta}}(x^{k+1},x^k) - \bar\omega + \frac{1}{\vartheta + \zeta}A_{k+1} \leq \varrho^{k - k_4 +1}\left(E_{\hat{\theta}}(x^{k_4},x^{k_4 - 1}) - \bar\omega + \frac{1}{\vartheta + \zeta}A_{k_4}\right),
\end{equation*}
which means, there exists $M>0$ such that, for any $k\geq k_4$,
\begin{align}\label{inequ}
0&\leq E_{\hat{\theta}}(x^k,x^{k-1}) - \bar\omega\leq M\varrho^{k} - \frac{1}{\vartheta + \zeta}A_{k} \notag\\
&\overset{\rm(a)}= M\varrho^{k} \!-\! \frac{L_g}{2(\vartheta + \zeta)}\left(\!\frac{1}{\gamma}\d(x^k, S)^2 - \|x^k - x^{k-1}\|^2\!\right)\!\leq\! M\varrho^{k} \!+\! \frac{L_g}{2(\vartheta + \zeta)} \|x^k - x^{k-1}\|^2\notag\\
&\overset{\rm(b)}\leq M\varrho^{k} + \frac{1}{(\vartheta + \zeta)(1 - \bar{\beta}^2)}\left(E_{\hat{\theta}}(x^k,x^{k-1}) - E_{\hat{\theta}}(x^{k+1},x^{k})\right),
\end{align}
where (a) follows from the definition of $A_k$, and (b) holds because of \eqref{upperK}.

Taking $\mu > \max\{\frac{1}{(\vartheta + \zeta)(1 - \bar{\beta}^2)}, \frac{1}{1 - \varrho}\}$. From $\varrho\in(0, 1)$,  we see that
\begin{equation}\label{defmu}
\mu > 1 \text{ and } 1 - \mu^{-1} > \varrho,
\end{equation}
and from \eqref{inequ} (and \eqref{upperK}, which asserts the nonnegativity of the difference $E_{\hat{\theta}}(x^k,x^{k-1}) - E_{\hat{\theta}}(x^{k+1},x^{k})$), we have that
\begin{equation*}
E_{\hat{\theta}}(x^k,x^{k-1}) - \bar\omega \leq M\varrho^{k} + \mu(E_{\hat{\theta}}(x^k,x^{k-1}) - E_{\hat{\theta}}(x^{k+1},x^{k})),
\end{equation*}
which implies
\begin{equation*}
\mu(E_{\hat{\theta}}(x^{k+1},x^{k}) - \bar\omega) \leq (\mu - 1)\left(E_{\hat{\theta}}(x^k,x^{k-1}) - \bar\omega\right) + M\varrho^{k}.
\end{equation*}
Dividing $\mu>0$ on the both sides in the above display, we see that for any $k\geq k_4$,
\begin{align*}
&E_{\hat{\theta}}(x^{k+1},x^{k}) - \bar\omega \leq \left(1 - \mu^{-1}\right) \left(E_{\hat{\theta}}(x^k,x^{k-1}) - \bar\omega\right) + \frac{M}{\mu}\varrho^{k} \nonumber\\
&= \left(1 - \mu^{-1}\right) \left(E_{\hat{\theta}}(x^k,x^{k-1}) - \bar\omega\right) + \frac{M}{\mu}\left( \frac{1 - \mu^{-1}}{1 - \mu^{-1} - \varrho} - \frac{\varrho}{1 - \mu^{-1} - \varrho}\right)\varrho^{k} \nonumber\\
&= \left(1 - \mu^{-1}\right) \left(E_{\hat{\theta}}(x^k,x^{k-1}) - \bar\omega + \frac{M}{\mu(1 - \mu^{-1} - \varrho)}\varrho^{k} \right) - \frac{M}{\mu(1 - \mu^{-1} - \varrho)}\varrho^{k+1}, \nonumber
\end{align*}
where the division by $1 - \mu^{-1} - \varrho$ is valid thanks to \eqref{defmu}.
Rearranging terms in the above display inequality, we have that for any $k\geq k_4$,
\begin{equation*}
E_{\hat{\theta}}(x^{k+1},x^{k}) - \bar\omega + \frac{M\varrho^{k+1}}{\mu(1 - \mu^{-1} - \varrho)}
\leq \!\left(1 - \mu^{-1}\right)\! \left(E_{\hat{\theta}}(x^k,x^{k-1}) - \bar\omega + \frac{M\varrho^{k}}{\mu(1 - \mu^{-1} - \varrho)} \right)\!.
\end{equation*}
Inductively, since $1 - \mu^{-1} >0$ thanks to \eqref{defmu}, we see that for any $k\geq k_4$,
\begin{align}\label{inductE}
&\!\!\!E_{\hat{\theta}}(x^{k+1},x^{k}) - \bar\omega
 \leq
 E_{\hat{\theta}}(x^{k+1},x^{k}) - \bar\omega + \frac{M\varrho^{k+1}}{\mu(1 - \mu^{-1} - \varrho)} \nonumber\\
&\!\!\!\leq \left(\!1 - \mu^{-1}\!\right)^{k-k_4+1} \!\!\!\ \left(E_{\hat{\theta}}(x^{k_4},x^{k_4-1}) \!-\! \bar\omega \!+\! \frac{M\varrho^{k_4}}{\mu(1 - \mu^{-1} - \varrho)} \right) \overset{\rm(a)} = c_2 \left(\!1 - \mu^{-1}\!\right)^{k + 1},
\end{align}
where (a) holds with $c_2 := \left(1 - \mu^{-1}\right)^{-k_4}\left(E_{\hat{\theta}}(x^{k_4},x^{k_4-1}) - \bar\omega + \frac{M}{\mu(1 - \mu^{-1} - \varrho)}\varrho^{k_4} \right)> 0$.

Finally, we obtain that for any $k\geq k_4$,
\begin{align*}
\|x^{k+1} - x^k\|^2 & \overset{\rm(a)}\leq \frac{2}{L_g(1 - \bar{\beta})}\left(E_{\hat{\theta}}(x^{k+1}, x^k) - E_{\hat{\theta}}(x^{k}, x^{k-1})\right)\\
&\overset{\rm(b)}\leq\frac{2}{L_g(1 - \bar{\beta})} \left(E_{\hat{\theta}}(x^{k+1}, x^{k}) - \bar\omega\right)\overset{\rm(c)}\leq \frac{2c_2}{L_g(1 - \bar{\beta})}\left(1 - \mu^{-1}\right)^{k+1},
\end{align*}
where (a) holds because of \eqref{upperK}, (b) follows from $E_{\hat{\theta}}(x^{k+1}, x^k) \geq \bar\omega$ (see \eqref{valueomega}), and (c) holds because of \eqref{inductE}.
Consequently, for any $j$, $k\geq k_4$,
\begin{align}\label{bound}
\sum_{i = k}^{j}\|x^{i+1} - x^i\|&\leq \sum_{i = k}^{\infty} \sqrt{\frac{2c_2}{L_g(1 - \bar{\beta})}}\left(\sqrt{1 - \mu^{-1}}\right)^{i+1}= c_3 \left(\sqrt{1 - \mu^{-1}}\right)^{k+1},
\end{align}
with $c_3 := \sqrt{\frac{2c_2}{L_g(1 - \bar{\beta})}}\cdot\frac{1}{1 - \sqrt{1 - \mu^{-1}}} > 0$,
which implies that $\{x^k\}$ is a Cauchy sequence. Combining this with Remark~\ref{rem43} and Theorem~\ref{subconver}, we see that $\{x^k\}$ converges to a minimizer $x^*$ of \eqref{eq1}. The claimed linear rate of convergence also follows immediately from \eqref{bound}.
\end{proof}

\section{KL exponent and exact penalty}\label{sec5}

In Section~\ref{sec42}, the KL exponent of the $F_{\hat\theta}$ in Theorem~\ref{Edecres}(iii) was used for establishing the convergence rate of the $\{x^k\}$ generated by Algorithm~\ref{alg:Framwork} in the convex setting. In this section, we examine how the KL exponent of functions of the form \eqref{definfalpha} can be deduced from the corresponding problem \eqref{eq1}.

Specifically, we consider the following constrained optimization problem:
\begin{equation}\label{KLproblem}
\min_{x\in \R^n}\hat F(x):= P_1(x) + \delta_C(x) + \delta_{{\cal F}}(x),
\end{equation}
where $P_1:\R^n \to \R$ is convex, $C$ is compact and convex, ${\cal F} := \{x\in \R^n: g_i(x)\le 0, i = 1,\ldots,m\}$ with each $g_i:\mathbb{R}^n\to \R$ being convex, and $C\cap {\rm int}\,{\cal F} \neq \emptyset$; we also consider the associated penalty function
\begin{equation}\label{Feta}
\hat F_{\eta}(x): = P_1(x) + \delta_C(x) + \eta\max_i[g_i(x)]_+,
\end{equation}
where $\eta > 0$. Notice that for \eqref{eq1}, the KL property of the corresponding $\hat F_{\hat\theta}$ was the key for establishing the convergence rate of the $\{x^k\}$ generated by Algorithm~\ref{alg:Framwork}; see Theorem~\ref{Edecres}(iii).

We next recall the definition of exact penalty parameter.
\begin{definition}[Exact penalty parameter]\label{Exact}
Consider \eqref{KLproblem} and \eqref{Feta}. If there exists $\bar{\eta} > 0$ such that for all $\eta \geq \bar{\eta}$,
\[
\Argmin_{x\in C}\{P_1(x) + \eta\max_{i = 1,\cdots,m}[g_i(x)]_+\} = \Argmin_{x\in C\cap\mathcal{F}}\{P_1(x)\}
\]
then $\bar{\eta}$ is called an exact penalty parameter of \eqref{KLproblem}.
\end{definition}
We will argue that the set of exact penalty parameters of \eqref{KLproblem} is nonvoid. We start by recalling the following well-known result, whose short proof is included for the convenience of the readers.
\begin{lemma}\label{distF}
  Let $C$ and ${\cal F}$ be as in \eqref{KLproblem}. Then there exist $\kappa>0$ and $\tau > 0$ such that
  \begin{equation}\label{eb2}
  \d(x,C\cap {\cal F})\le \kappa \d(x,{\cal F}) \le \tau \max_{i=1,\ldots,m}[g_i(x)]_+\ \ \ \forall x\in C.
  \end{equation}
\end{lemma}
\begin{proof}
  First, since $C\cap {\rm int}\,{\cal F}\neq \emptyset$ (say, it contains $\hat x$), we deduce from \cite[Corollary~3]{BauschkeBorweinLi99} that there exists $\kappa > 0$ such that
  \begin{equation}\label{eb1}
  \d(x,C\cap {\cal F}) \le \kappa \d(x,{\cal F})\ \ \ \ \ \forall x\in C.
  \end{equation}
  Moreover, $\hat x \in {\rm int}\,{\cal F}$ implies that $\max_{i = 1,\ldots,m}g_i(\hat x) < 0$.
  We then apply Lemma~\ref{RobEB} with $\Omega:=\mathcal{F}$, $g(x) = (g_1(x), g_2(x), \cdots, g_m(x))$, $x^s = \hat{x}$ and $\delta_0 = \left|\max_i{g_i(\hat{x})}\right|$ to obtain
\begin{equation*}
\d(x, \mathcal{F}) \leq \frac{\|x - \hat{x}\|}{\left|\max_i{g_i(\hat{x})}\right|}\d(0, g(x) + \R^m_+) ~~~~ \forall x\in \R^n.
\end{equation*}
Since $C$ is compact, we deduce further that there exists $M_1> 0 $ such that
\begin{equation}\label{upperxF}
\d(x, \mathcal{F}) \leq M_1\max_{i = 1,\cdots,m}[g_i(x)]_+ ~~ \forall x\in C.
\end{equation}
The desired conclusion now follows upon combining \eqref{eb1} and \eqref{upperxF}.
\end{proof}
\begin{remark}[Nonemptiness of the set of exact penalty parameters]\label{rem51}
Since $C\cap {\cal F}$ is compact and $P_1$ is continuous, we see that $\Argmin \hat F\neq \emptyset$. Using this together with \eqref{eb2}, we can now deduce from \cite[Lemma~3.1]{ChenLuPong16} that any $\eta > \bar L_{P_1}\tau$ is an exact penalty parameter of \eqref{KLproblem}, where $\bar L_{P_1}$ is a Lipschitz continuity modulus for $P_1$ on the compact convex set $C$.
\end{remark}

Now, we show that if the $\hat F$ in \eqref{KLproblem} is a KL function with exponent $\alpha$, then for any $\eta>\bar{\eta}$,
the $\hat F_{\eta}$ is a KL function with exponent $\alpha\in (0,1)$, where $\bar{\eta}$ is an exact penalty parameter of \eqref{KLproblem}.

\begin{theorem}[KL exponent of $\hat F_{\eta}$ from that of $\hat F$]\label{FetaKL}
Let $\hat F$ be as in \eqref{KLproblem}, $\bar{x}\in\Argmin \hat F$ and $\bar{\eta}$ be an exact penalty parameter of \eqref{KLproblem}. If $\hat F$ satisfies the KL property with exponent $\alpha\in (0,1)$ at $\bar{x}$, then for any $\eta>\bar{\eta}$, the $\hat F_{\eta}$ defined in \eqref{Feta} satisfies the KL property with exponent $\alpha$ at $\bar{x}$.
\end{theorem}

\begin{proof}
Fix any $\eta > \bar\eta$. Since $\bar{\eta}$ is an exact penalty parameter of \eqref{KLproblem}, we see that $\Argmin \hat F = \Argmin \hat F_{\eta}$; also, note that $\dom\partial \hat F_{\eta} = C$.

Since $\hat F$ satisfies the KL property with exponent $\alpha$ at $\bar{x}$ and $C$ is compact, there exist $c>0$ and $a$, $\epsilon\in (0,1)$ such that
\begin{equation}\label{FKL}
\d(x, \Argmin \hat F) \leq c(\hat F(x) - \hat F(\bar{x}))^{\alpha},
\end{equation}
whenever $ x\in\dom \partial \hat F = C\cap \mathcal{F}$ satisfies $\|x - \bar{x}\|\leq \epsilon$ %$\d(x, \Argmin F)\leq\epsilon$
and $\hat F(\bar{x}) \leq \hat F(x) \leq \hat F(\bar{x}) + a$. Since $\hat F$ is continuous on its domain, by shrinking $\epsilon$ further if necessary, we assume that \eqref{FKL} holds whenever $ x\in\dom \partial \hat F = C\cap \mathcal{F}$ satisfies $\|x - \bar{x}\|\leq \epsilon$.

Next, since $P_1:\R^n\to\R$ is convex, we know that $P_1$ is locally Lipschitz continuous at $\bar{x}$. Hence, there exist $\bar{\epsilon} > 0$ and $\hat L_{P_1}>0$ such that
\begin{equation}\label{LipschP}
|P_1(x) - P_1(y)|\leq \hat L_{P_1}\|x - y\| ~~~~\forall x,y\in B(\bar{x}, \bar{\epsilon}).
\end{equation}

Now, take $\epsilon_0 := \min\{\epsilon, \bar{\epsilon}\}$. Then for any $x\in C = \dom\partial \hat F_{\eta}$ satisfying $\|x - \bar{x}\|\leq \epsilon_0$, we have upon letting $\Pi_{C\cap\mathcal{F}}(x)$ denote the orthogonal projection of $x$ onto $C\cap \mathcal{F}$ that
\begin{align}\label{distFeta}
&\d(x, \Argmin \hat F_{\eta})\leq \d(\Pi_{C\cap \mathcal{F}} (x), \Argmin \hat F_{\eta}) + \d(x, C\cap \mathcal{F}) \notag\\
&\overset{\rm(a)} = \d(\Pi_{C\cap \mathcal{F}} (x), \Argmin F) + \d(x, C\cap \mathcal{F}) \notag\\
&\overset{\rm(b)}\leq c(\hat F(\Pi_{C\cap \mathcal{F}} (x)) - \hat F(\bar{x}))^{\alpha} \!+\! \kappa\d(x, \mathcal{F}) = c(P_1(\Pi_{C\cap \mathcal{F}} (x)) - P_1(\bar{x}))^{\alpha} \!+\! \kappa\d(x, \mathcal{F}) \notag\\
&\overset{\rm(c)}\leq c(P_1(x) - P_1(\bar{x}) + \hat L_{P_1}\d(x, C\cap \mathcal{F}))^{\alpha} + \kappa\d(x, \mathcal{F}) \notag\\
&\overset{\rm(d)}\leq c(P_1(x) - P_1(\bar{x}) + \hat L_{P_1}\kappa\d(x, \mathcal{F}))^{\alpha} + \kappa\d(x, \mathcal{F})^{\alpha} \notag\\
& \overset{\rm (e)}\le\hat c \big[(P_1(x) - P_1(\bar{x}) + \hat L_{P_1}\kappa\d(x, \mathcal{F}))^{\alpha} + [\kappa^{1/\alpha}\d(x, \mathcal{F})]^{\alpha} \big]\notag\\
&\overset{\rm(f)}\leq \bar{c}(P_1(x) \!-\! P_1(\bar{x}) + \kappa_1\d(x, \mathcal{F}))^{\alpha}
\!\overset{\rm(g)}\leq\! \bar{c}\big(P_1(x) \!-\! P_1(\bar{x}) + \kappa_2\!\max_{i = 1,\cdots,m}\![g_i(x)]_+\big)^{\alpha},
\end{align}
where (a) holds because $\Argmin \hat F = \Argmin \hat F_{\eta}$, (b) holds because of Lemma~\ref{distF}, \eqref{FKL} and the fact that $\|\Pi_{C\cap\mathcal{F}}(x) - \bar{x}\|\leq\|x - \bar{x}\|\leq\epsilon_0\le\epsilon$ (thanks to $\bar{x}\in C\cap\mathcal{F}$ and the projection mapping being nonexpansive), (c) follows from \eqref{LipschP} and the fact that $\epsilon_0 \le \bar\epsilon$, (d) follows from \eqref{eb2} and the facts that $\d(x,\mathcal{F})\leq \|x - \bar{x}\|\leq\epsilon_0\le \epsilon<1$ and $\alpha\in (0,1)$, (e) holds with $\hat{c} = \max\{c, 1\}$, (f) holds with $\bar c = 2^{1-\alpha}\hat c$ and $\kappa_1 = \hat L_{P_1}\kappa + \kappa^{\frac{1}{\alpha}}$ thanks to the fact that $a^{\alpha} + b^{\alpha}\leq 2^{1 - \alpha}(a + b)^{\alpha}$ for any $a$, $b\ge0$ and $\alpha\in (0,1)$, and (g) holds with $\kappa_2 = \kappa_1\tau/\kappa$ thanks to \eqref{eb2}.

Now, if $\eta\geq\kappa_2$, then, from \eqref{distFeta}, we have that
\[
\d(x, \Argmin \hat F_{\eta})\leq \bar{c}\left(P_1(x) - P_1(\bar{x}) + \eta\max_{i = 1,\cdots,m}[g_i(x)]_+\right)^{\alpha}.
\]
On the other hand, if $\kappa_2 >\eta>\bar{\eta}$, then, from \eqref{distFeta}, we obtain that
\begin{align*}
&\d(x, \Argmin \hat F_{\eta})\leq \bar{c}\left(\!P_1(x) \!-\! P_1(\bar{x}) \!+\! \bar{\eta}\!\max_{i = 1,\cdots,m}[g_i(x)]_+ \!+\! (\kappa_2 - \bar{\eta})\!\max_{i = 1,\cdots,m}[g_i(x)]_+\!\right)^{\alpha}\\
&\overset{\rm(a)} \leq \bar{c}\left(\frac{\kappa_2 - \bar{\eta}}{\eta - \bar{\eta}}\right)^{\alpha}\left(P_1(x) - P_1(\bar{x}) + \bar{\eta}\max_{i = 1,\cdots,m}[g_i(x)]_+ + (\eta - \bar{\eta}) \max_{i = 1,\cdots,m}[g_i(x)]_+\right)^{\alpha}\\
&= \bar{c}\left(\frac{\kappa_2 - \bar{\eta}}{\eta - \bar{\eta}}\right)^{\alpha}\left(\hat F_{\eta}(x) - \hat F_{\eta}(\bar{x})\right)^{\alpha},
\end{align*}
where (a) holds because $a + b\leq\frac{1}{\epsilon}(a + \epsilon b)$ for any $a\geq 0$, $b \geq 0$ and $0<\epsilon\leq 1$.\footnote{We apply this relation to $\epsilon := (\eta-\bar\eta)/(\kappa_2 - \bar\eta)\in (0,1)$, $b:= (\kappa_2 - \bar{\eta})\max_{i = 1,\cdots,m} [g_i(x)]_+ \geq 0$, and $a:= P_1(x) - P_1(\bar{x}) + \bar{\eta}\max_{i = 1,\cdots,m}[g_i(x)]_+$, which is nonnegative because $\bar{\eta}$ is an exact penalty parameter, $\bar{x}\in\Argmin \hat F = \Argmin \hat F_{\bar\eta}$ and $x\in C$.} The desired conclusion now follows immediately.
\end{proof}

In the next remark, we comment on the condition $\eta > \bar\eta$ required in Theorem~\ref{FetaKL}.
\begin{remark}[On the condition $\eta > \bar \eta$ in Theorem~\ref{FetaKL}]\label{rem52}
  We comment on the applicability of Theorem~\ref{FetaKL}, which only infers the KL exponent of $\hat F_\eta$ when $\eta > \bar \eta$ for some exact penalty parameter $\bar\eta$.

  Particularly, we consider \eqref{eq1}. Suppose that Assumptions~\ref{B1} and \ref{B2} hold and let $\{\theta_k\}$ be generated by Algorithm~\ref{alg:Framwork}. Then we deduce by combining \eqref{conditionunhappy} and Lemma~\ref{lem43} that the set of exact penalty parameters is nonempty; indeed, it contains the interval $(\hat\theta,\infty)$. Hence, if we let $\tilde \eta$ denote the infimum of the set of exact penalty parameters, then $\hat\theta\ge \tilde\eta$.

  Now, note that we have $\theta_k \equiv \hat\theta$ whenever $k\ge N_0$ (where $N_0$ is defined in Theorem~\ref{alpha}) and $\theta_k$ is strictly increasing when $k \le N_0$. Intuitively, it is likely that the update rule of $\theta_k$ will result in $\hat\theta > \tilde\eta$. In this case, \eqref{assumptionunhappy} holds (thanks to Lemma~\ref{lem43}) and Theorem~\ref{FetaKL} asserts that the KL property required in Theorem~\ref{Edecres}(iii) can be inferred from that of $P_1+\delta_C+\delta_{\mathscr{F}}$ in \eqref{eq1}. On the other hand, in the case $\hat\theta = \tilde\eta$, it is unclear whether \eqref{assumptionunhappy} holds and Theorem~\ref{Edecres}(iii) may not be applicable.
\end{remark}

\begin{example}\label{RemarkKL}
Suppose that in \eqref{KLproblem}, $P_1 = \|\cdot\|_1$, $C$ is a polyhedron, $m = 1$, and $g_1 = q_1\circ A_1$ for some matrix $A_1\in \R^{s_1\times n}$ and $q_1:\mathbb{R}^{s_1} \rightarrow \mathbb{R}$ taking one of the following forms with $b\in \R^{s_1}$ and $\sigma > 0$ chosen so that the origin is not feasible and that Assumption~\ref{B2} holds:
\begin{enumerate}[{\rm (i)}]
   \item (Basis pursuit denoising \cite{Ca18}) $q_1(z) = \frac{1}{2}\|z - b\|^2 - \sigma$.
   \item (Logistic loss \cite{HoLS13}) $q_1(z) = \sum_{i=1}^{s_1}\log(1 + \exp(b_iz_i)) - \sigma$ for some $b\in \{-1,1\}^{s_1}$.
 \end{enumerate}
 Let $\bar{\eta}$ be the exact penalty parameter of \eqref{KLproblem}. We deduce from \cite[Corollary~5.1]{li18}, \cite[Corollary~5.1]{zhang23} and Theorem~\ref{FetaKL} that, for any $\eta>\bar{\eta}$, the KL exponent of the corresponding $\hat F_{\eta}$ in \eqref{Feta} (and hence the $F_\eta$ in \eqref{definfalpha}) is $\frac{1}{2}$.
\end{example}




\section{Numerical experiments}\label{sec6}

In this section, we perform numerical experiments to illustrate the performance of Algorithm~\ref{alg:Framwork}. Particularly, we consider the following model for compressed sensing:
\begin{equation}\label{CompSen}
\begin{aligned}
\min_{x\in\R^n}\quad &\|x\|_1 - \mu\|x\| \\
{\rm s.t.} \quad & h(Ax - b)\leq \sigma,
\end{aligned}
\end{equation}
where $\mu\in[0, 1]$, $A\in\R^{q\times n}$ has full row rank, $b\in \R^q$, $h: \R^q\rightarrow\R_+$ is an analytic function whose gradient is Lipschitz continuous with modulus $L_{h}$ and satisfies $h(0) = 0$, and $\sigma\in (0, h(-b))$.

Although the feasible region of \eqref{CompSen} is unbounded and Algorithm~\ref{alg:Framwork} cannot be directly applied to solving \eqref{CompSen}, one can argue as in the discussion following \cite[Eq.~(6.2)]{zhang23} that \eqref{CompSen} is equivalent to the following model:
\begin{equation}\label{CompSen1}
\begin{aligned}
\min_{x\in\R^n}\quad &\|x\|_1 - \mu\|x\| \\
{\rm s.t.} \quad & h(Ax - b) \leq \sigma,\\
           \quad & \|x\|_{\infty} \leq M,
\end{aligned}
\end{equation}
where $M: = (1 - \mu)^{-1}\Big(\|A^\dagger b\|_1 - \mu\|A^\dagger b\|\Big)$. Notice that the equivalent problem \eqref{CompSen1} is a special case of \eqref{eq1} with $P_1(x) = \|x\|_1$, $P_2(x) = \mu\|x\|$, $m = 1$, $g_1(x) = h(Ax - b)-\sigma$ and $C = \{x: \|x\|_{\infty}\leq M\}$. Since $A$ has full row rank and $h(0) = 0< \sigma$, we see that $A^\dagger b \in C\cap \{x: g_1(x) < 0 \}\not=\emptyset$.

Next, we will focus on \eqref{CompSen1} and consider two specific choices of $h$.

\subsection{$h(\cdot) = \frac{1}{2}\|\cdot\|^2$}
In this subsection, we take $h(\cdot) = \frac{1}{2}\|\cdot\|^2$, then \eqref{CompSen1} becomes
\begin{equation}\label{CompSen1-1}
\begin{aligned}
\min_{x\in\R^n}\quad &\|x\|_1 - \mu\|x\| \\
{\rm s.t.} \quad & 0.5\cdot\|Ax - b\|^2 \leq \sigma,\\
           \quad & \|x\|_{\infty} \leq M.
\end{aligned}
\end{equation}
%where $M: = (1 - \mu)^{-1}\Big(\|A^{\dag}b\|_1 - \mu\|A^{\dag}b\|\Big)$.
Notice that $h$ is convex, the Slater condition holds for the feasible region of \eqref{CompSen1-1}, and the origin is not feasible as $\sigma\in(0, \frac{1}{2}\|b\|^2)$. Then Assumptions~\ref{A1} and \ref{A2} hold. Since the $H$ corresponding to \eqref{CompSen1-1} is clearly a semi-algebraic and hence KL function, one can then apply Theorem~\ref{th2.2} with $\ell_g = 0$ to deduce the convergence of the (whole) sequence $\{x^k\}$ generated by Algorithm~\ref{alg:Framwork} for solving \eqref{CompSen1-1}.\footnote{Though we are not considering $\mu = 0$ in our experiments below, we also point out that when $\mu = 0$ in \eqref{CompSen1-1}, thanks to $\sigma\in(0, \frac{1}{2}\|b\|^2)$ and the fact that $A^\dagger b \in C\cap \{x: g_1(x) < 0 \}$, we can deduce from Example~\ref{RemarkKL} that $x\rightarrow \|x\|_1 + \delta_C(x) + \eta\max_{i = 1,\cdots,m} [g_i(x)]_+$ is a KL function with exponent $\frac{1}{2}$ whenever $\eta$ exceeds some exact penalty parameter. Therefore, according to Remark~\ref{rem52}, for the sequence $\{(x^k,\theta_k)\}$ generated by Algorithm~\ref{alg:Framwork}, if $\hat\theta$ exceeds some exact penalty parameter, then $\{x^k\}$ converges locally linearly thanks to Theorem~\ref{Edecres}(iii).}

We compare SCP$_{\rm ls}$ in \cite{yu21}, ESQM (i.e., setting $\beta_k \equiv 0$ in Algorithm~\ref{alg:Framwork}) and ESQM$_e$ (i.e., Algorithm~\ref{alg:Framwork}). We use the same parameter settings for SCP$_{\rm ls}$ in \cite{yu21}, and the initial point of SCP$_{\rm ls}$ is chosen as $x^0 = A^\dagger b$. In ESQM and ESQM$_e$, we take $L_g = \|A\|^2$, $\ell_g = 0$, $d = 1$ and $\theta_0 = 1$, and the initial points are chosen as $x^0 = 0$. We terminate all algorithms when $\|x^{k+1} - x^k\|< 10^{-8}\max\{1, \|x^{k+1}\|\}$. The subproblems in these algorithms are solved according to the procedures described in the appendices of \cite{yu21} and \cite{zhang23}.

We use the same strategy of choosing $\beta_k$ as in the FISTA with fixed and adaptive restart described in \cite{DonoghueCandes15}. In more detail, we set the initial values $\vartheta_{-1}=\vartheta_{0}=1$ and define, for $k\geq 0$,
\begin{align}\label{beta}
\beta_k = \frac{\vartheta_{k-1}-1}{\vartheta_{k}} \ \ {\rm with}\ \ \vartheta_{k+1}=\frac{1+\sqrt{1+4\vartheta_{k}^2}}{2},
\end{align}
and we reset $\vartheta_{k-1}=\vartheta_{k}=1$ every $K = 200$ iterations or when $\langle y^{k-1} - x^k, x^k - x^{k-1} \rangle>0$. One can show that $\{\beta_k\}$ generated this way satisfies $\{\beta_k\}\subseteq[0,1)$ and $\sup_{k}\beta_k<1$.

We perform tests on random instances of \eqref{CompSen1-1}. Specifically, we generate an $A\in\R^{q\times n}$ with independent and identically distributed (i.i.d.) standard Gaussian entries, and then normalize this matrix so that each column of it has unit norm. Then we choose a subset $T$ of size $k$ uniformly at random from $\{1, 2, \cdots, n\}$ and a $k$-sparse vector $x_{\rm orig}$ having i.i.d. standard Gaussian entries on $T$ is generated. We let $b = Ax_{\rm orig} + 0.01\cdot\hat{n}$ with $\hat{n}$ being a random vector having i.i.d. standard Gaussian entries, and $\sigma = 0.5\sigma_1^2$ with $\sigma_1 = 1.1\cdot\|0.01\cdot\hat{n}\|$.

In our numerical tests, we let $\mu =0.95$ in \eqref{CompSen1-1} and $(p,n,k) = (720i,2560i,80i)$ with $i\in \{2, 4, 6, 8, 10\}$. For each $i$, we generate 20 random instances as described above. We present the computational results in Table~\ref{table1}, averaged over the $20$ random instances. Here, we present the time for computing the QR decomposition of $A^T$ (denoted by $t_{\rm QR}$), the time for computing $\|A\|$ (denoted by $t_{\|A\|}$),  the time for computing $x^0 = A^\dagger b$ given the QR factorization of $A^T$ (denoted by $t_{A^\dagger b}$),\footnote{Note that $A^\dagger b$ is only used by SCP$_{\rm ls}$, and $\|A\|$ is only used by ESQM and ESQM$_e$.} the CPU times of the algorithms (CPU time), the number of iterations (denoted by Iter), the recovery errors $\text{RecErr} := \frac{\|x^* - \xorig\|}{\max\{1, \|\xorig\|\}}$ and the residuals ${\rm Residual} := \frac{\|Ax^* - b\| - \sigma }{\sigma}$, where $x^*$ is the approximate solution returned by the respective algorithm.


\begin{table}[h]
{\color{black}
\caption{Computational results for problem \eqref{CompSen1-1}}\label{table1}
\begin{center}
{\footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|}\hline
\phantom{\diagbox{Date}{$i$}} & \multicolumn{1}{|c|}{Method} & \multicolumn{1}{|c|}{$i = 2$} & \multicolumn{1}{c|}{ $i = 4$ }
& \multicolumn{1}{c|}{ $i = 6$ } & \multicolumn{1}{|c|}{ $i = 8$ } & \multicolumn{1}{c|}{ $i = 10$ }
\\\cline{1-7}\multirow{6}*{CPU time} & \multirow{1}*{$t_{\rm QR}$}
&  0.727 &  4.977 & 17.050 & 41.958 & 97.176        \\\cline{2-2} \multirow{1}*{} & \multirow{1}*{$t_{A^\dagger b}$}
&  0.007 &  0.030 &  0.069 &  0.123 &  0.252        \\\cline{2-2} \multirow{1}*{} & \multirow{1}*{$t_{\|A\|}$}
&  0.734 &  1.693 &  5.871 & 13.707 & 30.315        \\\cline{2-7} \multirow{1}*{} & \multirow{1}*{SCP$_{\rm ls}$}
&  1.784 &  5.817 & 12.781 & 21.565 & 44.421        \\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM}
&  5.552 & 20.651 & 47.041 & 79.073 & 161.768        \\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm e}}$}
&  0.759 &  2.715 &  6.345 & 10.671 & 22.689        \\\cline{1-7} \multirow{3}*{Iter} & \multirow{1}*{SCP$_{\rm ls}$}
&    138 &    140 &    137 &    137 &    140        \\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM}
&    753 &    819 &    805 &    806 &    804        \\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm e}}$}
&    112 &    112 &    114 &    113 &    114        \\\cline{1-7} \multirow{3}*{RecErr} & \multirow{1}*{SCP$_{\rm ls}$}
&  0.017 &  0.017 &  0.017 &  0.017 &  0.018       \\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM}
&  0.017 &  0.017 &  0.017 &  0.017 &  0.018       \\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm e}}$}
&  0.017 &  0.017 &  0.017 &  0.017 &  0.018       \\\cline{1-7} \multirow{3}*{Residual} & \multirow{1}*{SCP$_{\rm ls}$}
& -9.28e-14 & -3.25e-13 & -2.31e-13 & -2.69e-13 & -2.15e-13        \\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM}
& -3.52e-09 & -1.72e-09 & -1.14e-09 & -8.59e-10 & -6.76e-10        \\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm e}}$}
& -8.26e-10 & -1.18e-09 & -6.44e-10 & -7.86e-10 & -4.78e-10        \\\cline{1-7}
\end{tabular}
}
\end{center}
}
\end{table}

From Table~\ref{table1}, one can see that ESQM$_{{\rm e}}$ is the fastest, and the recovery errors of all three methods are comparable.

\subsection{When $h$ is the Lorentzian norm}

In this subsection, we consider $h$ being the Lorentzian norm \cite{CarrBarAys10}, which is defined as follows for any given $\gamma>0$:
\[
\|y\|_{L L_2, \gamma}:=\sum_{i=1}^q \log\left(1 + \frac{y_i^2}{\gamma^2}\right).
\]
Then, problem~\eqref{CompSen1} becomes the following problem:
\begin{equation}\label{CompSen1-2}
\begin{aligned}
\min_{x\in\R^n}\quad & \|x\|_1 - \mu\|x\| \\
{\rm s.t.}\quad & \|Ax - b\|_{L L_2, \gamma}\leq \sigma,\\
\quad & \|x\|_{\infty} \le M.
\end{aligned}
\end{equation}
We first argue that Assumption~\ref{A1} holds for \eqref{CompSen1-2} under our assumptions on $A$ and $\sigma$ in \eqref{CompSen1}. To this end, let $\hat h(y) := \|y\|_{L L_2, \gamma} - \sigma$ for notational simplicity. Then \eqref{CompSen1-2} is an instance of \eqref{eq1} with $g_1(x) = \hat h(Ax - b) - \sigma$ and $C := \{x:\; \|x\|_\infty \le M\}$. Now, recall that $A^\dagger b\in C$ by construction. Moreover, observe that for any $x\in C$, we have
\begin{align}\label{RCQexpression}
&\langle \nabla g_1(x), A^\dagger b - x\rangle = \langle A^T\nabla \hat h(Ax-b), A^\dagger b - x\rangle =  \langle \nabla \hat h(Ax-b), b - Ax\rangle\notag\\
& = 2\sum_{i=1}^q\frac{a_i^Tx - b_i}{\gamma^2 + (a_i^Tx - b_i)^2}\cdot (b_i - a_i^Tx)
= -2\sum_{i=1}^q\frac{(a_i^Tx - b_i)^2}{\gamma^2 + (a_i^Tx - b_i)^2},
\end{align}
where $a_i^T$ is the $i$-th row of $A$. Now, suppose that $I(x)$ (defined in Assumption~\ref{A1}) is nonempty, i.e., $I(x) = \{1\}$. If there exists $u_1$ such that \eqref{A11} holds, then we see from \eqref{RCQexpression} that $a_i^Tx = b_i$ for all $i$. But then $g_1(x) = \hat h(0) - \sigma = -\sigma < 0$, contradicting the assumption that $I(x) = \{1\}$. This together with Remark~\ref{remarkRCQ}(i) shows that Assumption~\ref{A1} holds.

Next, observe that the $\hat h$ has Lipschitz continuous gradient with modulus $\frac{2}{\gamma^2}$. The following proposition shows that $\hat h$ can be represented as the difference of two convex functions $\hat h_1$ and $\hat h_2$ with Lipschitz continuous gradients, and the Lipschitz continuity modulus of $\nabla \hat h_1$ is $\frac{2}{\gamma^2}$ while that of $\nabla \hat h_2$ is $\frac{1}{4\gamma^2}$.
\begin{proposition}
Let $\hat h(y): = \|y\|_{L L_2, \gamma} - \sigma$. Then there exist two convex functions $\hat h_1$ and $\hat h_2$ with Lipschitz continuous gradients such that $\hat h(y) = \hat h_1(y) - \hat h_2(y)$ and the Lipschitz continuity modulus of $\nabla \hat h_1$ is $\frac{2}{\gamma^2}$ while that of $\nabla \hat h_2$ is $\frac{1}{4\gamma^2}$.
\end{proposition}

\begin{proof}
First, notice that
\[
\frac{d^2}{dt^2}\log(1 + t^2) = \frac{2(1 - t^2)}{(1 + t^2)^2} = \left[\frac{2(1 - t^2)}{(1+t^2)^2}\right]_+ - \left[\frac{2(1 - t^2)}{(1+t^2)^2}\right]_-,
\]
where $s_+ := \max\{s,0\}\ge 0$ and $s_- := -\min\{s,0\}\ge 0$ for any $s\in \R$.
Now, define, for each $t\in \R$,
\[
r_1(t) =\int_0^t (t-s)\left[\frac{2(1 - s^2)}{(1+s^2)^2}\right]_+ds{\text{~ and ~}} r_2(t) =\int_0^t (t-s)\left[\frac{2(1 - s^2)}{(1+s^2)^2}\right]_-ds.
\]
Then we have
$r_1''(t) = \left[\frac{2(1 - t^2)}{(1 + t^2)^2}\right]_+$ and $r_2''(t) = \left[\frac{2(1 - t^2)}{(1 + t^2)^2}\right]_-$. Moreover, a direct computation shows that $\sup_t |r''(t)| = 2$ and $\sup_{t}|r_2''(t)| = \frac{1}{4}$.
Taking
\[
\hat h_1(y) = \sum_{i = 1}^m r_1(y_i/\gamma) - \sigma, \text{~  and  ~} \hat h_2(y) = \sum_{i = 1}^m r_2(y_i/\gamma),
\]
one can see that $\hat h_1$ and $\hat h_2$ are two convex functions with Lipschitz continuous gradients, and $\hat h(y) = \hat h_1(y) - \hat h_2(y)$. Furthermore, the continuity modulus of Lipschitz continuous gradients of $\hat h_1$ and $\hat h_2$ are $\frac{2}{\gamma^2}$ and $\frac{1}{4\gamma^2}$, respectively.
\end{proof}
In view of the above discussions, and the observation that the $H$ corresponding to \eqref{CompSen1-1} is a subanalytic function that is continuous on its closed domain (and hence a KL function in view of \cite[Theorem~3.1]{bolte07}), one can apply Theorem~\ref{th2.2} with $L_g = \frac{2\|A\|^2}{\gamma^2}$ and $\ell_g = \frac{\|A\|^2}{4\gamma^2}$ to deducing the convergence of the $\{x^k\}$ generated by Algorithm~\ref{alg:Framwork} with $\sup_{k}\beta_k < \sqrt{\frac{L_g}{L_g + \ell_g}} = \sqrt{\frac89}$ for solving \eqref{CompSen1-2}.

As in the previous subsection, we compare SCP$_{\rm ls}$ in \cite{yu21}, ESQM (i.e., setting $\beta_k \equiv 0$ in Algorithm~\ref{alg:Framwork}) and ESQM$_e$ (i.e., Algorithm~\ref{alg:Framwork}). We use the same parameter settings for SCP$_{\rm ls}$ in \cite{yu21}, and the initial point of SCP$_{\rm ls}$ is chosen as $x^0 = A^\dagger b$. In ESQM and ESQM$_e$, we take $L_g = \frac{2\|A\|}{\gamma^2}$, $\ell_g = \frac{\|A\|}{4\gamma^2}$, $d = \frac{\gamma^2}{20}$ and $\theta_0 = 0.05$, and the initial points are chosen as $x^0 = 0$. We terminate all algorithms when $\|x^{k+1} - x^k\|< 10^{-8}\max\{1, \|x^{k+1}\|\}$. Furthermore, the subproblems in these algorithms are solved according to the procedures described in the appendices of \cite{yu21} and \cite{zhang23}.

We also choose $\{\beta_k\}$ as described in \eqref{beta} but we set the fixed restart frequency as $K = 49$. This parameter will ensure that $\{\beta_k\}$ satisfies $\{\beta_k\}\subseteq\left[0,\sqrt{\frac{L_g}{L_g + \ell_g}}\right)$ and $ \sup_{k}\beta_k<\sqrt{\frac{L_g}{L_g + \ell_g}}$.

We perform tests on random instances of \eqref{CompSen1-1}. As in the previous section, we generate an $A\in\R^{q\times n}$ with i.i.d. standard Gaussian entries, and then normalize its columns. We then choose a subset $T$ of size $k$ uniformly at random from $\{1, 2, \cdots, n\}$ and generate a $k$-sparse vector $x_{\rm orig}$ with i.i.d. standard Gaussian entries on $T$. We let $b = Ax_{\rm orig} + 0.01\cdot\bar{n}$ with $\bar{n}_i\sim{\rm Cauchy}(0, 1)$, specifically, we generate $\bar{n}_i$ as $\tan(\pi(\tilde{n}_i - \frac{1}{2}))$ with $\tilde{n}$ being a random vector with i.i.d. entries uniformly chosen in $[0, 1]$. We then set $\sigma = 1.1\cdot\|0.01\cdot\tilde{n}\|_{L L_2, \gamma}$ with $\gamma = 0.05$.


In our numerical tests, we let $\mu =0.95$ in \eqref{CompSen1-2} and $(p,n,k) = (720i,2560i,80i)$ with $i\in \{2, 4, 6, 8, 10\}$. For each $i$, we generate 20 random instances as described above. The computational results averaged over the $20$ random instances are presented in Table~\ref{table2}. As before, we present the time for computing the QR decomposition of $A^T$ (denoted by $t_{\rm QR}$), the time for computing $\|A\|$ (denoted by $t_{\|A\|}$), the time for computing $x^0 = A^\dagger b$ given the QR factorization of $A^T$ (denoted by $t_{A^\dagger b}$), the CPU times of the algorithms (CPU time), the number of iterations (denoted by Iter), the recovery errors $\text{RecErr} := \frac{\|x^* - \xorig\|}{\max\{1, \|\xorig\|\}}$ and the residuals ${\rm Residual} := \frac{\|Ax^* - b\|_{LL_2,\gamma} - \sigma }{\sigma}$, where $x^*$ is the approximate solution returned by the respective algorithm.


\begin{table}[h]
{\color{black}
\caption{Computational results for problem \eqref{CompSen1-2}}\label{table2}
\begin{center}
{\footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|}\hline
\phantom{\diagbox{Date}{$i$}} & \multicolumn{1}{|c|}{Method} & \multicolumn{1}{|c|}{$i = 2$} & \multicolumn{1}{c|}{ $i = 4$ }
& \multicolumn{1}{c|}{ $i = 6$ } & \multicolumn{1}{|c|}{ $i = 8$ } & \multicolumn{1}{c|}{ $i = 10$ }
\\
\cline{1-7}\multirow{6}*{CPU time} & \multirow{1}*{$t_{\rm QR}$}
&  0.689 &  7.391 & 56.262 & 129.634 & 209.607      \\
\cline{2-2} \multirow{1}*{} & \multirow{1}*{$t_{A^\dagger b}$}
&  0.008 &  0.048 &  0.237 &  0.424 &  0.578     \\
\cline{2-2} \multirow{1}*{} & \multirow{1}*{$t_{\|A\|}$}
&  0.721 &  2.571 & 18.111 & 39.223 & 63.022     \\
\cline{2-7} \multirow{1}*{} & \multirow{1}*{SCP$_{\rm ls}$}
&  2.232 & 19.284 & 44.192 & 220.783 & 122.567       \\
\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM}
&  5.639 & 32.396 & 149.598 & 265.260 & 354.146        \\
\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm e}}$}
&  0.997 &  5.705 & 24.665 & 44.876 & 58.754     \\
\cline{1-7} \multirow{3}*{Iter} & \multirow{1}*{SCP$_{\rm ls}$}
&    207 &    332 &    196 &    596 &    266     \\
\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM}
&   1107 &   1122 &   1125 &   1151 &   1177     \\
\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm e}}$}
&    189 &    191 &    185 &    194 &    196     \\
\cline{1-7} \multirow{3}*{RecErr} & \multirow{1}*{SCP$_{\rm ls}$}
&  0.082 &  0.086 &  0.086 &  0.086 &  0.088  \\
\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM}
&  0.082 &  0.086 &  0.086 &  0.086 &  0.088  \\
\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm e}}$}
&  0.082 &  0.086 &  0.086 &  0.086 &  0.088  \\
\cline{1-7} \multirow{3}*{Residual} & \multirow{1}*{SCP$_{\rm ls}$}
& -5.53e-15 & -6.57e-15 & -6.76e-15 & -6.08e-15 & -7.00e-15       \\
\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM}
& -5.96e-14 & -2.21e-15 & -1.44e-14 & -4.53e-15 & -1.37e-15       \\
\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm e}}$}
& -2.73e-15 & 2.18e-15 & 1.35e-14 & 2.79e-15 & 1.31e-14   \\
\cline{1-7}
\end{tabular}
}
\end{center}
}
\end{table}

From Table~\ref{table2}, we observe a similar pattern as shown in Table~\ref{table1}, i.e., ESQM$_{{\rm e}}$ is the fastest algorithm and the recovery errors of all three algorithms are comparable.




\bmhead{Acknowledgments}
The second author is supported in part by the Hong Kong Research Grants Council PolyU153001/22p.





\begin{thebibliography}{99}
\bibitem{attouch10}
H. Attouch, J. Bolte, P. Redont and A. Soubeyran.
\newblock Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the Kurdyka-{\L}ojasiewicz inequality.
\newblock {\em Mathematics of Operations Research} 35, 438--457, 2010.


\bibitem{attouch13}
H. Attouch, J. Bolte and B. F. Svaiter.
\newblock Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward--backward splitting, and regularized Gauss--Seidel methods.
\newblock {\em Mathematical Programming} 137, 91--129, 2013.

\bibitem{Auslender13}
A. Auslender.
\newblock A very simple SQCQP method for a class of smooth convex constrained minimization problems with nice convergence results.
\newblock {\em Mathematical Programming} 142, pp. 349--369, 2013.

\bibitem{Ausleder13}
A. Auslender.
\newblock An extended sequential quadratically constrained quadratic programming algorithm for nonlinear, semidefinite, and second-order cone programming.
\newblock{\em Journal of Optimization Theory and Applications} 156, 183--212, 2013.

\bibitem{AusSheTeb10}
A. Auslender, B. Shefi and M. Teboulle.
\newblock A moving balls approximation method for a class of smooth constrained minimization problems.
\newblock {\em SIAM Journal on Optimization} 20, 3232--3259, 2010.


%\bibitem{Bauschke96}
%H. H. Bauschke and J. M. Borwein.
%\newblock On projection algorithms for solving convex feasibility problems.
%\newblock {\em SIAM Review} 38(3), 367--426, 1996.

\bibitem{BauschkeBorweinLi99}
H. H. Bauschke, J. M. Borwein and W. Li.
\newblock Strong conical hull intersection property, bounded linear regularity, Jameson's property (G), and error bounds in convex optimization.
\newblock {\em Math. Program.} 86, 135--160, 1999.


\bibitem{beck09}
A. Beck and M. Teboulle.
\newblock Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems.
\newblock {\em IEEE Transactions on Image Processing} 18, 2419--2434, 2009.

\bibitem{BeckerCandesGrant11}
S. R. Becker, E. J. Cand\`{e}s and M. C. Grant.
\newblock Templates for convex cone problems with applications to sparse signal recovery.
\newblock {\em Mathematical Programming Computation} 3, 165--218, 2011.


\bibitem{bolte07}
J. Bolte, A. Daniilidis and A. Lewis.
\newblock The {\L}ojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems.
\newblock {\em SIAM Journal on Optimization} 17, 1205--1223, 2007.

\bibitem{bolte07_2}
J. Bolte, A. Daniilidis, A. Lewis and M. Shiota.
\newblock Clark subgradients of stratifiable functions.
\newblock {\em SIAM Journal on Optimization} 18, 556--572, 2007.


\bibitem{bolte16}
J. Bolte and E. Pauwels.
\newblock Majorization-minimization procedures and convergence of SQP methods for semi-algebraic and tame programs.
\newblock {\em Mathematics of Operations Research} 41, 442--465, 2016.


\bibitem{bolte14}
J. Bolte, S. Sabach and M. Teboulle.
\newblock Proximal alternating linearized minimization for nonconvex and nonsmooth problems.
\newblock {\em Mathematical Programming} 146, 459--494, 2014.

%\bibitem{bolte17}
%J. Bolte, T.P. Nguyen, J. Peypouquet, B. W. Suter.
%\newblock From error bounds to the complexity of first-order descent methods for convex functions.
%\newblock {\em Mathematical Programming} 165, 471¨C507, 2017.

\bibitem{Brezinski00}
C. Brezinski.
\newblock Convergence acceleration during the 20th century.
\newblock {\em Journal of Computational and Applied Mathematics} 122, 1--21, 2000.

\bibitem{Ca18}
E. J. Cand\`{e}s.
\newblock The restricted isometry property and its implications for compressed sensing.
\newblock {\em  Comptes Rendus Mathematique} 346, 589--592, 2008.

\bibitem{CarrBarAys10}
R. E. Carrillo, K. E. Barner and T. C. Aysal.
\newblock Robust sampling and reconstruction methodsfor sparse signals in the presence of impulsive noise.
\newblock {\em IEEE Journal of Selected Topics in Signal Processing} 4, 392--408, 2010.

\bibitem{ChenLuPong16}
X. Chen, Z. Lu and T. K. Pong.
\newblock Penalty methods for a class of non-Lipschitz optimization problems.
\newblock {\em SIAM Journal on Optimization} 26, 1465--1492, 2016.

\bibitem{DonoghueCandes15}
B. O'Donoghue and E. J. Cand\`{e}s.
\newblock Adaptive restart for accelerated gradient schemes.
\newblock {\em Foundations of Computational Mathematics} 15, 715--732, 2015.

\bibitem{GillWong12}
P. E. Gill and E. Wong.
\newblock Sequential quadratic programming methods.
\newblock In: Lee, J., Leyffer, S. (eds.) {\em Mixed
Integer Nonlinear Programming}, 147--224. Springer, New York, 2012.

\bibitem{SOR00}
A. Hadjidimos.
\newblock Successive overrelaxation (SOR) and related methods.
\newblock {\em Journal of Computational and Applied Mathematics} 123, 177--199, 2000.

\bibitem{HoLS13}
J. D. W. Hosmer, S. Lemeshow and R. X. Sturdivant.
\newblock {\em  Applied Logistic Regression}.
\newblock John Wiley Sons, 3rd edition, 2013.

\bibitem{li18}
G. Li and T. K. Pong.
\newblock Calculus of the exponent of Kurdyka--{\L}ojasiewicz inequality and its applications to linear convergence of first-order methods.
\newblock {\em Foundations of Computational Mathematics} 18, 1199--1232, 2018.

\bibitem{LionsMercier79}
P. L. Lions and B. Mercier.
\newblock Splitting algorithms for the sum of two nonlinear operators.
\newblock {\em SIAM Journal on Numerical Analysis} 16, 964--979, 1979.

%\bibitem{liu19}
%T. X. Liu, T. K. Pong, and A. Takeda.
%\newblock A refined convergence analysis of pDCA$_e$ with applications to simultaneous sparse recovery and outlier detection.
%\newblock {\em Computational Optimization and Applications} 73, 69--100, 2019.

\bibitem{nesterov83}
Y. Nesterov.
\newblock A method for unconstrained convex minimization problem with the rate of convergence O ($\frac{1}{k^2}$).
\newblock {\em Doklady AN USSR} 269, 543--547, 1983.

\bibitem{Nesterov2004}
\newblock Y. Nesterov.
\newblock {\em Introductory Lectures on Convex Optimization: A Basic Course.}
\newblock Kluwer Academic Publishers, Boston, 2004.

\bibitem{Nesterov2005}
\newblock Y. Nesterov.
\newblock Smooth minimization of non-smooth functions.
\newblock {\em Mathematical Programming} 103, 127--152, 2005.

\bibitem{Nesterov2013}
\newblock Y. Nesterov.
\newblock Gradient methods for minimizing composite objective function.
\newblock {\em Mathematical Programming} 140, 125--161, 2013.

\bibitem{Polyak64}
B. T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock {\em USSR Computational Mathematics and Mathematical Physics} 4(5), 1--17, 1964.


%\bibitem{Tseng08}
%P. Tseng.
%\newblock On accelerated proximal gradient methods for convex-concave optimization.
%\newblock Preprint. Available at https://www.mit.edu/~dimitrib/PTseng/papers/apgm.pdf.
%
%\bibitem{Tseng10}
%P. Tseng.
%\newblock Approximation accuracy, gradient methods, and error bound for structured convex optimization.
%\newblock {\em Mathematical Programming} 125, 263-295, 2010.

\bibitem{Ro70}
R. T. Rockafellar.
\newblock {\em Convex Analysis.}
\newblock Princeton University Press, Princeton, 1970.

\bibitem{rock97a}
R. T. Rockafellar and R. J-B. Wets.
\newblock {\em Variational Analysis.}
\newblock Springer, 1997.

\bibitem{Rob75}
S. M. Robinson.
\newblock An application of error bounds for convex programming in a linear space.
\newblock {\em SIAM Journal on Control} 13, 271--273, 1975.

\bibitem{SmithFordSidi87}
D. A. Smith, W. F. Ford  and A. Sidi.
\newblock Extrapolation methods for vector sequences.
\newblock {\em SIAM Review} 29, 199--233, 1987.

%\bibitem{Tu98}
%H. Tuy.
%\newblock {\em Convex Analysis and Global Optimization}.
%\newblock Springer, 1998.

\bibitem{wen17}
B. Wen, X. Chen and T. K. Pong.
\newblock Linear convergence of proximal gradient algorithm with extrapolation for a class of nonconvex nonsmooth minimization problems.
\newblock {\em SIAM Journal on Optimization} 27, 124--145, 2017.

\bibitem{wen18}
B. Wen, X. Chen, and T. K. Pong.
\newblock A proximal difference-of-convex algorithm with extrapolation.
\newblock {\em Computational Optimization and Applications} 69, 297--324, 2018.

\bibitem{YinLouHeXin15}
P. Yin, Y. Lou, Q. He and J. Xin.
\newblock Minimization of $\ell_{1-2}$ for compressed sensing.
\newblock {\em SIAM Journal on Scientific Computing} 37, A536--A563, 2015.


\bibitem{yu21}
P. Yu, T. K. Pong and Z. Lu.
\newblock Convergence rate analysis of a sequential convex programming method with line search for a class of constrained difference-of-convex optimization problems.
\newblock {\em SIAM Journal on Optimization} 31, 2024--2054, 2021.

\bibitem{zhang23}
Y. Zhang, G. Li, T. K. Pong and S. Xu.
\newblock Retraction-based first-order feasible methods for difference-of-convex programs with smooth inequality and simple geometric constraints.
\newblock {\em Advances in Computational Mathematics} 49, Article number: 8, 2023.

%\bibitem{zo04}
%G. Zou.
%\newblock A modified Poisson regression approach to prospective studies with binary data.
%\newblock {\em American Journal of Epidemiology} 159, 702--706, 2004.




\end{thebibliography}


\end{document}
