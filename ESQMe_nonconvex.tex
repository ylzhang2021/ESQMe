%Version 2.1 April 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove “Numbered?in the optional parenthesis.
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst, sn-mathphys.bst. %

%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-mathphys,Numbered]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[default]{sn-jnl}% Default
%%\documentclass[default,iicol]{sn-jnl}% Default with double column layout

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{multirow,diagbox,multicol,booktabs}
\usepackage{graphicx}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
%\usepackage{refcheck}%
\usepackage{algorithm,algorithmic}%

\usepackage{latexsym,enumerate,verbatim,amsfonts,microtype}
\usepackage{color} % added by TK
\usepackage{graphicx}
\usepackage[active]{srcltx}
\usepackage{cases}
\usepackage[colorinlistoftodos,prependcaption,textsize=footnotesize]{todonotes}
\usepackage[framemethod=tikz]{mdframed}% added by TK. Create nice boxes
\mdfsetup{%
	skipbelow=4pt,
	skipabove=8pt,
	linewidth=1.25pt,
	backgroundcolor=gray!10,
	userdefinedwidth=\textwidth,
	roundcorner=10pt,
}

\allowdisplaybreaks[2]
\numberwithin{equation}{section}

% def by Ting Kei Pong
\def\cS{{\mathcal{S}}}
\def\cU{{\mathcal{U}}}
\def\cF{{\mathcal{F}}}
\def\cD{{\mathcal{D}}}
\def\cL{{\mathcal{L}}}
\def\R{{\mathbb{R}}}
\def\tr{{\rm tr}}
\def\argmin{\mathop{\rm arg\,min}}
\def\Argmin{\mathop{\rm Arg\,min}}
\def\argmax{\mathop{\rm arg\,max}}
\def\Diag{{\rm Diag}}
\def\tx{{\widetilde x}}
\def\hx{{\widehat x}}
\def\conv{{\rm conv}\,}

\def\prox{{\rm Prox}}
\def\xfeas{x^\odot}
\def\betamin{{\rm \beta_{min}}}
\def\sigmamin{{\rm\tilde{\sigma}_{min}}}
\def\diag{{\rm diag}}
\def\d{{\rm dist}}
\def\dom{{\rm dom}\,}
\def\xorig{{x_{\rm orig}}}
%\def\xfeasss{x^\circledcirc}
%\def\xfeasss{x^\circledast}

\newcommand{\ling}[3]{{\rm lin}_{g_{#1}}(#2,#3)}

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published
%%%%  by Springer Nature. The guidance has been prepared in partnership with
%%%%  production teams to conform to Springer Nature technical requirements.
%%%%  Editorial and presentation requirements differ among journal portfolios and
%%%%  research disciplines. You may find sections in this template are irrelevant
%%%%  to your work and are empowered to omit any such section if allowed by the
%%%%  journal you intend to submit to. The submission guidelines and policies
%%%%  of the journal take precedence. A detailed User Manual is available in the
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%\jyear{2021}%

%% as per the requirement new theorem styles can be included as shown below
%\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}[section]%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
%\newtheorem{proposition}[theorem]{Proposition}%
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.
\newtheorem{definition}{Definition}[section]%
\newtheorem{lemma}{Lemma}[section]
%\newtheorem{definition}{Definition}[section]
\newtheorem{fact}{Fact}[section]
%\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
%\newtheorem{remark}{Remark}[section]
%\newtheorem{example}{Example}[section]
\newtheorem{assumption}{Assumption}[section]

%\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}[section]%
\newtheorem{remark}{Remark}[section]%

%\theoremstyle{thmstylethree}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[ESQM]{An extended sequential quadratic method with extrapolation}

%%=============================================================%%
%% Prefix	-> \pfx{Dr}
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% NatureName	-> \tanm{Poet Laureate} -> Title after name
%% Degrees	-> \dgr{MSc, PhD}
%% \author*[1,2]{\pfx{Dr} \fnm{Joergen W.} \spfx{van der} \sur{Ploeg} \sfx{IV} \tanm{Poet Laureate}
%%                 \dgr{MSc, PhD}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1]{\fnm{Yongle} \sur{Zhang}}\email{xxx.com}

\author[2]{\fnm{Ting Kei} \sur{Pong}}\email{tk.pong@polyu.edu.hk}
%\equalcont{These authors contributed equally to this work.}

\author[3]{\fnm{Siqi} \sur{Xu}}\email{xxx.com}
%\equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{100190}, \state{State}, \country{Country}}}

\affil[2]{\orgdiv{Department of Applied Mathematics}, \orgname{the Hong Kong Polytechnic University}, \orgaddress{\city{Hong Kong}, \country{People's Republic of China}}}

\affil[3]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{10587}, \state{State}, \country{Country}}}

%%==================================%%
%% sample for unstructured abstract %%
%%==================================%%

\abstract{The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. Authors are advised to check the author instructions for the journal they are submitting to for word limits and if structural elements like subheadings, citations, or equations are permitted.}

\keywords{keyword1, Keyword2, Keyword3, Keyword4}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle



\section{Introduction}

 In this paper, we consider the following difference-of-convex (DC) optimization problem with smooth inequality and simple geometric constraints:
\begin{align}\label{eq1}
\min_{x\in \mathbb {R}^n }\quad& P(x):= P_1(x) - P_2(x) \notag \\
\text{s.t.}\quad & g_i(x)\leq 0,~~ i = 1,\ldots, m,\\
& x\in C,  \notag
\end{align}
where $P_1:\mathbb{R}^n \to \R$ and $P_2:\mathbb{R}^n\to \R$ are convex, each $g_i:\mathbb{R}^n\to \R$ is smooth and $\nabla g_i$ is Lipschitz with continuity modulus $L_{g_i}>0$. $C\subseteq \mathbb {R}^n$ is a nonempty compact convex set, the feasible set $\mathcal{G} := C\cap \mathscr{F}$ is nonempty with $\mathscr{F}:=\{x\in \mathbb{R}^n : g_i(x)\leq 0, ~i = 1,\dots,m\}$.

\section{Notation and preliminaries}\label{sec2}

In this paper, we let $\mathbb{R}$, $\mathbb{R}_+$ and $\mathbb{Z}$ denote the sets of real numbers, nonnegative real numbers and integers respectively. We also let $\mathbb{R}^n$ and $\mathbb{R}_+^n$ denote the $n$-dimensional Euclidean space and its nonnegative orthant. For an $x\in \mathbb{R}^n$, we let $\|x\|$ denote its Euclidean norm; moreover, for $x$ and $y\in \mathbb{R}^n$, we let $\langle x,y \rangle$ denote their inner product.

For an extended-real-valued function $f:\mathbb{R}^n \to (-\infty,+\infty]$, we say that $f$ is proper if $\dom f := \{ x: f(x)<\infty \}\neq \emptyset$. A proper function $f$ is said to be closed if it is lower semicontinuous. We use $x^k\overset{f}{\to} x$ to denote $x^k\to x$ and $f(x^k)\to f(x)$. For a proper closed function $f$, the regular subdifferential of $f$ at $w\in \dom f$ is given by
$$
\widehat{\partial} f(w):= \left\{ \xi\in\mathbb{R}^n :\liminf_{v\to w,v\neq w} \frac{f(v) - f(w) - \langle \xi, v - w \rangle }{\| v - w\|}\geq 0 \right\}.
$$
The (limiting) subdifferential of $f$ at $w\in \dom f$ is given by
\begin{equation*}
\partial f(w):= \left\{ \xi\in\mathbb{R}^n:\exists w^{k}\overset{f}{\to} w, \xi^{k}\to\xi \text{ with } \xi^{k} \in \widehat{\partial} f(w^{k}) \text{ for each } k\right\},
\end{equation*}
and we set $\partial f(x)=\widehat{\partial}f(x) = \emptyset$ when $ x\notin \dom f$ by convention. We also define $\dom \partial f := \{x:\partial f(x)\neq \emptyset\}$. The above subdifferential of $f$ is consistent with the classical subdifferential of $f$ when $f$ is in addition convex; indeed, in this case, we have
$$
\partial f(w)=\left\{ \xi\in\mathbb{R}^n:\langle \xi, v - w \rangle\leq f(v) - f(w) ~~ \forall v\in \mathbb{R}^n \right\};
$$
see, for example, \cite[proposition 8.12]{rock97a} and \cite[proposition 8.8]{rock97a}.
For a nonempty closed set $C\subseteq \mathbb{R}^n$, the indicator function $\delta_C$ is defined by
\begin{equation*}
\delta_C(x) = \left\{
\begin{array}{lr}
 0~~ &x\in C,
 \\  \infty ~~ &x\notin C.
 \end{array}
 \right.
\end{equation*}
The normal cone of $C$ at $x\in C$ is defined by
$$ \mathcal{N}_C(x) :=\partial \delta_C(x)=\{\xi \in \mathbb{R}^n:\langle \xi, y - x \rangle \leq 0 ~~\forall y\in C\}.$$
Finally, the distance from a point $x$ to $C$ is denoted by $\d(x, C)$, and the convex hull of $C$ is denoted by $\conv C$.

We next recall some important definitions that will be used in the sequel. We start by recalling the following constraint qualification for \eqref{eq1} (which was first introduced in \cite{Ausleder13}), and the (associated) first-order optimality conditions for \eqref{eq1}.
\begin{definition}[{{\bf RCQ}}]\label{RCQ}
We say that the Robinson constraint qualification holds at $x\in\R^n$ for \eqref{eq1} if the following statement holds:
$$ RCQ(x):~\exists y\in C \text{ such that } g_i(x) + \langle\nabla g_i(x), y-x\rangle < 0~~ \forall i = 1,2,\cdots m.$$
\end{definition}

\begin{definition}[{{\bf Critical point}}]\label{Stationary}
For \eqref{eq1}, we say that $x$ is a critical point of \eqref{eq1} if $x\in C$ and there exists $\lambda=(\lambda_1, \lambda_2, \dots, \lambda_m)\in \mathbb{R}_+^m$ such that $(x, \lambda)$ satisfies the following conditions:
\begin{enumerate}[{\rm (i)}]
    \item $ g_i(x)\leq 0 ~~\forall i=1,\dots,m,$
    \item $ \lambda_i g_i(x)=0 ~~\forall i=1,\dots,m,$
    \item $0\in\partial P_1(x) - \partial P_2(x) + \sum_{i=1}^{m}\lambda_i\nabla g_i(x) + \mathcal{N}_C(x).$
\end{enumerate}	
\end{definition}

One can show using similar arguments as in \cite[Section 2]{yu21} that if the RCQ holds at every point in $\mathcal{G}$, then any local minimizer of \eqref{eq1} is a critical point of \eqref{eq1}.

Next, we recall the definitions of Kurdyka-{\L}ojasiewicz (KL) property and Kurdyka-{\L}ojasiewicz (KL) exponent.
\begin{definition}[{{\bf Kurdyka-{\L}ojasiewicz (KL) property and exponent}}]\label{KLd}
A proper closed function $f$ is said to satisfy the KL property at $\bar{x}\in \dom\partial f$ if there exist $r\in (0,\infty]$, a neighborhood $U$ of $\bar{x}$, and a continuous concave function $\phi:[0,r)\to \mathbb{R}_+$ satisfying $\phi(0)=0$ such that:
\begin{enumerate}[{\rm (i)}]
    \item $\phi$ is continuously differentiable on $(0,r)$ with $\phi'>0$;
    \item for all $x\in U$, $f(\bar{x})< f(x) < f(\bar{x}) + r$, it holds that
        \begin{align}\label{eq21}
        \phi'(f(x) - f(\bar{x}))\d(0,\partial f(x))\geq 1.
        \end{align}
\end{enumerate}
If $f$ satisfies the KL property at $\bar{x}\in \dom\partial f$, and $\phi$ in \eqref{eq21} can be chosen as $\phi(\varsigma)= \rho \varsigma^{1-\alpha}$ for some $\rho>0$ and $\alpha\in[0,1)$, then we say that $f$ satisfies the KL property with exponent $\alpha$ at $\bar{x}$.

A proper closed function $f$ satisfying the KL property at every point in $\dom\partial f$ is called a KL function. A proper closed function $f$ satisfying the KL property with exponent $\alpha\in[0,1)$ at every point in $\dom\partial f$ is called a KL function with exponent $\alpha$.
\end{definition}

Many functions are known to satisfy the KL property. For instance, proper closed functions satisfy the KL property with some exponent $\alpha$; see \cite{bolte07}. The KL property plays an important role in the convergence rate analysis of first order methods and the exponent is important in establishing convergence rates; see, for example, \cite{bolte14,attouch13,beck09,bolte07,attouch10,li18}.

Finally, before ending this section, we recall two technical lemmas. The first lemma concerns the uniformized KL property and was established in \cite[Lemma~3.10]{yu21}. The second lemma is a special case of Robinson \cite{Rob75} concerning error bounds for convex functions, which will be used in Section~\ref{sec5} for studying the KL property of a penalty function associated with \eqref{eq1}.
\begin{lemma}\label{KLinequ}
Let $f:\R^n\rightarrow (-\infty,+\infty]$ be a level-bounded proper closed convex function with $\Lambda:= \Argmin f\not=\emptyset$. Let $\underline{f}:=\inf f$. Suppose that $f$ satisfies the KL property at each point in $\Lambda$ with exponent $\alpha\in[0,1)$. Then there exist $\epsilon >0$, $r_0>0$ and $c_0>0$ such that
\[
\d(x, \Lambda)\leq c_0(f(x) - \underline{f})^{1-\alpha}
\]
for any $x\in\dom \partial f$ satisfying $\d(x, \Lambda)\leq \epsilon$ and $\underline{f}\leq f(x) \leq \underline{f} + r_0$.
\end{lemma}

\begin{lemma}\label{RobEB}
Let $g:\R^n\to \R^m$ with each component function $g_i$ being convex. Let $\Omega := \{x\in \R^n:\; 0 \in g(x) + \R^m_+\}$ and suppose there exist $x^s\in \Omega$ and $\delta_0 > 0$ such that $B(0,\delta_0)\subseteq g(x^s) + \R^m_+$.
Then
\[
\d(x,\Omega)\leq \frac{\|x - x^s\|}{\delta_0}\d(0, g(x) + \R^m_+)~~~~~  \forall x\in \mathbb{X}.
\]
\end{lemma}

\section{Algorithmic framework}\label{sec3}

In this section, we present our algorithm for solving \eqref{eq1}. To describe our algorithm, following the discussion in \cite[Section~3]{wen17}, for each $i$, notice that we can rewrite $g_i$ (whose gradient is Lipschitz continuous) as $g_i = g_i^1 - g_i^2$, where $g_i^1$ and $g_i^2$ are two convex functions with Lipschitz continuous gradients. The next remark concerns the Lipschitz continuity moduli of $\nabla g_i^1$ and $\nabla g_i^2$.

\begin{remark}\label{Remarkg}
Here and throughout, we denote a Lipschitz continuity modulus of $\nabla g_i^1$ by $L_{g_i} > 0$ and a Lipschitz continuity modulus of $\nabla g_i^2$ by $\ell_{g_i} > 0$. In addition, by taking a larger $L_{g_i}$ if necessary, we will assume without loss of generality that $L_{g_i} \geq \ell_{g_i}$. Then one can show that $\nabla g_i$ is Lipschitz continuous with a modulus $L_{g_i}$. We also let $L_g := \max\{L_{g_i}: i=1,\dots,m\}$ and $\ell_g = \max\{\ell_{g_i}: i=1,\dots,m\}$ for brevity.
\end{remark}

The algorithm we study in this paper is presented as Algorithm \ref{alg:Framwork} below; here and throughout, for notational simplicity, for each $u$, $w\in \R^n$, we define
\begin{equation}\label{ling}
\ling{0}{u}{v}\equiv 0 \ \ {\rm and}\ \ \ling{i}{u}{w} := g_i(w) + \langle\nabla g_i(w), u - w\rangle \ \ \ \forall i = 1,\ldots,m.
\end{equation}
We identify our algorithm as an extended sequential quadratic method with extrapolation (ESQM$_{\text{e}}$), where ``extrapolation" refers to the step \eqref{defyk}. This is because when $\beta_k \equiv 0$, our algorithm reduces to an instance of the ESQM proposed in \cite{Ausleder13}, whose convergence was established for solving \eqref{eq1} when the $P$ in \eqref{eq1} is in addition smooth with Lipschitz gradient.\footnote{More precisely, when the $P$ in \eqref{eq1} is smooth with Lipschitz gradient (say, with Lipschitz constant $L_P$) and $\beta_k \equiv 0$, our algorithm applied to \eqref{eq1} with $P_1(x) := \frac{L_P}2\|x\|^2$ and $P_2(x) := \frac{L_P}2\|x\|^2 - P(x)$ becomes an instance of the ESQM in \cite{Ausleder13}.} Notice that the subproblem in \eqref{eq2} has a unique solution as an optimization problem with a strongly convex objective and a nonempty feasible set. While it requires an iterative solver in general, we refer the readers to \cite[Appendix~A]{zhang23} for an efficient routine for solving \eqref{eq2} when $m = 1$.
\begin{algorithm}
\caption{ESQM$_{\text{e}}$ for solving \eqref{eq1}}\label{alg:Framwork}
\begin{algorithmic}
\STATE
\begin{description}
  \item[\bf Step 0.] Choose $x^{-1}=x^0\in C$, $\{\beta_k\}\subseteq\left[0,\sqrt{\frac{L_g}{L_g + \ell_g}}~\right]$, $\theta_0>0$, $d>0$, where $L_g = \max\{L_{g_i}, ~ i=1,\dots,m\}$ and $\ell_g = \max\{\ell_{g_i}, ~ i=1,\dots,m\}$.
  \item[\bf Step 1.] Set
    \begin{equation}\label{defyk}
      y^k = x^k + \beta_k(x^k - x^{k-1}).
    \end{equation}
  \item[\bf Step 2.] Take any $\xi^k\in \partial P_2(x^{k})$ and compute
    \begin{align}\label{eq2}
    (x^{k+1},s^{k+1})\in \Argmin\limits_{(x,s)\in \mathbb{R}^{n+1}}\quad &P_1(x) - \langle \xi^k, x \rangle
	+ \theta_k s + \frac{\theta_k L_g}{2}\| x - y^k \|^2   \notag
	\\ \text{s.t.} \quad &\ling{i}{x}{y^k} \leq s ~~\forall i=1,\dots,m,
	\\& (x,s)\in C\times \mathbb{R_+},  \notag
	\end{align}
where ${\rm lin}_{g_i}$ is defined in \eqref{ling}.
  \item[\bf Step 3.]  If $\ling{i}{x^{k+1}}{y^k}\leq 0$ for all $i$, then $\theta_{k+1}=\theta_k$; otherwise $\theta_{k+1}=\theta_k+d$. Update $k\leftarrow k+1$ and go to step 1.
\end{description}
\end{algorithmic}
\end{algorithm}

The convergence properties of our algorithm will be studied in Section~\ref{sec4}, and we end this section by presenting some useful facts concerning the subproblem \eqref{eq2}. The first two items are trivial observations, and they are stated here for easy reference later.
\begin{lemma}\label{subproremarks}
Suppose that $x^{k-1}, x^k\in C$ are generated at the beginning of the $k$-th iteration of Algorithm \ref{alg:Framwork} for some $k\geq 0$. Then the following statements hold:
\begin{enumerate}[{\rm (i)}]
    \item $s^{k+1} = \max_{i = 1,\cdots,m}[\ling{i}{x^{k+1}}{y^k}]_+$.
    \item Problem \eqref{eq2} has a unique solution.
    \item Let $g_0\equiv 0$. Then $x^{k+1}$ is a component of the minimizer of \eqref{eq2} if and only if there exist $\lambda_i^k \geq 0$ for all $i\in I_k(x^{k+1})$ such that $\sum_{i\in I_k(x^{k+1})}\lambda_i^k = 1$ and
        \begin{equation*}%\label{KKT2}
        0\in \partial P_1(x^{k+1}) - \xi^k + \theta_k\sum_{i\in I_k(x^{k+1})} \lambda_i^k\nabla g_i(y^k) + \theta_kL_g(x^{k+1} - y^k) + \mathcal{N}_C(x^{k+1}),
        \end{equation*}
        where
        \begin{equation}\label{defiIk}
        I_k(x): = \left\{s\in\{0,1,\cdots,m\}: \ling{s}{x}{y^k} = \max_{i = 0, 1,\cdots,m}\ling{i}{x}{y^k}\right\}.
        \end{equation}
\end{enumerate}
\end{lemma}

\begin{proof}
The proofs of (i) and (ii) are standard and we omit the proofs for brevity.

We now prove (iii). Combining $g_0\equiv 0$ with (i), we see that the $x^{k+1}$ in \eqref{eq2} satisfies
\begin{equation}\label{subproblem2}
x^{k+1} = \argmin\limits_{x\in C}~ P_1(x) - \langle \xi^k, x \rangle + \theta_k \max_{i = 0, 1,\cdots,m}\{\ling{i}{x}{y^k}\} + \frac{\theta_k L_g}{2}\| x - y^k \|^2.
\end{equation}
Then, from \cite[Theorem 23.8]{Ro70}, we have that $x^{k+1}$ is a minimizer of the convex problem \eqref{subproblem2} if and only if
\begin{align*}
0&\!\in\! \partial P_1(x^{k+1}) \!-\! \xi^k \!+\! \theta_k\partial\!\left(\max_{i = 0, 1,\cdots,m}\{\ling{i}{\cdot}{y^k}\}\right)\!(x^{k+1}) \!+\! \theta_kL_g(x^{k+1} - y^k) \!+\! \mathcal{N}_C(x^{k+1})\\
& \!\!\overset{\rm(a)}=\! \partial P_1(x^{k+1}) - \xi^k + \theta_k\conv\{\nabla g_i(y^k): i\in I_k(x^{k+1})\} + \theta_kL_g(x^{k+1} - y^k) + \mathcal{N}_C(x^{k+1}),
\end{align*}
where (a) follows from \cite[Exercise~8.31]{rock97a} with $I_k(\cdot)$ defined in \eqref{defiIk}.
%
%That is, for each $k\geq 0$, there exist $\lambda_i^k \geq 0$ for all $i\in I_k(x^{k+1})$ such that $\sum_{i\in I_k(x^{k+1})}\lambda_i^k = 1$ and
%\begin{equation*}
%0\in \partial P_1(x^{k+1}) - \xi^k + \theta_k\sum_{i\in I_k(x^{k+1})} \lambda_i^k\nabla g_i(y^k) + \theta_kL_g(x^{k+1} - y^k) + \mathcal{N}_C(x^{k+1}).
%\end{equation*}
%This completes the proof.
\end{proof}


\section{Convergence properties}\label{sec4}

\subsection{Convergence analysis for ESQM$_{\rm e}$}
We first show that the successive changes of the $\{x^k\}$ generated by ESQM$_{\rm e}$ vanish under suitable assumptions on $\beta_k$.
\begin{theorem}[Vanishing successive changes]\label{suffdec}
Consider \eqref{eq1} and let $\{(x^k,y^k,s^k,\theta_k)\}$ be generated by Algorithm \ref{alg:Framwork}. Then the following statements hold:
\begin{enumerate}[{\rm (i)}]
    \item The sequence $\{x^k\}$ belongs to $C$ and is bounded.
    \item Let $\bar{m} = \inf\{P(x):x\in C\}$. Then for any $k\geq 1$,
        $$
        Q(x^{k+1},x^{k},y^{k},\theta_{k+1}) \leq Q(x^k,x^{k-1},y^{k-1},\theta_k) - \left(1 - \frac{L_g + \ell_g}{L_g}\beta_k^2\right)\frac{L_g}{2}\| x^{k} - x^{k-1}\|^2,
        $$
    where
    \[
    Q(x,y,z,\theta):=\frac{P(x) - \bar{m}}{\theta} + \max_{i = 1,\cdots,m}\left[\ling{i}{x}{z}\right]_++ \frac{L_g}{2}\| x - y \|^2 + \frac{L_g}{2}\| x - z \|^2.
    \]
    \item It holds that $\sum_{k = 1}^{\infty} \frac{L_g - (L_g + \ell_g)\beta_k^2}{2} \| x^k - x^{k-1}\|^2 < \infty$. Moreover, if $\bar{\beta}: = \sup_k\beta_k < \sqrt{\frac{L_g}{L_g + \ell_g}}$, then $\lim_{k \rightarrow\infty}\|x^k - x^{k-1}\| = 0$ and  $\lim_{k \rightarrow\infty}\|x^k - y^k\| = 0$.
\end{enumerate}
\end{theorem}

\begin{proof}
(i): Note that $\{x^k\}\subseteq C$ according to \eqref{eq2}. Since $C$ is compact, $\{x^k\}$ is bounded.

(ii): Note that the objective in the subproblem \eqref{eq2} is strongly convex. Using this, and noting the definition of $(x^{k+1},s^{k+1})$ in \eqref{eq2} as a minimizer and the fact that $(x^k,\max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^k}\right]_+)$ is feasible for \eqref{eq2}, we have for any $k\ge 0$ that
\begin{equation}
\begin{split}\label{eq6}
& P_1(x^{k+1}) \!-\! \langle \xi^k,x^{k+1}-x^k\rangle + \theta_k s^{k+1} + \frac{\theta_k L_g}{2}\| x^{k+1} - y^k  \|^2 \\
&\leq P_1(x^{k}) \!+\! \theta_k \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^k}\right]_+ \!+\! \frac{\theta_k L_g}{2}\| x^{k}-y^k \|^2 \!-\! \frac{\theta_k L_g}{2}\| x^{k+1}-x^k \|^2.
\end{split}
\end{equation}
Meanwhile, from Remark~\ref{Remarkg} and the definition of ${\rm lin}_{g_i}$ in \eqref{ling}, we see that whenever $k \ge 1$,
\begin{align}\label{gnonconvex}
&\!\! \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^k}\right]_+ \notag\\
&\!\!= \max_{i = 1,\cdots,m}\left[g^1_i(y^k) + \langle\nabla g^1_i(y^k), x^k - y^k \rangle - g^2_i(y^k) - \langle\nabla g^2_i(y^k), x^k - y^k \rangle\right]_+ \notag\\
&\!\!\overset{\rm(a)}{\leq}\!\! \max_{i = 1,\cdots,m}\left[g^1_i(x^{k}) \!-\! g^2_i(x^{k}) \!+\! \frac{\ell_{g_i}}{2}\|x^k \!-\! y^{k}\|^2\right]_+ \!\!\!\!=\!\!\! \max_{i = 1,\cdots,m}\left[g_i(x^{k}) \!+\! \frac{\ell_{g_i}}{2}\|x^k \!-\! y^{k}\|^2\right]_+ \\
&\!\! \overset{\rm(b)}{\leq} \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^{k-1}}+ \frac{L_{g_i}}{2}\|x^k - y^{k-1}\|^2 + \frac{\ell_{g_i}}{2}\|x^k - y^{k}\|^2\right]_+ \notag\\
&\!\!\overset{\rm(c)}{\leq} \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^{k-1}}\right]_+ + \frac{L_g}{2}\|x^k - y^{k-1}\|^2 + \frac{\ell_{g}}{2}\|x^k - y^{k}\|^2, \notag
\end{align}
where (a) holds because of the convexity of $g^1_i$ and the Lipschitz continuity of $\nabla g^2_i$, (b) follows from the Lipschitz continuity of $\nabla g_i$, and (c) holds because $L_g=\max\{L_{g_i}:\;i=1,\dots,m\}$ and $\ell_g=\max\{\ell_{g_i}:\;i=1,\dots,m\}$.
Then, we obtain that when $k \ge 1$,
\begin{align*}
&P(x^{k+1}) = P_1(x^{k+1}) - P_2(x^{k+1})\overset{\rm(a)}{\leq} P_1(x^{k+1}) - \langle\xi^k, x^{k+1} - x^k\rangle - P_2(x^{k})\\
& = P_1(x^{k+1}) - \langle \xi^k, x^{k+1} - x^k\rangle + \frac{\theta_k L_g}{2}\| x^{k+1} - y^k \|^2 - \frac{\theta_k L_g}{2}\| x^{k+1} - y^k\|^2 - P_2(x^{k})\\
&\overset{\rm(b)}{\leq} P_1(x^{k}) + \frac{\theta_k L_g}{2}\| x^{k}-y^k \|^2 - \theta_k s^{k+1} + \theta_k \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^{k}}\right]_+ \\
&~~~~~~ - \frac{\theta_k L_g}{2}\| x^{k+1} - x^k\|^2 - \frac{\theta_k L_g}{2}\| x^{k+1} - y^k\|^2 - P_2(x^{k}) \\
&\overset{\rm(c)}{\leq} P(x^{k}) + \theta_k \left(\max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^{k-1}}\right]_+
 + \frac{L_g}{2}\| x^k - y^{k-1}\|^2 + \frac{\ell_{g}}{2}\|x^k - y^{k}\|^2\right)\\
  & ~~~~~  + \frac{\theta_k L_g}{2}\| x^{k} - y^k\|^2 - \theta_k s^{k+1} - \frac{\theta_k L_g}{2}\| x^{k+1} - x^k\|^2 - \frac{\theta_k L_g}{2}\| x^{k+1} - y^k\|^2,
\end{align*}
where (a) holds because $P_2$ is convex and $\xi^k\in \partial P_2(x^k)$, (b) holds thanks to \eqref{eq6}, and (c) holds because of \eqref{gnonconvex}.

Rearranging terms in the above display and noting that $y^k - x^k = \beta_k(x^k - x^{k - 1})$ for $k \ge 0$ (thanks to the definition of $y^k$ in \eqref{defyk}), we have that for $k\ge 1$,
\begin{align}\label{eq8}
&P(x^{k+1}) + \theta_k s^{k+1} + \frac{\theta_k L_g}{2}\| x^{k+1} - x^k\|^2 + \frac{\theta_k L_g}{2}\| x^{k+1} - y^k\|^2 \notag\\
&\leq P(x^{k}) + \theta_k\max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^{k-1}}\right]_+ + \frac{\theta_k L_g}{2}\| x^k - y^{k-1}\|^2 \\
&~~~~ + \frac{\theta_k (L_g + \ell_g)}{2}\beta_k^2\| x^{k} - x^{k-1}\|^2 \notag\\
&= P(x^{k}) + \theta_k\max_{i = 1,\cdots,m}[\ling{i}{x^{k}}{y^{k-1}}]_+ + \frac{\theta_k L_g}{2}\| x^{k} - x^{k-1}\|^2 \notag\\
&~~~~~~ + \frac{\theta_k L_g}{2}\| x^k - y^{k-1}\|^2 - \left(1 - \frac{L_g + \ell_g}{L_g}\beta_k^2\right)\frac{\theta_k L_g}{2}\| x^{k} - x^{k-1}\|^2. \notag
\end{align}

Since $P$ is continuous and $C$ is a compact set, we see that $\bar{m} = \inf\{P(x):x\in C\} > -\infty$. Then we can deduce from the definition of $Q$ and the observation $s^{k+1} = \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k+1}}{y^{k}}\right]_+$ (thanks to Lemma~\ref{subproremarks}(i)) that whenever $k\ge 1$,
\begin{align}\label{eq81}
&Q(x^{k+1},x^{k},y^{k},\theta_{k+1}) \notag\\
&= \frac{P(x^{k+1}) - \bar{m}}{\theta_{k+1}} + s^{k+1} + \frac{L_g}{2}\| x^{k+1} - x^{k} \|^2 + \frac{L_g}{2}\| x^{k+1} - y^{k} \|^2 \notag\\
&\overset{\rm(a)}{\leq} \frac{P(x^{k+1}) - \bar{m}}{\theta_k} + s^{k+1} + \frac{L_g}{2}\| x^{k+1} - x^{k} \|^2 + \frac{L_g}{2}\| x^{k+1} - y^{k} \|^2 \notag\\
&\overset{\rm(b)}{\leq} \frac{1}{\theta_k}\bigg[ P(x^{k}) - \bar{m} + \theta_k \max_{i = 1,\cdots,m}[\ling{i}{x^{k}}{y^{k-1}}]_+ + \frac{\theta_k L_g}{2}\| x^{k} - x^{k-1}\|^2 \notag\\
&~~~~ + \frac{\theta_k L_g}{2}\|x^k - y^{k-1}\|^2- \left(1 - \frac{L_g + \ell_g}{L_g}\beta_k^2\right)\frac{\theta_k L_g}{2}\| x^{k} - x^{k-1}\|^2\bigg] \notag\\
&\overset{\rm(c)}{=} \frac{1}{\theta_k}\left( P(x^{k}) -\bar{m}\right) + \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k}}{y^{k-1}}\right]_+ \notag\\
&~~~~ + \frac{L_g}{2}\| x^{k} - x^{k-1}\|^2 + \frac{L_g}{2}\|x^k - y^{k-1}\|^2- \left(1 - \frac{L_g + \ell_g}{L_g}\beta_k^2\right)\frac{L_g}{2}\| x^{k} - x^{k-1}\|^2 \notag\\
& = Q(x^k,x^{k-1},y^{k-1},\theta_k) - \left(1 - \frac{L_g + \ell_g}{L_g}\beta_k^2\right)\frac{L_g}{2}\| x^{k} - x^{k-1}\|^2,
\end{align}
where (a) holds because the definition of $\bar{m}$ and the fact that $\{\theta_k^{-1}\}$ is nonincreasing, (b) follows from \eqref{eq8} and the fact that $\frac{1}{\theta_k} > 0$, and (c) holds because $x^{k}\in C$.

(iii): Observe that, for any $k\geq 0$,
\begin{equation*}
\begin{split}
&Q(x^{k+1},x^{k},y^k,\theta_{k+1})  \\
&= \frac{P(x^{k+1}) - \bar{m}}{\theta_{k+1}} \!+\! \max_{i = 1,\cdots,m}[\ling{i}{x^{k+1}}{y^k}]_+ \!+\! \frac{L_g}{2}\| x^{k+1} - x^{k} \|^2 \!+\! \frac{L_g}{2}\| x^{k+1}-y^{k} \|^2 \!\geq\! 0.
\end{split}
\end{equation*}
Combining the above display with \eqref{eq81}, we have
\begin{align*}
&\sum_{k=1}^{\infty}\left(1 - \frac{L_g + \ell_g}{L_g}\beta_k^2\right)\frac{L_g}{2}\| x^{k} - x^{k-1} \|^2 \\
&\leq Q(x^1,x^{0},y^{0},\theta_1) - \liminf_{k\to \infty} Q(x^{k+1},x^{k},y^k,\theta_{k+1})\leq Q(x^1,x^{0},y^{0},\theta_1) <\infty .
\end{align*}
Finally, if $\bar{\beta}: = \sup_k\beta_k < \sqrt{\frac{L_g}{L_g + \ell_g}}$, then we deduce from the above display that
$$\lim_{k\to \infty}\| x^k - x^{k-1}\|=0.$$
Combining this with the definition of $y^k$ in \eqref{defyk}, we can obtain further that
$
\lim_{k\to \infty}\| y^k - x^k\| = \lim_{k\to \infty}\beta_k\| x^k - x^{k-1}\| =0$.
\end{proof}

Next, we recall the following assumption concerning the RCQ in Definition~\ref{RCQ}. This assumption was first introduced in \cite[Assumption~A1]{Ausleder13} for studying ESQM.

\begin{assumption}\label{A1}
For \eqref{eq1}, the $RCQ(x)$ holds at every point $x\in C\cap \mathscr{F}$, and for every $x\in C\setminus \mathscr{F}$, there cannot exist $u_i$, $i\in I(x)$, such that
\begin{equation}\label{A11}
u_i\geq 0 ~~ \forall i\in I(x), ~~ \sum_{i\in I(x)}u_i=1, ~~ \left\langle\sum_{i\in I(x)}u_i\nabla g_i(x), z - x\right\rangle\geq0 ~~\forall z\in C.
\end{equation}
where $I(x) := \Big\{ s\in\{1, 2, \dots, m\}: g_s(x) = \max_{i = 1, 2, \dots, m} \{ g_i(x), 0\} \Big\}$.\footnote{Note that, from the definition of $I_k(\cdot)$ in \eqref{defiIk}, one can see that the difference between $I_k(\cdot)$ and $I(\cdot)$ is that $I_k(\cdot)\subseteq\{0, 1,\cdots,m\}$, while $I(\cdot)\subseteq\{1,\cdots,m\}$.}
\end{assumption}
\begin{remark}\label{remarkRCQ}
\begin{enumerate}[{\rm (i)}]
  \item According to \cite[Remark~2.1]{Ausleder13}, we have that if Assumption~\ref{A1} holds, then for any $x\in C$, there cannot exist $u_i$, $i\in I(x)$, such that \eqref{A11} holds.
  \item From \cite[Remark~2.2]{Ausleder13}, we know that if the $RCQ(x)$ holds at every $x\in C$, then Assumption~\ref{A1} holds.
\end{enumerate}
\end{remark}
Using Assumption~\ref{A1} and Theorem~\ref{suffdec}, we will prove in the next theorem that the sequence $\{\theta_k\}$ in Algorithm~\ref{alg:Framwork} is bounded.
\begin{theorem}[Boundedness of $\{\theta_k\}$]\label{alpha}
Consider \eqref{eq1}. Suppose that Assumption~\ref{A1} holds and $\bar{\beta}: = \sup_k\beta_k < \sqrt{\frac{L_g}{L_g + \ell_g}}$. Let $\{(x^k,y^k,\theta_k)\}$ be generated by Algorithm~\ref{alg:Framwork}, $A := \{k\in \mathbb{N}:\theta_{k+1}>\theta_k\}$, and let $|A|$ denote the number of elements in A. Then $|A|$ is finite, i.e., there exists $N_0\in \mathbb{N}$, such that $\theta_k \equiv \theta_{N_0}$, whenever $k\geq N_0$. Moreover, $s^{k+1} = 0$ whenever $k\geq N_0$.
\end{theorem}

\begin{proof}
%By the definition of $A$, $|A|$ is either finite or infinite.
Suppose to the contrary that $|A| = \infty$. Then by the definition of $\theta_k$ in Step~3 of Algorithm~\ref{alg:Framwork}, we have $\lim_{k\to \infty}\theta_k = \infty$ and $\lim_{k\to \infty}\theta_k^{-1}=0$.
	
We first claim that for each $i$, there exists $n_i\in \mathbb{Z}$ such that, for all $k\geq n_i$,
\begin{equation*}
 g_i(y^k) + \langle \nabla g_i(y^k),x^{k+1} - y^k \rangle\leq 0.
\end{equation*}
If not, then there exists $i_0\in\{1,\dots, m\}$, and (infinite) subsequences $\{x^{k_j}\}$ and $\{y^{k_j}\}$, such that
\begin{equation*}
 g_{i_0}(y^{k_j}) + \langle \nabla g_{i_0}(y^{k_j}),x^{{k_j}+1} - y^{k_j} \rangle > 0.
\end{equation*}
Using this and the definition of $I_k(\cdot)$ in \eqref{defiIk}, we have that for all $i\in I_{k_j}(x^{k_j+1})$,
\begin{equation*}
 g_{i}(y^{k_j}) + \langle \nabla g_{i}(y^{k_j}),x^{{k_j}+1} - y^{k_j} \rangle > 0.
\end{equation*}
In particular, $0 \notin  I_{k_j}(x^{k_j+1})$ (recall that $g_0 \equiv 0$). Now, in view of the finiteness of $\left\{I_{k_j}(x^{k_j+1})\right\}$ (since $I_{k_j}(x^{k_j+1})\subseteq \{1, \dots, m\}$ for all $j$), by passing to a further subsequence if necessary, we deduce that there exists a nonempty subset $I_0\subseteq \{1,\dots,m\}$ such that
$I_{k_j}(x^{k_j+1})\equiv I_0$ for all $j$. That is, for all $i\in I_0$,
\begin{equation}\label{eq10}
\ling{i}{x^{k_j+1}}{y^{k_j}} = \max_{s = 0, 1, \dots, m} \left\{ \ling{s}{x^{k_j+1}}{y^{k_j}}\right\} > 0~~ \forall j.
\end{equation}
In addition, from Lemma~\ref{subproremarks}(iii), we have that for each $k_j$, there exist $\lambda_i^{k_j} \geq 0$ for each $i\in I_{k_j}(x^{k_j + 1}) \equiv I_0$, such that $\sum_{i\in I_0}\lambda_i^{k_j} = 1$ and
\begin{align}\label{eq11}
0\in \theta_{k_j}^{-1}(\partial P_1(x^{k_j+1}) - \xi^{k_j}) + L_g(x^{k_j+1} - y^{k_j}) \!+\! \sum_{i\in I_0} \lambda_i^{k_j} \nabla g_i(y^{k_j}) \!+\! \mathcal{N}_C(x^{k_j+1}).
\end{align}
Now, since the sequences $\{x^k\}$ and $\{\lambda_i^{k_j}: i \in I_0\}$ are bounded, by passing to a further subsequence if necessary, we assume that $\lim_{j \to \infty} x^{k_j} = x^{*}$ for some $x^*$ and that for each $i\in I_0$, $\lim_{j \to \infty}\lambda_i^{k_j}= \bar{\lambda}_i$ for some $\bar \lambda_i$. Then $x^*\in C$, $\bar{\lambda}_i \ge 0$ (for each $i\in I_0$), $\sum_{i\in I_0} \bar{\lambda}_i = 1$ and $I_0 \subseteq \left\{i \in\{0,1,\cdots,m\}: g_i(x^*) = \max_{i=0,1,\cdots,m} g_i(x^*)\right\}$ (thanks to \eqref{eq10}, \eqref{ling} and Theorem~\ref{suffdec}(iii)). Since $0\notin I_0$, we see that
\[
I_0 \subseteq I(x^*) :=\left\{i \in\{1,2,\cdots,m\}: g_i(x^*) = \max_{i=1,\cdots,m} \{g_i(x^*), 0\}\right\}.
\]
Passing to the limit in \eqref{eq11}, and noting that $\lim_{j \to \infty} \theta_{k_j}^{-1} = 0$, $\lim_{j \to \infty}\| x^{k_j+1} - y^{k_j} \|=\lim_{j \to \infty}\| x^{k_j+1} - x^{k_j} \|=0$ (thanks to Theorem~\ref{suffdec}(iii)) and the fact that $\{\partial P_1(x^{k_j+1})\}$ and $\{\xi^{k_j}\}$ are uniformly bounded (thanks to the real-valuedness and convexity of $P_1$, $P_2$, the compactness of $C$ and \cite[Theorem~24.7]{Ro70}), we have that
$$0\in \sum_{i\in I_0}\bar{\lambda}_i \nabla g_i(x^*) + \mathcal{N}_C(x^*),$$
which implies that
\begin{align*}
\left\langle \sum_{i\in I_0}\bar{\lambda}_i \nabla g_i(x^*), x-x^* \right\rangle \geq 0 ~~~~ \forall x\in C.
\end{align*}
Since $I_0\subseteq I(x^*)$, this contradicts Assumption~\ref{A1} (see also Remark~\ref{remarkRCQ}(i)).

Therefore, if $|A| = \infty$, then it must hold that for each $i$, there exists $n_i\in \mathbb{Z}$, such that for any $k\geq n_i$,
\[
g_i(y^k) + \langle \nabla g_i(y^k),x^{k+1} - y^k \rangle\leq 0.
\]
Let $N_0 =  \max_{i = 1, \dots, m} n_i$. Then for all $i\in\{1, 2, \dots, m\}$ and for any $k\ge N_0$, we have
\[
g_i(y^k) + \langle \nabla g_i(y^k),x^{k+1} - y^k \rangle\leq 0.
\]
In view of this and the definition of $\theta_k$ in Step 3 of Algorithm \ref{alg:Framwork}, we must have $\theta_k\equiv \theta_{N_0}$ for all $k\geq N_0$, which contradicts $\theta_k\rightarrow\infty$. Thus, it must hold that $|A|\not= \infty$.

Since $|A|$ is finite, there exists $N_0\in \mathbb{Z}$, such that $\theta_k\equiv\theta_{N_0}$ whenever $k\geq N_0$. From Step 3 of Algorithm \ref{alg:Framwork}, we known that for each $i$, $ g_i(y^k) + \langle \nabla g_i(y^k), x^{k+1} - y^k \rangle\leq 0$, for all $k\geq N_0$. Then Lemma~\ref{subproremarks}(i) asserts that $s^{k+1}=0$ for any $k\geq N_0$.
\end{proof}

We are now ready to prove that any cluster point of sequence $\{x^k\}$ generated by Algorithm~\ref{alg:Framwork} is a critical point of \eqref{eq1}.
\begin{theorem}[Subsequential convergence]\label{subconver}
Consider \eqref{eq1}. Suppose that Assumption \ref{A1} holds and $\bar{\beta}: = \sup_k\beta_k < \sqrt{\frac{L_g}{L_g + \ell_g}}$. Let $\{(x^k,y^k,\theta_k)\}$ be generated by Algorithm \ref{alg:Framwork}. Then for any accumulation point $\bar x$ of $\{x^k\}$, there exists $\bar{\lambda}_i\geq 0$ for each $i\in \tilde{I}(\bar{x})$ such that $\sum_{i\in \tilde{I}(\bar{x})} \bar{\lambda}_i = 1$ and
\begin{equation}\label{critical3333}
        0\in \partial P_1(\bar{x}) - \partial P_2(\bar x) + \theta_{N_0}\sum_{i\in \tilde{I}(\bar{x})} \bar{\lambda}_i\nabla g_i(\bar{x}) + \mathcal{N}_C(\bar{x}),
\end{equation}
where $\tilde{I}(\bar{x}):=\left\{s \in\{0,1,\cdots,m\}: g_s(\bar{x}) = \max_{i=0,1,\cdots,m} \{g_i(\cdot)\}\right\}$ and $\theta_{N_0}$ is defined in Theorem~\ref{alpha}; moreover, $\bar x$ is a critical point of \eqref{eq1}.
%
%the following statements hold:
%\begin{enumerate}[{\rm (i)}]
%    \item For any accumulation point $\bar{x}$ of $\{x^k\}$, there exists $\bar{\lambda}_i\geq 0$ for each $i\in \tilde{I}(\bar{x})$ such that $\sum_{i\in \tilde{I}(\bar{x})} \bar{\lambda}_i = 1$ and
%        \begin{equation}\label{critical3333}
%        0\in \partial P_1(\bar{x}) - \bar{\xi} + \theta_{N_0}\sum_{i\in \tilde{I}(\bar{x})} \bar{\lambda}_i\nabla g_i(\bar{x}) + \mathcal{N}_C(\bar{x}),
%        \end{equation}
%        where $\tilde{I}(\bar{x}):=\left\{s \in\{0,1,\cdots,m\}: g_s(\bar{x}) = \max_{i=0,1,\cdots,m} \{g_i(\cdot)\}\right\}$.
%    \item Any
%\end{enumerate}
\end{theorem}
\begin{proof}
Suppose that $\bar{x}$ is an accumulation point of $\{x^k\}$ with $\lim_{j\to \infty} x^{k_j} = \bar{x}$ for some convergence subsequence $\{x^{k_j}\}$. In view of the finiteness of $\{I_{k_j}(x^{k_j+1})\}$ (since $I_{k_j}(x^{k_j+1})\subseteq \{0, 1, 2, \dots, m\}$ for all $j$), by passing to a further subsequence if necessary, we see that there exists a nonempty subset $I_0\subseteq \{0, 1,\dots,m\}$ such that\footnote{Notice that $\tilde{I}(\cdot)\subseteq\{0,1,2,\cdots,m\}$ and $I(\cdot)\subseteq\{1,2,\cdots,m\}$}
\begin{equation}\label{I0}
I_{k_j}(x^{k_j+1})\equiv I_0\subseteq \tilde{I}(\bar{x}):=\{s = 0,1,\cdots,m: g_s(\bar{x}) = \max_{i=0,1,\cdots,m} \{g_i(\bar{x})\}\}.
\end{equation}
Let $\{\xi^k\}$ be generated in Algorithm~\ref{alg:Framwork} and $\{\lambda^k_i\}$ with $i\in I_k(x^{k+1})$ be as in Lemma~\ref{subproremarks}(iii). Then $\{\lambda^{k_j}_i\}$ for each $i \in I_{k_j}(x^{k_j+1})\equiv I_0$ is bounded as sequences of nonnegative numbers at most $1$, and $\{\xi^k\}$ is bounded thanks to the real-valuedness and convexity of $P_2$ and \cite[Theorem~24.7]{Ro70}. Passing to a further subsequence if necessary, we assume without loss of generality that $\lim_{j\to \infty} \lambda_i^{k_j} = \bar{\lambda}_i\geq 0$ for each $i\in I_0$ and $\lim_{j\to \infty} \xi^{k_j} = \bar{\xi}$; moreover, the property of $\{\lambda^{k_j}_i\}$ with $i\in I_{k_j}(x^{k_j+1})\equiv I_0$ guaranteed by Lemma~\ref{subproremarks}(iii) asserts that for all $j$, it holds that
\begin{equation}\label{inclusion}
\begin{aligned}
&0\in \partial P_1(x^{k_j+1}) \!-\! \xi^{k_j} \!+\! \theta_{k_j}L_g(x^{{k_j}+1} - y^{k_j}) \!+\! \theta_{k_j}\! \sum_{i\in I_0} \lambda_i^{k_j}\nabla g_i(y^{k_j}) \!+\! \mathcal{N}_C(x^{{k_j}+1})\\
&{\rm and}\ \ \sum_{i\in I_0}\lambda^{k_j}_i = 1,\ \ \ \lambda^{k_j}_{i} \ge 0\ \ \forall i \in I_{k_j}(x^{k_j+1})\equiv I_0.
\end{aligned}
\end{equation}
In addition, in view of the subproblem \eqref{eq2}, we obtain that for each $j$,
\begin{equation}\label{giineq}
g_i(y^{k_j}) + \langle\nabla g_i(y^{k_j}), x^{{k_j}+1} - y^{k_j} \rangle \leq s^{{k_j}+1} ~~~~ \forall i =1,\dots,m.
\end{equation}

Now, note that $\lim_{j\to \infty} \| x^{k_j} - x^{k_j-1} \|=\lim_{m\to \infty} \| x^{k_j+1} - y^{k_j} \|=0$ (thanks to Theorem~\ref{suffdec}(iii)), $s^{k_j+1} = 0$ and $\theta_{k_j} \equiv \theta_{N_0}$, for any $k_j\geq N_0$ (thanks to Theorem~\ref{alpha}). Passing to the limit in \eqref{giineq} and \eqref{inclusion}, we see that
\begin{equation}\label{critical1}
g_i(\bar{x})\le 0 ~~ \forall i=1,\dots,m,\ \ \sum_{i\in I_0}\bar \lambda_i = 1,\ \ \ \bar\lambda_i \ge 0 \ \ \forall i\in I_0,
\end{equation}
and
\begin{equation}\label{critical3}
0\in \partial P_1(\bar{x}) - \partial P_2(\bar x) + \theta_{N_0}\sum_{i\in I_0} \bar{\lambda}_i\nabla g_i(\bar{x}) + \mathcal{N}_C(\bar{x}).
\end{equation}
where we also invoked the closedness of $\partial P_1$, $\partial P_2$ and $\mathcal{N}_C$ to deduce \eqref{critical3}. Then the inclusion \eqref{critical3333} follows from \eqref{critical3} and \eqref{critical1} upon noting that $I_0 \subseteq \tilde I(\bar x)$ (see \eqref{I0}) and defining $\bar \lambda_i = 0$ for $i\in \tilde I(\bar x)\setminus I_0$.

Finally, let $\hat{\lambda}_i := \theta_{N_0}\bar{\lambda}_i\geq 0$ for all $i\in I_0\cap\{1,2,\cdots,m\}$, and $\hat{\lambda}_i = 0$ for all $i\in \{1,2,\cdots,m\}\setminus I_0$. Then by \eqref{critical1} and $I_0\subseteq \tilde{I}(\bar{x})$ (see \eqref{I0}), we have that
\begin{equation}\label{critical2}
\hat{\lambda}_i g_i(\bar{x}) = 0 ~~ \forall i=1,\dots,m.
\end{equation}
Indeed, for each $i\in I_0$, we have that $g_i(\bar{x}) = 0$, and for each $i\notin I_0$, we have that $\hat{\lambda}_i = 0$.

Notice that $\nabla g_0 (\bar{x}) = 0$ (thanks to $g_0 \equiv 0$), using the definition of $\hat{\lambda}_i$ and \eqref{critical3}, we have
\begin{equation}\label{critical33}
\begin{aligned}
0&\in \partial P_1(\bar{x}) - \partial P_2(\bar{x}) + \sum_{i=1}^m \hat{\lambda}_i\nabla g_i(\bar{x}) + \mathcal{N}_C(\bar{x}).
\end{aligned}
\end{equation}
Combining \eqref{critical1}, \eqref{critical2} and \eqref{critical33}, we conclude that $\bar{x}$ is a critical point of \eqref{eq1}.
%This completes the proof.
\end{proof}


%\subsection{Global convergence}

We next derive the global convergence property of the $\{x^k\}$ generated by Algorithm~\ref{alg:Framwork}. We will need to make use of the following function,
\begin{equation}\label{defH}
H(x,y,z) := \frac{P(x) - \bar{m}}{\hat{\theta}} + \delta_{C}(x) + \max_i[\ling{i}{x}{z}]_+ + \frac{L_g}{2}\| x-y \|^2 + \frac{L_g}{2}\| x - z \|^2,
\end{equation}
where $\bar{m}$ is defined in Theorem~\ref{suffdec}(ii), and $\hat{\theta}:= \theta_{N_0}$ with $N_0$ defined in Theorem~\ref{alpha}. Our analysis follows the nowadays standard convergence arguments based on Kurdyka-{\L}ojasiewicz property; see, for example, \cite{attouch10,attouch13,bolte14}. In essence, under Assumption~\ref{A1}, we will show that $H$ has sufficient descent along the sequence $\{(x^{k+1},x^{k},y^k)\}$ for all sufficiently large $k$, and $H$ is constant on the set of accumulation points of $\{(x^{k+1},x^{k},y^k)\}$. We will also show that $ \d(0,\partial H(x^{k+1},x^{k},y^k))$ is suitably bounded by successive changes of the iterates by imposing additional differentiability assumptions on $g$ and $P_2$. These together with the assumption that $H$ satisfies the KL property will be used to establish global convergence of the sequence $\{x^k\}$ generated by Algorithm~\ref{alg:Framwork}.

We start with a remark concerning the sufficient descent property.
\begin{remark}[Sufficient descent]\label{rebarh}
In fact, from the definition of $Q$ in Theorem~\ref{suffdec}{\rm (ii)}, we see that $H(x,y,z) = Q(x,y,z,\theta_{N_0}) + \delta_C(x)$. Now, according to Theorem~\ref{alpha}, there exists an integer $N_0$ such that $\theta_k \equiv \theta_{N_0}$ and $s^{k+1} = 0$ for all $k\geq N_0$. Thus, we have $H(x^{k+1},x^{k},y^k) = Q(x^{k+1},x^{k},y^k,\hat{\theta})$ for all $k\geq N_0$. Then one can see that the sequence $\{H(x^{k+1},x^{k},y^k)\}_{k\ge N_0}$ is nonincreasing thanks to Theorem~\ref{suffdec}{\rm (ii)}, and it holds that
\[
H(x^{k+1},x^{k},y^{k}) \leq H(x^{k},x^{k-1},y^{k-1}) - \frac{L - (L + \ell)\bar{\beta}^2}{2}\| x^{k} - x^{k-1}\|^2\ \ \ \ \forall k\ge N_0,
\]
where $\bar{\beta}: = \sup_k\beta_k$.
\end{remark}

\begin{lemma}\label{lemma1}
Consider \eqref{eq1}. Suppose that Assumption~\ref{A1} holds and $\bar{\beta}: = \sup_k\beta_k < \sqrt{\frac{L_g}{L_g + \ell_g}}$. Let $\{(x^k,y^k)\}$ be generated by Algorithm \ref{alg:Framwork}, and $\Omega$ be the set of accumulation points of $\{(x^{k+1},x^{k},y^{k})\}$. Then $\Omega$ is a nonempty compact set, $\omega := \lim_{k\to \infty} H(x^{k+1},x^{k}, y^{k})$ exists, and $H \equiv \omega$ on $\Omega$.
\end{lemma}
\begin{proof}
From Theorem~\ref{suffdec}(i), we have that the set of accumulation points of $\{x^k\}$, denoted by $\varLambda$, is a nonempty compact set. Since $\lim_{k\to \infty} \| x^k - x^{k-1} \|=\lim_{k\to \infty} \| x^k - y^k\| = 0$ thanks to Theorem~\ref{suffdec}(iii), one can see that $\Omega= \{(\bar{x},\bar{x},\bar{x}): \bar{x}\in \varLambda\}$, which is a nonempty compact set.

Next, according to Remark~\ref{rebarh}, the sequence $\{H(x^{k+1},x^{k}, y^{k})\}_{k\ge N_0}$ is nonincreasing. Moreover, one can see from the definition of $H$ (see \eqref{defH}) that $\{H(x^{k+1},x^{k}, y^{k})\}$ is bounded from below (by zero). Thus, $\omega := \lim_{k\to \infty} H(x^{k+1},x^{k}, y^{k})$ exists.

For any $(\bar{x},\bar{x},\bar{x})\in \Omega$, let $\{x^{k_j}\}$ be a convergent subsequence with $\lim_{j\to \infty} x^{k_j} = \bar{x}$.
Since $P$ and $g_i$ are continuous, and $\lim_{k\to \infty} \| x^k - x^{k-1}\| = \lim_{k\to \infty} \| x^k - y^{k}\| = 0$ (see Theorem~\ref{suffdec}(iii)), we obtain that
\begin{align*}
& H(\bar{x}, \bar{x}, \bar{x}) = \frac{P(\bar{x}) - \bar{m}}{\hat{\theta}} + \max_{i = 1,\cdots,m}\left[\ling{i}{\bar x}{\bar x}\right]_+\\
&\!\!=\!\lim_{j\to \infty}\! \frac{P(x^{k_j+1}) \!-\! \bar{m}}{\hat{\theta}} \!+\!\!\max_{i = 1,\cdots,m}\!\left[\ling{i }{x^{k_j+1}}{y^{k_j}}\right]_+ \!\!\!+\! \frac{L_g}{2}\| x^{k_j+1} \!\!-\! x^{k_j} \|^2 \!+\! \frac{L_g}{2}\| x^{k_j+1} \!\!-\! y^{k_j} \|^2 \\
&\!\!=\lim_{j\to \infty} H(x^{k_j+1},x^{k_j},y^{k_j}) = \lim_{k\to \infty} H(x^{k+1},x^{k}, y^{k}) = \omega.
\end{align*}
Since $(\bar{x}, \bar{x}, \bar{x})\in \Omega$ is arbitrary, we conclude that $H \equiv \omega$ on $\Omega$.
%This completes the proof.
\end{proof}


Next, we introduce an assumption for deriving a bound on $\d(0,\partial H (x^{k+1}, x^k, y^k))$. This assumption appears in \cite{wen18,yu21} and is satisfied in many applications; see \cite{wen18}.
\begin{assumption}\label{A2}
Each $g_i$ in \eqref{eq1} is twice continuously differentiable. The function $P_2$ is continuously differentiable on an open set $U_0$ containing $\mathcal{X}$, and $\nabla P_2$ is locally Lipschitz continuous on $U_0$, where $\mathcal{X}$ be the set of critical points of \eqref{eq1}.
\end{assumption}

Now, we present the following bound on $\d(0,\partial H(x^{k+1},x^{k},y^{k}))$.
\begin{lemma}\label{th2.1}
Consider \eqref{eq1}. Suppose that Assumptions~\ref{A1} and \ref{A2} hold, and $\bar{\beta}: = \sup_k\beta_k < \sqrt{\frac{L_g}{L_g + \ell_g}}$. Let $\{(x^k,y^k)\}$ be generated by Algorithm \ref{alg:Framwork}. Then there exist $\tau > 0$ and a positive integer $N_1$ such that for all $k \ge N_1$, we have
\[
\d (0,\partial H(x^{k+1},x^{k},y^{k}))\le \tau (\|x^{k+1} - x^{k}\| + \|x^{k} - x^{k-1}\|).
\]
%\begin{enumerate}[{\rm (i)}]
%    \item
%    \item The sequence $\{x^k\}$ globally converges to a critical point of \eqref{eq1}. Moreover, $\sum_{k=1}^{\infty} \| x^{k+1} - x^{k}\|<\infty.$
%\end{enumerate}
\end{lemma}
\begin{proof}
Let $\varLambda$ be the set of accumulation points of $\{x^k\}$. Then $\varLambda$ is nonempty and compact in view of Theorem~\ref{suffdec}(i), and $\varLambda\subseteq {\cal X}$ thanks to Theorem~\ref{subconver}. Moreover, we have $\lim_{k\to \infty}\d(x^k,\varLambda)=0$. Thus, for any $\gamma>0$, there exists $N_2>0$, such that dist$(x^k,\varLambda)<\gamma$ and $x^k\in U_0$ (where $U_0$ is defined as in Assumption \ref{A2}) for all $k\geq N_1$.

Next, let $N_0$ be defined as in Theorem \ref{alpha}. Since $P_2$ is continuously differentiable on $U_0$ and $x^k\in U_0$ for any $k\geq N_1 := \max\{N_0,N_2\}$, we obtain from \cite[Theorem~8.6]{rock97a} that for any $k \ge N_1$,
\begin{align}\label{eq16}
&\!\!\!\!\!\!\!\!\!\!~~~\partial H(x^{k+1},x^{k},y^{k})\supseteq \widehat{\partial} H(x^{k+1},x^{k},y^{k})\notag \\
&\!\!\!\!\!\!\!\!\!\!~~\overset{\rm(a)}\supseteq\!\!\!
\left[\!
\begin{array}{c}
\Xi_k    \\ [2 pt]
 - L_g(x^{k+1} - x^{k}) \\ [2 pt]
 \conv\!_{i\in I_k(x^{k+1})}\{\nabla^2 g_i(y^{k})(x^{k+1} - y^{k})\} - L_g(x^{k+1}-y^{k})
\end{array}\!
\right] \notag\\
&\!\!\!\!\!\!\!\!\!\!~~\overset{\rm(b)}{\supseteq}\!\!\!
\left[
\begin{array}{c}
\!\frac{1}{\hat{\theta}}\partial P(x^{k+1}) \!+ \!\!\!\!\sum\limits_{i\in I_k(x^{k+1})}\!\!\! \lambda_i^k \nabla g_i(y^k) \!+\! \mathcal{N}_C(x^{k+1}) \!+\! L_g(x^{k+1} \!-\! x^k) \!+\! L_g(x^{k+1} \!-\! y^k)    \\
 - L_g(x^{k+1} - x^{k}) \\ [5 pt]
\sum_{i\in I_k(x^{k+1})} \lambda_i^k\nabla^2 g_i(y^{k})(x^{k+1} - y^{k}) - L_g(x^{k+1}-y^{k})
\end{array}\!
\right]\!\!,\!\!\!\!
\end{align}
where $I_k(x^{k+1})$ and $\lambda_i^k$ are defined as in Lemma~\ref{subproremarks}(iii) and
\[
\Xi_k \!\!:=\! \frac{1}{\hat{\theta}}\partial P(x^{k+1}) + \conv\!_{i\in I_k(x^{k+1})}\{\nabla g_i(y^k)\} + \mathcal{N}_C(x^{k+1}) + L_g(x^{k+1} - x^k) + L_g(x^{k+1} - y^k),
\]
and (a) holds because of the subdifferential calculus rules in \cite[Proposition~10.5, Corollary~10.9, Exercise~8.31]{rock97a} and the regularity of the closed convex convex set $C$ and the convex function $P_1$ (thanks to \cite[Corollary~10.9, Proposition~8.12, Exercise~8.8]{rock97a}), and (b) holds as $\sum_{i\in I_k(x^{k+1})} \lambda_i^k = 1$ and $\lambda_i^k\ge 0$ for all $i\in I_k(x^{k+1})$.
%$(c)$ holds for any $\lambda^k \in \widehat{\mathcal{N}}_{-\mathbb{R}_+^m}(\bar{g}(x^{k+1}, y^k)) = \mathcal{N}_{-\mathbb{R}_+^m}(\bar{g}(x^{k+1}, y^k))$ thanks to \cite[Theorem 6.14]{rock97a}.

On the other hand, according to Theorem \ref{alpha} and the definition of $\hat\theta$ in \eqref{defH}, we have that $\theta_k \equiv \theta_{N_0} = \hat \theta$ for any $k\ge N_0$. Using this together with the property of $\lambda^k_i$ from Lemma~\ref{subproremarks}(iii) and the differentiability assumption of $P_2$, we obtain that
\begin{align*}
0\in \partial P_1(x^{k+1}) - \nabla P_2(x^{k}) + \hat{\theta}\sum_{i\in I_k(x^{k+1})} \lambda_i^k \nabla g_i(y^k) + \hat{\theta}L_g(x^{k+1} - y^{k})+ \mathcal{N}_C(x^{k+1}).
\end{align*}
Rearranging terms in the above display, we see that
\begin{align}\label{eq23}
\nabla P_2(x^{k}) \!-\! \hat{\theta}\sum_{i\in I_k(x^{k+1})} \lambda_i^k \nabla g_i(y^k) - \hat{\theta}L_g(x^{k+1} - y^{k})\in \partial P_1(x^{k+1}) + \mathcal{N}_C(x^{k+1}).
\end{align}

%We claim that for any $k\geq\max\{N_0, N_1\}$,
%\begin{align}
%&\frac{1}{\hat{\theta}}\left(-\hat{\theta}L_g(x^{k} - y^{k}) + \nabla P_2(x^{k}) - \nabla P_2(x^{k+1})\right)  \nonumber\\
%& \in \frac{1}{\hat{\theta}}\partial P(x^{k+1}) + \sum_{i\in I_k(x^{k+1})} \lambda_i^k \nabla g_i(y^k) + \mathcal{N}_C(x^{k+1}) + L_g(x^{k+1} - x^k).
%\end{align}
Since $P_2$ is continuously differentiable in $U_0$ (and hence at $x^k$ and $x^{k+1}$ when $k\ge N_1$), we obtain for any $k\ge N_1$ that
\begin{align}\label{eq22}
&\frac{1}{\hat{\theta}}\left(-\hat{\theta}L_g(x^{k} - y^{k}) + \nabla P_2(x^{k}) - \nabla P_2(x^{k+1})\right) \notag\\
& = \frac{1}{\hat{\theta}}\left(\hat{\theta}L_g(x^{k+1} - x^{k}) - \nabla P_2(x^{k+1}) + \hat{\theta}\sum_{i\in I_k(x^{k+1})} \lambda_i^k\nabla g_i(y^k)\right) \notag\\
&~~~~ + \frac{1}{\hat{\theta}}\left(\nabla P_2(x^{k}) - \hat{\theta}\sum_{i\in I_k(x^{k+1})} \lambda_i^k\nabla g_i(y^k)
 - \hat{\theta}L_g(x^{k+1} - y^{k})\right) \notag\\
& \overset{\rm(a)}{\in}\! \frac{1}{\hat{\theta}}\!\left(\hat{\theta}L_g(x^{k+1} \!-\! x^{k}) \!-\! \nabla P_2(x^{k+1}) + \hat{\theta}\!\!\!\!\sum_{i\in I_k(x^{k+1})}\!\! \lambda_i^k\nabla g_i(y^k)\right) \!+\! \frac{1}{\hat{\theta}}\partial P_1(x^{k+1}) \!+\! \mathcal{N}_C(x^{k+1}) \notag\\
& = \frac{1}{\hat{\theta}}\partial P(x^{k+1}) + \sum_{i\in I_k(x^{k+1})} \lambda_i^k \nabla g_i(y^k) + \mathcal{N}_C(x^{k+1}) + L_g(x^{k+1} - x^k)
\end{align}
where (a) follows from \eqref{eq23}, and the last equality holds because $P = P_1 - P_2$.

Combining \eqref{eq16} and \eqref{eq22}, for any $k\geq N_1$, we have
\begin{equation}
\begin{aligned}
\nonumber
\left[
\begin{array}{c}
\frac{1}{\hat{\theta}}\left(-\hat{\theta}L_g(x^{k} - y^{k}) + \nabla P_2(x^{k}) - \nabla P_2(x^{k+1})\right) + L_g(x^{k+1} - y^{k})\\
-L_g(x^{k+1} - x^{k})\\
\sum_{i\in I_k(x^{k+1})}\lambda_i^k\nabla^2 g_i(y^{k})(x^{k+1} - y^{k}) - L_g(x^{k+1} - y^{k})
\end{array}
\right]
\in \partial H(x^{k+1},x^{k},y^{k}).
\end{aligned}
\end{equation}
Since $\nabla P_2$ is Lipschitz continuous with modulus $L_{P_2}$, $y^k = x^k + \beta_k(x^k - x^{k-1})$ (see \eqref{defyk}), we see that for any $k\geq \max\{N_0,N_1\}$,
\begin{align*}
& \d\left(0,\partial H(x^{k+1},x^{k},y^{k})\right)^2 \\
&\leq \left\| \frac{1}{\hat{\theta}}\left(\!-\hat{\theta}L_g(x^{k} - y^{k}) + \nabla P_2(x^{k}) - \nabla P_2(x^{k+1})\right) + L_g(x^{k+1} \!-\! y^{k})\right\|^2\!\!\!\! +\! \| L_g(x^{k+1} - x^{k})\|^2 \\
&~~~~~~ + \left\| \sum_{i\in I_k(x^{k+1})} \lambda_i^k\nabla^2 g_i(y^k)(x^{k+1} - y^{k}) - L_g(x^{k+1} - y^{k})\right\|^2 \\
&\leq 3L_g^2\| x^{k} - y^{k}\|^2 + \frac{3}{\hat{\theta}^2}L_{P_2}^2\| x^{k+1} - x^{k}\|^2 + 3L_g^2\| x^{k+1} - y^{k}\|^2 + L_g^2\| x^{k+1} - x^{k} \|^2 \\
&~~~~~~ + 2\left(\sum_{i\in I_k(x^{k+1})} \lambda_i^k\nabla^2 g_i(y^k)\right)^2 \| x^{k+1}-y^{k} \|^2 + 2L_g^2\| x^{k+1} - y^{k}\|^2.
\end{align*}
The desired conclusion now follows immediately from the above display, the definition and the boundedness of $\{y^k\}$ (thanks to Theorem~\ref{suffdec} and \eqref{defyk}) and the continuity of $\nabla^2 g_i$ (thanks to Assumption~\ref{A2}).
\end{proof}

Now, we present the convergence rate of the $\{x^k\}$ generated by Algorithm~\ref{alg:Framwork} under suitable assumptions. The proof is routine and we refer the readers to, for example, the proofs of Theorems 4.2 and 4.3 of \cite{wen18}.
\begin{theorem}[Convergence rate of Algorithm \ref{alg:Framwork} in nonconvex setting]\label{th2.2}
Consider \eqref{eq1}. Suppose that Assumptions~\ref{A1} and \ref{A2} hold, $\bar{\beta}: = \sup_k\beta_k < \sqrt{\frac{L_g}{L_g + \ell_g}}$, and $H$ in \eqref{defH} is a KL function. Let $\{(x^k, y^k)\}$ be generated by Algorithm~\ref{alg:Framwork} and $\Omega$ be the set of accumulation points of $\left\{(x^{k+1},x^{k},y^{k})\right\}$. Then $\{x^k\}$ converges to a critical point $\bar{x}$ of \eqref{eq1}. Moreover, if $H$ satisfies the KL property with exponent $\alpha\in [0,1)$ at every point in $\Omega$, then there exists a positive integer $\underline{N}$ such that the following holds:
\begin{enumerate}[{\rm (i)}]
    \item If $\alpha=0$, then $\{x^k\}$ converges finitely.
    \item If $\alpha\in (0,\frac{1}{2}]$, then there exist $a_0\in(0, 1)$ and $a_1>0$ such that \[
        \|x^k - \bar{x}\|\leq a_1a_0^k ~~\forall k > \underline{N}.
        \]
    \item If $\alpha\in (\frac{1}{2},1)$, then there exists $a_2>0$ such that
        \[
        \|x^k - \bar{x}\|\leq a_2k^{-\frac{1-\alpha}{2\alpha-1}} ~~\forall k > \underline{N}.
        \]
\end{enumerate}
\end{theorem}


\subsection{Convergence analysis in convex setting}\label{sec42}
We study the convergence properties of Algorithm \ref{alg:Framwork} under the following convex settings.

\begin{assumption}\label{B1}
Suppose that in \eqref{eq1}, $P_2 = 0$ and $g_1,\ldots, g_m$ are convex.
\end{assumption}

\begin{assumption}\label{B2}
The Slater condition holds for $\mathcal{G}$ in \eqref{eq1}, i.e., there exists $\hat{x}\in C$ with $g_i(\hat{x})<0$ for $i=1,\dots,m$.
\end{assumption}
\begin{remark}\label{rem43}
If each $g_i$ is convex and Assumption~\ref{B2} holds, then $RCQ(x)$ holds at every point $x\in C$, which implies that Assumption~\ref{A1} holds thanks to Remark~\ref{remarkRCQ}.
\end{remark}

Now, we present the convergence properties of Algorithm \ref{alg:Framwork} under the Assumptions~\ref{B1} and \ref{B2}. Unlike our convergence rate result in Theorem~\ref{th2.2} which was based on the KL property of the function $H$ in \eqref{defH}, our analysis in this section is based on the KL property of the following function:
\begin{equation}\label{definfalpha}
F_{\eta}(x) := \frac{1}{\eta}(P_1(x) - \hat m) + \delta_C(x) + \max_{i = 1,\cdots,m}\left[g_i(x)\right]_+,
\end{equation}
where $\eta > 0$ and $\hat{m} = \min\{P_1(x): x\in C\}$. Compared with $H$, the explicit KL exponent is more readily obtainable, as we will discuss in Section~\ref{sec5}. On the other hand, we will impose an additional assumption concerning the set of minimizers of $F_{\hat\theta}$, where $\hat\theta$ is defined in \eqref{defH}. The next lemma recalls a well-known fact and serves to motivate such an assumption.
\begin{lemma}\label{lem43}
  Consider \eqref{eq1}. Suppose that Assumption~\ref{B1} holds and let $F_\eta$ be defined in \eqref{definfalpha}. If
  $\bar\eta > 0$ is such that $\Argmin F_{\bar\eta}\cap \Argmin_{x\in\cal G}P_1(x)\neq \emptyset$, then $\Argmin F_{\eta} = \Argmin_{x\in {\cal G}}P_1(x)$ whenever $\eta > \bar\eta$.
%  \begin{equation}\label{set3}
%\emptyset\not=\Lambda\subseteq \Argmin_{x\in {\cal G}}P_1(x) = \Argmin F_{\hat\theta} \cap \mathscr{F} = \Argmin F_{{\hat\theta}+\iota},
%\end{equation}
%for any $\iota > 0$.
\end{lemma}
\begin{proof}
Fix any $\eta > \bar\eta$ and let $\hat x \in \Argmin F_{\bar\eta}\cap \Argmin_{x\in\cal G}P_1(x)$.
We first argue that $\Argmin_{x\in\cal G}P_1(x) = \Argmin F_{\eta}\cap \mathscr{F}$. Indeed, if $\tilde x\in \Argmin_{x\in {\cal G}}P_1(x)$, then $\tilde x \in {\cal G}\subseteq \mathscr{F}$ and hence $\max_{i=1,\ldots,m}[g_i(\tilde x)] = 0$. Moreover, it holds that
\begin{align*}
\eta F_{\eta}(\tilde x) = P_1(\tilde x)\overset{\rm (a)}= P_1(\hat x) = \bar\eta F_{\bar\eta}(\hat x)\overset{\rm (b)}\le \bar\eta F_{\bar\eta}(x)\overset{\rm (c)}\le \eta F_{\eta}(x)
\end{align*}
for any $x\in C$, where (a) holds because both $\hat x$ and $\tilde x$ minimizes $P_1$ over ${\cal G}$, (b) holds because $\hat x$ also minimizes $\bar\eta F_{\bar\eta}$, and (c) holds because $\eta> \hat\eta$. As for the converse inclusion, let $\tilde x \in \Argmin F_{\eta}\cap \mathscr{F}$. Then for any $x\in {\cal G} = C\cap \mathscr{F}$, we have
\[
P_1(\tilde x) = {\eta} F_{{\eta}}(\tilde x)\le {\eta} F_{{\eta}}(x) = P_1(x),
\]
where the equalities hold because $x\in \mathscr{F}$, which implies $\max_{i=1,\ldots,m}[g_i(x)]_+=0$. The above arguments establish $\Argmin_{x\in\cal G}P_1(x) = \Argmin F_{\eta}\cap \mathscr{F}$.

To complete the proof, it now suffices to show that $\Argmin F_{\eta}\subseteq \mathscr{F}$. To this end, let $\tilde x \in \Argmin F_{\eta}$. Then we have
\begin{align*}
&P_1(\tilde x) + \eta \max_{i=1,\ldots,m}[g_i(\tilde x)]_+=\eta F_{\eta}(\tilde x) \le \eta F_{\eta}(\hat x) = P_1(\hat x) + \eta \max_{i=1,\ldots,m}[g_i(\hat x)]_+ \\
&\overset{\rm (a)}= P_1(\hat x) + \bar\eta \max_{i=1,\ldots,m}[g_i(\hat x)]_+
= \bar\eta F_{\bar\eta}(\hat x) \overset{\rm (b)}\le \bar\eta F_{\bar\eta}(\tilde x) = P_1(\tilde x) + \bar\eta\max_{i=1,\ldots,m}[g_i(\tilde x)]_+,
\end{align*}
where (a) holds because $\hat x\in {\cal G}$ and (b) holds because $\hat x$ also minimizes $\bar\eta F_{\bar\eta}$. Rearranging the above inequality, we obtain $(\eta-\bar\eta)\max_{i=1,\ldots,m}[g_i(\tilde x)]_+ = 0$, which means $\tilde x\in \mathscr{F}$.
\end{proof}

\begin{theorem}[Convergence rate of Algorithm \ref{alg:Framwork} in convex setting]\label{Edecres}
Consider \eqref{eq1}. Suppose that Assumptions~\ref{B1} and \ref{B2} hold, and $\bar{\beta}: = \sup_k\beta_k < 1$. Let $\{(x^k,\theta_k)\}$ be generated by Algorithm \ref{alg:Framwork}. Then the following statements hold:
\begin{enumerate}[{\rm (i)}]
    \item For any $k\ge 1$,
          \[
          E(x^{k+1},x^{k},\theta_{k+1}) \leq E(x^k,x^{k-1},\theta_k) - \frac{(1 - \beta_k^2)L_g}{2}\|x^{k} - x^{k-1}\|^2,
          \]
          where $E(x,y,\theta):=\frac{1}{\theta}\big(P_1(x) - \hat{m} + \delta_C(x) + \theta\max_{i = 1,\cdots,m}[g_i(x)]_+ + \frac{\theta L_g}{2}\| x - y \|^2\big)$, $\hat m$ is defined as in \eqref{definfalpha} and $L_g$ is defined in Remark~\ref{Remarkg}.
    \item Let $\Omega$ be the set of accumulation points of $\left\{(x^{k+1},x^{k},\theta_k)\right\}$. Then $\Omega$ is a nonempty compact set, $\bar{\omega} := \lim_{k\to \infty} E(x^{k+1},x^{k}, \theta_k)$ exists, and $E \equiv \bar{\omega}$ on $\Omega$.
    \item If $F_{\hat\theta}$ in \eqref{definfalpha} is a KL function with exponent $\frac{1}{2}$ and it holds that
    \begin{equation}\label{assumptionunhappy}
    \Argmin F_{\hat\theta} = \Argmin_{x\in {\cal G}}P_1(x),
    \end{equation}
     then $\{x^k\}$ converges to a minimizer $x^*$ of \eqref{eq1}, and there exist $c_0 > 0$, $s\in (0,1)$ and a positive integer $k_0$ such that
        \[
        \|x^k - x^*\|\leq c_0 s^k ~~\forall k > k_0.
        \]
\end{enumerate}
\end{theorem}
\begin{remark}[Comments on \eqref{assumptionunhappy}]
Consider \eqref{eq1}. Suppose that Assumptions~\ref{B1} and \ref{B2} hold, and $\bar{\beta}: = \sup_k\beta_k < 1$. Let $\{\theta_k\}$ be generated by Algorithm \ref{alg:Framwork}. Using Theorem~\ref{subconver} (see also Remark~\ref{rem43}) and the formula for the subdifferential of $\max_{i=1,\ldots,m}[g_i(\cdot)]_+$, we deduce from the definition of $F_{\hat\theta}$ in \eqref{definfalpha} that
\begin{equation}\label{conditionunhappy}
\emptyset\neq \Lambda\subseteq \Argmin F_{\hat\theta} \cap \Argmin_{x\in\cal G}P_1(x).
\end{equation}
where ${\cal G}$ is the feasible set of \eqref{eq1}. Consequently, Lemma~\ref{lem43} implies that $\Argmin F_{\eta} = \Argmin_{x\in {\cal G}}P_1(x)$ whenever $\eta > \hat\theta$. In this regard, the assumption \eqref{assumptionunhappy} can be seen as a slightly stronger requirement where we require the equality of the two sets to hold when $\eta = \hat\theta$ as well. See Remark~\ref{rem52} below for further comments on \eqref{assumptionunhappy} based on the notion of exact penalty parameters in Definition~\ref{Exact}.
\end{remark}
%
%\begin{proof}
%(i): Since $\{x^k\}\subseteq C$ and $C$ is a nonempty compact convex set, $\{x^k\}$ is bounded.
\begin{proof}
Recall that $s^{k+1} = \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k+1}}{y^k}\right]_+$ (thanks to Lemma~\ref{subproremarks}(i)). Using the definition of $(x^{k+1},s^{k+1})$ in \eqref{eq2} as a minimizer, the feasibility of $(x,\max_{i = 1,\cdots,m}\left[\ling{i}{x}{y^k}\right]_+)$ for \eqref{eq2} when $x\in C$ and the strong convexity of the objective in \eqref{eq2}, we obtain that for any $x\in C$,
\begin{align}\label{stongconve}
&P_1(x^{k+1}) + \theta_k \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k+1}}{y^k}\right]_+ + \frac{\theta_k L_g}{2}\| x^{k+1} - y^k\|^2 \notag\\
&= P_1(x^{k+1}) + \theta_k s^{k+1} + \frac{\theta_k L_g}{2}\| x^{k+1} - y^k\|^2 \\
&\leq P_1(x) + \theta_k \max_{i = 1,\cdots,m}\left[\ling{i}{x}{y^k}\right]_+ + \frac{\theta_k L_g}{2}\| x - y^k \|^2 - \frac{\theta_k L_g}{2}\| x - x^{k+1}\|^2. \notag
\end{align}
Now we are ready to prove the three items one by one.

(i): For any $k \ge 1$, we see that
\begin{align*}
&\frac{1}{\theta_{k+1}}\!\left(P_1(x^{k+1}) \!-\! \hat{m}\right) \!+\!\! \max_{i = 1,\cdots,m}\left[g_i(x^{k+1})\right]_+\overset{\rm(a)}{\leq} \frac{1}{\theta_k}\!\left(P_1(x^{k+1}) \!-\! \hat{m}\right)\! +\!\! \max_{i = 1,\cdots,m}\left[g_i(x^{k+1})\right]_+\\
&\overset{\rm(b)}{\leq} \frac{P_1(x^{k+1}) - \hat{m}}{\theta_k} + \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k+1}}{y^k} + \frac{L_{g_i}}{2}\| x^{k+1} - y^k \|^2\right]_+\\
&\overset{\rm(c)}{\leq} \frac{P_1(x^{k+1}) - \hat{m}}{\theta_k} + \max_{i = 1,\cdots,m}\left[\ling{i}{x^{k+1}}{y^k} \right]_+ + \frac{L_{g}}{2}\| x^{k+1} - y^k \|^2\\
&\overset{\rm(d)}{\leq}\frac{P_1(x^{k}) - \hat{m}}{\theta_k} + \max_{i = 1,\cdots,m}\left[\ling{i}{x^k}{y^k}\right]_+ + \frac{L_{g}}{2}\| x^{k} - y^k \|^2 - \frac{L_g}{2}\| x^{k+1} - x^k \|^2\\
&\overset{\rm(e)}{\leq}\frac{P_1(x^{k}) - \hat{m}}{\theta_k} +  \max_{i = 1,\cdots,m}\left[g_i(x^k)\right]_+ + \frac{L_{g}}{2}\| x^{k} - y^k \|^2 - \frac{L_g}{2}\| x^{k+1} - x^k \|^2\\
&= \frac{1}{\theta_k}\left(P_1(x^{k}) - \hat{m} + \theta_k\max_{i = 1,\cdots,m}\left[g_i(x^k)\right]_+\right) + \frac{\beta_k^2 L_g}{2}\|x^k - x^{k-1}\|^2 - \frac{L_g}{2}\| x^{k+1} - x^k \|^2\\
&= E(x^{k},x^{k-1},\theta_k) - \frac{(1 - \beta_k^2)L_g}{2}\|x^k - x^{k-1}\|^2 - \frac{L_g}{2}\| x^{k+1} - x^k \|^2,
\end{align*}
where (a) holds thanks to $\theta_k \leq\theta_{k+1}$ and $\hat{m} = \min\{P_1(x): x\in C\}$, (b) holds because of the Lipschitz continuity of $\nabla g_i$, (c) follows from $L_g := \max\{L_{g_i}:\; i=1,\dots,m\}$, (d) holds thanks to \eqref{stongconve} with $x=x^k$ (as $x^k\in C$), (e) follows from the convexity of $g_i$, and the last equality follows from the definition of $E(x^{k},x^{k-1},\theta_{k})$. The desired inequality now follows immediately from the above display and the definition of $E(x^{k+1},x^{k},\theta_{k+1})$.

(ii): Using similar arguments as Lemma~\ref{lemma1} (but using item (i) in place of Remark~\ref{rebarh}, and noting that Assumption~\ref{A1} holds according to Remark~\ref{rem43}), one can show that (ii) holds. We omit its proof for brevity.

(iii): Let $\Lambda$ be the set of accumulation points of $\{x^k\}$ for notational simplicity. Using Theorem~\ref{subconver} and the assumption \eqref{assumptionunhappy}, we conclude that
\begin{equation}\label{set}
\emptyset\not=\Lambda\subseteq \Argmin_{x\in {\cal G}}P_1(x) = \Argmin F_{\hat{\theta}} =: S.
\end{equation}
Now, write $E_{\theta}(x, y) := E(x, y, \theta)$ for notational simplicity. By the definition of $F_{\hat{\theta}}(x)$ in \eqref{definfalpha} and $E(x, y, \theta)$ in (i), we see that $E_{\hat{\theta}}(x, y) = F_{\hat{\theta}}(x) + \frac{L_g}{2}\|x - y\|^2$. From Remark~\ref{rem43}, Theorem~\ref{alpha} and item (i), we have that for any $k\geq N_0$, it holds that $\theta_k = \hat\theta$ and
\begin{equation}\label{upperK}
E_{\hat{\theta}}(x^{k+1}, x^k)\leq E_{\hat{\theta}}(x^k, x^{k-1}) -\frac{L_g(1 - \bar{\beta}^2)}{2}\|x^k - x^{k-1}\|^2.
\end{equation}
Moreover, since $F_{\hat{\theta}}$ is a KL function with exponent $\frac{1}{2}$, we conclude from \cite[Theorem~3.6]{li18} that $E_{\hat{\theta}}$ is a KL function with exponent $\frac{1}{2}$.

Let $\bar{S} = \{(x^*, x^*): x^*\in S\}$ and $\bar{\Lambda} = \{(x^*, x^*): x^*\in \Lambda\}$.
In view of \eqref{set}, we have $F_{\hat\theta}(\bar x) = \inf F_{\hat\theta}$ for any $\bar x\in S$. Using this together with item (ii) and the definition of $E_{\hat\theta}$, one can show readily that whenever $\bar{x}\in S$
\begin{equation}\label{valueomega}
\bar{\omega} = E_{\hat{\theta}}(\bar{x}, \bar{x}) = F_{\hat{\theta}}(\bar{x}) = \inf_x F_{\hat{\theta}}(x) = \inf_{x,y} E_{\hat{\theta}}(x, y).
\end{equation}
Moreover, in view of \eqref{set} and the definition of $E_{\hat\theta}$, we have
\begin{equation}\label{set2}
\emptyset\not=\bar{\Lambda}\subseteq \bar{S}= \Argmin\limits_{x,y} E_{\hat{\theta}}(x, y).
\end{equation}
Using \eqref{valueomega}, \eqref{set2} and Lemma~\ref{KLinequ}, we deduce that there exist $\epsilon_0 >0$, $r_0>0$, and $c_0>0$ such that
\begin{equation}\label{erro}
\d((x, y), \bar{S})^2\leq c_0(E_{\hat{\theta}}(x, y) - \bar\omega),
\end{equation}
for any $(x, y)\in\dom \partial E_{\hat{\theta}}$ satisfying $\d((x, y), \bar{S})\leq \epsilon_0$ and $\bar\omega\leq E_{\hat{\theta}}(x, y) \leq \bar\omega + r_0$.

Next, notice that $\{(x^k, x^{k-1})\}\subset C\times C$ and $ \dom \partial E_{\hat{\theta}} = C\times C$. From Remark~\ref{rem43}, Theorem~\ref{subconver} and \eqref{set2}, we deduce that there exists $k_1>0$ such that
\begin{equation}\label{erro1}
\d((x^k, x^{k-1}), \bar{S})\leq\d((x^k, x^{k-1}), \bar{\Lambda})\leq\epsilon_0~~~~ \forall k\geq k_1.
\end{equation}
On the other hand, from Remark~\ref{rem43}, Theorem~\ref{alpha} and item (ii), we deduce the existence of $k_2>0$ such that
\begin{equation}\label{Ferro1}
\bar\omega\leq E_{\hat{\theta}}(x^k, x^{k-1})\leq\bar\omega + r_0 ~~~~ \forall k\geq k_2.
\end{equation}
Combining \eqref{erro}, \eqref{erro1} and \eqref{Ferro1}, we conclude that for any $k\geq k_3:=\max\{k_1, k_2\}$,
\begin{equation}\label{Eerro}
\d(x^{k}, S)^2\leq\d((x^k, x^{k-1}), \bar{S})^2\leq c_0(E_{\hat{\theta}}(x^k, x^{k-1}) - \bar\omega).
\end{equation}

Next, let $\bar{x}^k\in S$ satisfy $\|x^k - \bar{x}^k\| = \d(x^k, S)$. Then for any $k\geq N_0$ (note that $N_0$ is defined in Theorem~\ref{alpha}) and $\gamma\in(\frac{L_g c_0}{1 + L_gc_0}, 1)$, we have
\begin{align}\label{upperF}
&F_{\hat{\theta}}(x^{k+1})= \frac{1}{\hat{\theta}}\left(P_1(x^{k+1}) - \hat{m} + \hat{\theta}\max_{i = 1,\cdots,m}\left[g_i(x^{k+1})\right]_+\right) \nonumber\\
&\overset{\rm (a)}\leq \frac{1}{\hat{\theta}}\left(P_1(x^{k+1}) - \hat{m} + \hat{\theta}\max_{i = 1,\cdots,m}\left[\ling{i}{x^{k+1}}{y^k} + \frac{L_{g_i}}{2}\|x^{k+1} - y^k\|^2\right]_+\right) \nonumber\\
&\overset{\rm (b)}\leq\frac{1}{\hat{\theta}}\left(P_1(x^{k+1}) - \hat{m} + \hat{\theta}\max_{i = 1,\cdots,m}\left[\ling{i}{x^{k+1}}{y^k}\right]_+ + \frac{\hat{\theta}L_{g}}{2}\|x^{k+1} - y^k\|^2\right)\nonumber\\
&\overset{\rm (c)}\leq\frac{1}{\hat{\theta}}\left(P_1(\bar{x}^k) - \hat{m}\right) + \frac{L_g}{2}\|\bar{x}^k - y^k\|^2 - \frac{L_g}{2}\|\bar{x}^k - x^{k+1}\|^2 \nonumber\\
&\overset{\rm (d)}\leq F_{\hat{\theta}}(\bar{x}^k) + \frac{L_g}{2}\left(\|\bar{x}^k - x^k\| + \|x^k - y^k\|\right)^2 - \frac{L_g}{2}\|\bar{x}^k - x^{k+1}\|^2, \nonumber\\
&\overset{\rm(e)}\leq F_{\hat{\theta}}(\bar{x}^k)  + \frac{L_g}{2\gamma}\|\bar{x}^k - x^k\|^2 + \frac{L_g}{2(1 - \gamma)}\|x^k - y^k\|^2 - \frac{L_g}{2}\|\bar{x}^k - x^{k+1}\|^2\nonumber\\
&\overset{\rm(f)}\leq \bar\omega + \frac{L_g}{2\gamma}\d(x^k, S)^2 + \frac{L_g}{2(1 - \gamma)}\|x^k - y^k\|^2 - \frac{L_g}{2}\d(x^{k+1}, S)^2,
\end{align}
where (a) holds because of the Lipschitz continuity of $\nabla g_i$, (b) holds because $L_g = \max_{i = 1,\cdots,m}\{L_{g_i}\}$, (c) follows from \eqref{stongconve} with $x = \bar{x}^k$ (thanks to $\bar{x}^k\in S\subseteq C$) and $\ling{i}{\bar{x}^k}{y^k}\leq g_i(\bar{x}^k) \leq 0$ for each $i$ (thanks to the convexity of $g_i$ and $\bar{x}^k\in S \subseteq\mathcal{G}$), (d) holds because $g_i(\bar{x}^k)\leq 0$ for each $i$ (thanks to $\bar{x}^k\in S \subseteq\mathcal{G}$), and the triangle inequality, (e) follows from the fact that $(a + b)^2 = (\gamma\frac{a}{\gamma} + (1 - \gamma)\frac{b}{(1-\gamma)})^2\leq\frac{a^2}{\gamma} + \frac{b^2}{(1-\gamma)}$ as $\gamma\in(0,1)$, and (f) holds thanks to \eqref{valueomega} and the fact that $\bar{x}^k\in S$.

Then, we have for any $k\geq k_4:= \max\{k_3, N_0\}$ that
\begin{align*}%\label{upperE}
&E_{\hat{\theta}}(x^{k+1},x^k) - \bar\omega
= F_{\hat{\theta}}(x^{k+1}) - \bar\omega + \frac{L_g}{2}\|x^{k+1} - x^k\|^2 \\
&\overset{\rm(a)}\leq \frac{L_g}{2\gamma}\d(x^k, S)^2 + \frac{L_g}{2(1 - \gamma)}\|x^k - y^k\|^2 - \frac{L_g}{2\gamma}\d(x^{k+1}, S)^2  \nonumber\\
&~~~~+ \frac{L_g}{2}\left(\frac{1}{\gamma} - 1\right)\d(x^{k+1}, S)^2 + \frac{L_g}{2}\|x^{k+1} - x^k\|^2 \\
&\overset{\rm(b)}\leq \left(\frac{L_g}{2\gamma}\d(x^k, S)^2 - \frac{L_g}{2}\|x^k - x^{k-1}\|^2\right) - \left(\frac{L_g}{2\gamma}\d(x^{k+1}, S)^2 - \frac{L_g}{2}\|x^{k+1} - x^k\|^2\right) \nonumber\\
&~~~~+ \frac{L_g\bar{\beta}^2}{2(1 - \gamma)}\|x^k - x^{k-1}\|^2 + \frac{L_g}{2}\|x^k - x^{k-1}\|^2 + \frac{L_g}{2}\left(\frac{1}{\gamma} - 1\right)c_0(E_{\hat{\theta}}(x^{k+1}, x^{k}) - \bar\omega),
\end{align*}
where (a) holds because of \eqref{upperF}, and (b) follows from \eqref{Eerro}, $y^k = x^k + \beta_k(x^k - x^{k-1})$ and $\bar{\beta} = \sup_k\beta_k$.

Now, notice that $\gamma\in(\frac{L_g c_0}{1 + L_gc_0}, 1)$ implies $\frac{L_g}{2}(\frac{1}{\gamma} - 1)c_0 < \frac{1}{2}$. Letting $\vartheta: = 1 - \frac{L_g}{2}(\frac{1}{\gamma} - 1)c_0$, then we known that $\vartheta > \frac{1}{2}$. Rearranging terms in the above display inequality, we have that for any $k\ge k_4$,
\begin{align*}\label{upperE1}
&\vartheta \left(E_{\hat{\theta}}(x^{k+1},x^k) - \bar\omega\right) \\
& \leq \frac{L_g}{2}\left(\frac{1}{\gamma}\d(x^k, S)^2 - \|x^k - x^{k-1}\|^2\right) - \frac{L_g}{2}\left(\frac{1}{\gamma}\d(x^{k+1}, S)^2 -\|x^{k+1} - x^k\|^2\right) \nonumber\\
&~~~~+ \frac{L_g(1 - \gamma + \bar{\beta}^2)}{2(1 - \gamma)}\|x^k - x^{k-1}\|^2 \nonumber\\
&\overset{\rm(a)}\leq \frac{L_g}{2}\left(\frac{1}{\gamma}\d(x^k, S)^2 - \|x^k - x^{k-1}\|^2\right) - \frac{L_g}{2}\left(\frac{1}{\gamma}\d(x^{k+1}, S)^2 -\|x^{k+1} - x^k\|^2\right) \nonumber\\
&~~~~+ \frac{L_g(1 - \gamma + \bar{\beta}^2)}{2(1 - \gamma)}\cdot\frac{2}{L_g(1 - \bar{\beta}^2)}\left(E_{\hat{\theta}}(x^k, x^{k-1}) - E_{\hat{\theta}}(x^{k+1}, x^k)\right), \nonumber
\end{align*}
where (a) follows from \eqref{upperK}.

Denote $\zeta: = \frac{1 + \bar{\beta}^2 - \gamma}{(1-\gamma)(1 - \bar{\beta}^2)} > 1$ and $A_k := \frac{L_g}{2}(\frac{1}{\gamma}\d(x^k, S)^2 - \|x^k - x^{k-1}\|^2)$. Rearranging terms in the above inequality, we obtain that for any $k\ge k_4$,
\begin{equation*}
(\vartheta + \zeta) \left(E_{\hat{\theta}}(x^{k+1},x^k) - \bar\omega\right)  \leq A_k - A_{k+1} + \zeta\left(E_{\hat{\theta}}(x^{k},x^{k-1}) - \bar\omega\right).
\end{equation*}
Dividing $\vartheta + \zeta$ on both sides in the above inequality, we see that for any $k\ge k_4$,
\begin{equation}\label{upperE2}
E_{\hat{\theta}}(x^{k+1},x^k) - \bar\omega \leq \frac{\zeta}{\vartheta + \zeta}\left(E_{\hat{\theta}}(x^{k},x^{k-1}) - \bar\omega\right) + \frac{1}{\vartheta + \zeta}A_k - \frac{1}{\vartheta + \zeta}A_{k+1}.
\end{equation}
Since $\vartheta > \frac{1}{2}$ and $\zeta > 1$, we have that for any $k\ge k_4$,
\begin{align}\label{upperA}
&\left|\frac{A_k}{\vartheta + \zeta}\right|\leq \left|A_k\right|\leq \frac{L_g}{2}\left(\frac{1}{\gamma}\d(x^k, S)^2 + \|x^k - x^{k-1}\|^2\right) \nonumber\\
&\overset{\rm(a)}\leq\frac{L_g c_0}{2\gamma}\left(E_{\hat{\theta}}(x^k, x^{k-1}) - \bar\omega\right) + \frac{L_g}{2}\|x^k - x^{k-1}\|^2\nonumber\\
&\overset{\rm(b)}\leq\frac{L_g c_0}{2\gamma}\left(E_{\hat{\theta}}(x^k, x^{k-1}) - \bar\omega\right) + \frac{1}{(1 - \bar{\beta}^2)}\left(E_{\hat{\theta}}(x^k, x^{k-1}) - E_{\hat{\theta}}(x^{k+1}, x^{k})\right)\nonumber\\
&\overset{\rm(c)} \leq c_1\left(E_{\hat{\theta}}(x^k, x^{k-1}) - \bar\omega\right),
\end{align}
where (a) holds thanks to \eqref{Eerro}, (b) holds because of \eqref{upperK}, and (c) follows from $E_{\hat{\theta}}(x^{k+1}, x^{k})\geq\bar\omega$ (see \eqref{valueomega}) and $c_1:= \frac{L_g c_0}{2\gamma} + \frac{1}{(1 - \bar{\beta}^2)}$.

Let $\varrho = \frac{c_1 + \frac{\zeta}{\vartheta+\zeta}}{c_1+1}\in(0,1)$. Then one can see that
\begin{equation}\label{divi}
\frac{\zeta}{\vartheta+\zeta} + (1 - \varrho)c_1 = \varrho.
\end{equation}
Then, from \eqref{upperE2}, we obtain that for any $k\ge k_4$,
\begin{align*}
&E_{\hat{\theta}}(x^{k+1},x^k) - \bar\omega + \frac{1}{\vartheta + \zeta}A_{k+1} \leq \frac{\zeta}{\vartheta + \zeta}\left(E_{\hat{\theta}}(x^{k},x^{k-1}) - \bar\omega\right) + \frac{1}{\vartheta + \zeta}A_k \nonumber\\
&\overset{\rm(a)} \leq \frac{\zeta}{\vartheta + \zeta}\left(E_{\hat{\theta}}(x^{k},x^{k-1}) - \bar\omega\right) + \frac{\varrho}{\vartheta + \zeta}A_k + (1 - \varrho)\left|\frac{A_k}{\vartheta + \zeta}\right|\nonumber\\
&\overset{\rm(b)} \leq \left(\frac{\zeta}{\vartheta + \zeta} + (1 - \varrho)c_1\right)\left(E_{\hat{\theta}}(x^{k},x^{k-1}) - \bar\omega\right) + \frac{\varrho}{\vartheta + \zeta}A_k\nonumber\\
&\overset{\rm(c)} = \varrho \left(E_{\hat{\theta}}(x^{k},x^{k-1}) - \bar\omega + \frac{1}{\vartheta + \zeta}A_k \right),
\end{align*}
where (a) holds as $\varrho\in(0, 1)$, (b) follows from \eqref{upperA}, and (c) holds because of \eqref{divi}.

Inductively, since $\varrho > 0$, we see that for any $k\geq k_4$,
\begin{equation*}
E_{\hat{\theta}}(x^{k+1},x^k) - \bar\omega + \frac{1}{\vartheta + \zeta}A_{k+1} \leq \varrho^{k - k_4 +1}\left(E_{\hat{\theta}}(x^{k_4},x^{k_4 - 1}) - \bar\omega + \frac{1}{\vartheta + \zeta}A_{k_4}\right),
\end{equation*}
which means, there exists $M>0$ such that, for any $k\geq k_4$,
\begin{align}\label{inequ}
0&\leq E_{\hat{\theta}}(x^k,x^{k-1}) - \bar\omega\leq M\varrho^{k} - \frac{1}{\vartheta + \zeta}A_{k} \notag\\
&\overset{\rm(a)}= M\varrho^{k} \!-\! \frac{L_g}{2(\vartheta + \zeta)}\left(\!\frac{1}{\gamma}\d(x^k, S)^2 - \|x^k - x^{k-1}\|^2\!\right)\!\leq\! M\varrho^{k} \!+\! \frac{L_g}{2(\vartheta + \zeta)} \|x^k - x^{k-1}\|^2\notag\\
&\overset{\rm(b)}\leq M\varrho^{k} + \frac{1}{(\vartheta + \zeta)(1 - \bar{\beta}^2)}\left(E_{\hat{\theta}}(x^k,x^{k-1}) - E_{\hat{\theta}}(x^{k+1},x^{k})\right),
\end{align}
where (a) follows from the definition of $A_k$, and (b) holds because of \eqref{upperK}.

Taking $\mu > \max\{\frac{1}{(\vartheta + \zeta)(1 - \bar{\beta}^2)}, \frac{1}{1 - \varrho}\}$. From $\varrho\in(0, 1)$,  we see that
\begin{equation}\label{defmu}
\mu > 1 \text{ and } 1 - \mu^{-1} > \varrho,
\end{equation}
and from \eqref{inequ}, we have that
\begin{equation*}
E_{\hat{\theta}}(x^k,x^{k-1}) - \bar\omega \leq M\varrho^{k} + \mu(E_{\hat{\theta}}(x^k,x^{k-1}) - E_{\hat{\theta}}(x^{k+1},x^{k})),
\end{equation*}
which implies
\begin{equation*}
\mu(E_{\hat{\theta}}(x^{k+1},x^{k}) - \bar\omega \leq (\mu - 1)\left(E_{\hat{\theta}}(x^k,x^{k-1}) - \bar\omega\right) + M\varrho^{k}.
\end{equation*}
Dividing $\mu>0$ on the both sides in the above display, we see that for any $k\geq k_4$,
\begin{align*}
&E_{\hat{\theta}}(x^{k+1},x^{k}) - \bar\omega \leq \left(1 - \mu^{-1}\right) \left(E_{\hat{\theta}}(x^k,x^{k-1}) - \bar\omega\right) + \frac{M}{\mu}\varrho^{k} \nonumber\\
&= \left(1 - \mu^{-1}\right) \left(E_{\hat{\theta}}(x^k,x^{k-1}) - \bar\omega\right) + \frac{M}{\mu}\left( \frac{1 - \mu^{-1}}{1 - \mu^{-1} - \varrho} - \frac{\varrho}{1 - \mu^{-1} - \varrho}\right)\varrho^{k} \nonumber\\
&= \left(1 - \mu^{-1}\right) \left(E_{\hat{\theta}}(x^k,x^{k-1}) - \bar\omega + \frac{M}{\mu(1 - \mu^{-1} - \varrho)}\varrho^{k} \right) - \frac{M}{\mu(1 - \mu^{-1} - \varrho)}\varrho^{k+1}, \nonumber
\end{align*}
where the division by $1 - \mu^{-1} - \varrho$ is valid thanks to \eqref{defmu}.
Rearranging terms in the above display inequality, we have that for any $k\geq k_4$,
\begin{equation*}
E_{\hat{\theta}}(x^{k+1},x^{k}) - \bar\omega + \frac{M\varrho^{k+1}}{\mu(1 - \mu^{-1} - \varrho)}
\leq \left(1 - \mu^{-1}\right) \left(E_{\hat{\theta}}(x^k,x^{k-1}) - \bar\omega + \frac{M\varrho^{k}}{\mu(1 - \mu^{-1} - \varrho)} \right).
\end{equation*}
Inductively, since $1 - \mu^{-1} >0$ thanks to \eqref{defmu}, we see that for any $k\geq k_4$,
\begin{align}\label{inductE}
&\!\!\!E_{\hat{\theta}}(x^{k+1},x^{k}) - \bar\omega
 \leq
 E_{\hat{\theta}}(x^{k+1},x^{k}) - \bar\omega + \frac{M\varrho^{k+1}}{\mu(1 - \mu^{-1} - \varrho)} \nonumber\\
&\!\!\!\leq \left(\!1 - \mu^{-1}\!\right)^{k-k_4+1} \!\!\!\ \left(E_{\hat{\theta}}(x^{k_4},x^{k_4-1}) \!-\! \bar\omega \!+\! \frac{M\varrho^{k_4}}{\mu(1 - \mu^{-1} - \varrho)} \right) \overset{\rm(a)} = c_2 \left(\!1 - \mu^{-1}\!\right)^{k + 1},
\end{align}
where (a) holds with $c_2 := \left(1 - \mu^{-1}\right)^{-k_4}\left(E_{\hat{\theta}}(x^{k_4},x^{k_4-1}) - \bar\omega + \frac{M}{\mu(1 - \mu^{-1} - \varrho)}\varrho^{k_4} \right)> 0$.

Finally, combining \eqref{upperK} and \eqref{inductE}, we obtain that for any $k\geq k_4$,
\begin{align*}
\|x^{k+1} - x^k\|^2 & \overset{\rm(a)}\leq \frac{2}{L_g(1 - \bar{\beta})}\left(E_{\hat{\theta}}(x^{k+1}, x^k) - E_{\hat{\theta}}(x^{k}, x^{k-1})\right)\\
&\overset{\rm(b)}\leq\frac{2}{L_g(1 - \bar{\beta})} \left(E_{\hat{\theta}}(x^{k+1}, x^{k}) - \bar\omega\right)\overset{\rm(c)}\leq \frac{2c_2}{L_g(1 - \bar{\beta})}\left(1 - \mu^{-1}\right)^{k+1},
\end{align*}
where (a) holds because of \eqref{upperK}, (b) follows from $E_{\hat{\theta}}(x^{k+1}, x^k) \geq \bar\omega$ (see \eqref{valueomega}), and (c) holds because of \eqref{inductE}.
Consequently, for any $j$, $k\geq k_4$,
\begin{align}\label{bound}
\sum_{i = k}^{j}\|x^{i+1} - x^i\|&\leq \sum_{i = k}^{\infty} \sqrt{\frac{2c_2}{L_g(1 - \bar{\beta})}}\left(\sqrt{1 - \mu^{-1}}\right)^{i+1}= c_2 \left(\sqrt{1 - \mu^{-1}}\right)^{k+1},
\end{align}
with $c_2 := \sqrt{\frac{2c_2}{L_g(1 - \bar{\beta})}}\cdot\frac{1}{1 - \sqrt{1 - \mu^{-1}}} > 0$,
which implies that $\{x^k\}$ is a Cauchy sequence. Combining this with Remark~\ref{rem43} and Theorem~\ref{subconver}, we see that $\{x^k\}$ converges to a minimizer $x^*$ of \eqref{eq1}. The claimed linear convergence rate also follows immediately from \eqref{bound}.
\end{proof}

\section{KL exponent and exact penalty}\label{sec5}

In Section~\ref{sec42}, the KL exponent of the $F_{\hat\theta}$ in \eqref{definfalpha} was used for establishing the convergence rate of the $\{x^k\}$ generated by Algorithm~\ref{alg:Framwork} in the convex setting. In this section, we examine how the KL exponent of functions of the form \eqref{definfalpha} can be deduced from the corresponding problem \eqref{eq1}.

Specifically, we consider the following constrained optimization problem:
\begin{equation}\label{KLproblem}
\min_{x\in \R^n}\hat F(x):= P_1(x) + \delta_C(x) + \delta_{{\cal F}}(x),
\end{equation}
where $P_1:\R^n \to \R$ is convex, $C$ is compact and convex, ${\cal F} := \{x\in \R^n: g_i(x)\le 0, i = 1,\ldots,m\}$ with each $g_i:\mathbb{R}^n\to \R$ being convex, and $C\cap {\rm int}\,{\cal F} \neq \emptyset$; we also consider the associated penalty function
\begin{equation}\label{Feta}
\hat F_{\eta}(x): = P_1(x) + \delta_C(x) + \eta\max_i[g_i(x)]_+,
\end{equation}
where $\eta > 0$. Notice that the KL property of $\hat F_{\hat\theta}$ was the key for establishing the convergence rate of the $\{x^k\}$ generated by Algorithm~\ref{alg:Framwork}.

We next recall the definition of exact penalty parameter.
\begin{definition}[Exact penalty parameter]\label{Exact}
Consider \eqref{KLproblem} and \eqref{Feta}. If there exists $\bar{\eta} > 0$ such that for all $\eta \geq \bar{\eta}$,
\[
\Argmin_{x\in C}\{P_1(x) + \eta\max_{i = 1,\cdots,m}[g_i(x)]_+\} = \Argmin_{x\in C\cap\mathcal{F}}\{P_1(x)\}
\]
then $\bar{\eta}$ is called an exact penalty parameter of \eqref{KLproblem}.
\end{definition}
We will argue that the set of exact penalty parameters of \eqref{KLproblem} is nonvoid. We start by recalling the following well-known result, whose short proof is included for the convenience of the readers.
\begin{lemma}\label{distF}
  Let $C$ and ${\cal F}$ be as in \eqref{KLproblem}. Then there exist $\kappa>0$ and $\tau > 0$ such that
  \begin{equation}\label{eb2}
  \d(x,C\cap {\cal F})\le \kappa \d(x,{\cal F}) \le \tau \max_{i=1,\ldots,m}[g_i(x)]_+\ \ \ \forall x\in C.
  \end{equation}
\end{lemma}
\begin{proof}
  First, since $C\cap {\rm int}\,{\cal F}\neq \emptyset$ (say, it contains $\hat x$), we deduce from \cite[Corollary~3]{BauschkeBorweinLi99} that there exists $\kappa > 0$ such that
  \begin{equation}\label{eb1}
  \d(x,C\cap {\cal F}) \le \kappa \d(x,{\cal F})\ \ \ \ \ \forall x\in C.
  \end{equation}
  Moreover, $\hat x \in {\rm int}\,{\cal F}$ implies that $\max_{i = 1,\ldots,m}g_i(\hat x) < 0$.
  We then apply Lemma~\ref{RobEB} with $\Omega:=\mathcal{F}$, $g(x) = (g_1(x), g_2(x), \cdots, g_m(x))$, $x^s = \hat{x}$ and $\delta_0 = \left|\max_i{g_i(\hat{x})}\right|$ to obtain
\begin{equation*}
\d(x, \mathcal{F}) \leq \frac{\|x - \hat{x}\|}{\left|\max_i{g_i(\hat{x})}\right|}\d(0, g(x) + \R^m_+) ~~~~ \forall x\in \R^n.
\end{equation*}
Since $C$ is compact, we deduce further that there exists $M_1> 0 $ such that
\begin{equation}\label{upperxF}
\d(x, \mathcal{F}) \leq M_1\max_{i = 1,\cdots,m}[g_i(x)]_+ ~~ \forall x\in C.
\end{equation}
The desired conclusion now follows upon combining \eqref{eb1} and \eqref{upperxF}.
\end{proof}
\begin{remark}[Nonemptiness of the set of exact penalty parameters]\label{rem51}
Since $C\cap {\cal F}$ is compact and $P_1$ is continuous, we see that $\Argmin \hat F\neq \emptyset$. Using this together with \eqref{eb2}, we can now deduce from \cite[Lemma~3.1]{ChenLuPong16} that any $\eta > \bar L_{P_1}\tau$ is an exact penalty parameter of \eqref{KLproblem}, where $\bar L_{P_1}$ is a Lipschitz continuity modulus for $P_1$ on the compact convex set $C$.
\end{remark}

Now, we show that if the $\hat F$ in \eqref{KLproblem} is KL function with exponent $\alpha$, then for any $\eta>\bar{\eta}$,
the $\hat F_{\eta}$ is a KL function with exponent $\alpha$, where $\bar{\eta}$ is an exact penalty parameter of \eqref{KLproblem}.

\begin{theorem}[KL exponent of $\hat F_{\eta}$ from that of $\hat F$]\label{FetaKL}
Let $\hat F$ be as in \eqref{KLproblem}, $\bar{x}\in\Argmin \hat F$ and $\bar{\eta}$ be an exact penalty parameter of \eqref{KLproblem}. If $\hat F$ satisfies the KL property with exponent $\alpha$ at $\bar{x}$, then for any $\eta>\bar{\eta}$, the $\hat F_{\eta}$ defined in \eqref{Feta} satisfies the KL property with exponent $\alpha$ at $\bar{x}$.
\end{theorem}

\begin{proof}
Fix any $\eta > \bar\eta$. Since $\bar{\eta}$ is an exact penalty parameter of \eqref{KLproblem}, we see that $\Argmin \hat F = \Argmin \hat F_{\eta}$; also, note that $\dom\partial \hat F_{\eta} = C$.

Since $\hat F$ satisfies the KL property with exponent $\alpha$ at $\bar{x}$ and $C$ is compact, by Lemma~\ref{KLinequ}, there exist $c>0$ and $a$, $\epsilon\in (0,1)$ such that
\begin{equation}\label{FKL}
\d(x, \Argmin \hat F) \leq c(\hat F(x) - \hat F(\bar{x}))^{\alpha},
\end{equation}
whenever $ x\in\dom \partial \hat F = C\cap \mathcal{F}$ satisfies $\|x - \bar{x}\|\leq \epsilon$ %$\d(x, \Argmin F)\leq\epsilon$
and $\hat F(\bar{x}) \leq \hat F(x) \leq \hat F(\bar{x}) + a$.

Next, since $P_1:\R^n\to\R$ is convex, we know that $P_1$ is locally Lipschitz continuous at $\bar{x}$. Hence, there exist $\bar{\epsilon} > 0$ and $\hat L_{P_1}>0$ such that
\begin{equation}\label{LipschP}
|P_1(x) - P_1(y)|\leq \hat L_{P_1}\|x - y\| ~~~~\forall x,y\in B(\bar{x}, \bar{\epsilon}).
\end{equation}
Take $\epsilon_0 := \min\{\epsilon, \bar{\epsilon}, \frac{a}{3\hat L_{P_1}}\}$. Then for any $x\in C$ satisfying $\|x - \bar{x}\|\leq \epsilon_0$, we have
\begin{align}\label{upperproje}
|P_1(\Pi_{C\cap \mathcal{F}}(x)) - P_1(x)| &\leq |P_1(\Pi_{C\cap \mathcal{F}}(x)) - P_1(\bar{x})| + |P_1(\bar{x}) - P_1(x)| \nonumber\\
&\overset{\rm(a)}\leq \hat L_{P_1}\|\Pi_{C\cap \mathcal{F}}(x) - \bar{x}\| + \hat L_{P_1}\|x - \bar{x}\| \leq 2\epsilon_0 \hat L_{P_1},
\end{align}
where $\Pi_{C\cap\mathcal{F}}(x)$ denotes the orthogonal projection of the point $x$ onto $C\cap\mathcal{F}$, (a) follows from \eqref{LipschP} and the fact that $\|\Pi_{C\cap\mathcal{F}}(x) - \bar{x}\|\leq\|x - \bar{x}\|\leq \epsilon_0$ (thanks to $\bar{x}\in C\cap\mathcal{F}$ and the nonexpansiveness of the projection mapping).

Now, let $a_0 := a - 2\epsilon_0 \hat L_{P_1}>0$. Then for any $x\in C = \dom\partial \hat F_{\eta}$ satisfying $\hat F_{\eta}(\bar{x}) \leq \hat F_{\eta}(x) \leq \hat F_{\eta}(\bar{x}) + a_0$, we claim that
\begin{equation}\label{projecx}
\hat F(\bar{x}) \leq \hat F(\Pi_{C\cap \mathcal{F}}(x)) \leq \hat F(\bar{x}) + a.
\end{equation}
Indeed, from $\bar{x}\in\Argmin \hat F$, one can see that $\hat F(\bar{x}) \leq \hat F(\Pi_{C\cap \mathcal{F}}(x))$. At the same time, notice that
\[
P_1(x) \leq P_1(x) + \eta\max_i[g_i(x)]_+ = \hat F_{\eta}(x)\leq \hat F_{\eta}(\bar{x}) + a_0 = P_1(\bar{x}) + a - 2\epsilon_0 \hat L_{P_1}.
\]
Combining this with \eqref{upperproje}, one can see that $P_1(\Pi_{C\cap \mathcal{F}}(x))\leq P_1(\bar{x}) + a$. Combining this with the definition of $\hat F$, we conclude that \eqref{projecx} holds.

Hence, for any $x\in C = \dom\partial \hat F_{\eta}$ satisfying $\|x - \bar{x}\|\leq \epsilon_0$ and $\hat F_{\eta}(\bar{x}) \leq \hat F_{\eta}(x) \leq \hat F_{\eta}(\bar{x}) + a_0$, we have that
\begin{align}\label{distFeta}
&\d(x, \Argmin \hat F_{\eta})\leq \d(\Pi_{C\cap \mathcal{F}} (x), \Argmin \hat F_{\eta}) + \d(x, C\cap \mathcal{F}) \notag\\
&\overset{\rm(a)} = \d(\Pi_{C\cap \mathcal{F}} (x), \Argmin F) + \d(x, C\cap \mathcal{F}) \notag\\
&\overset{\rm(b)}\leq c(\hat F(\Pi_{C\cap \mathcal{F}} (x)) - \hat F(\bar{x}))^{\alpha} + \kappa\d(x, \mathcal{F}) = c(P_1(\Pi_{C\cap \mathcal{F}} (x)) - P_1(\bar{x}))^{\alpha} + \kappa\d(x, \mathcal{F}) \notag\\
&\overset{\rm(c)}\leq c(P_1(x) - P_1(\bar{x}) + \hat L_{P_1}\d(x, C\cap \mathcal{F}))^{\alpha} + \kappa\d(x, \mathcal{F}) \notag\\
&\overset{\rm(d)}\leq c(P_1(x) - P_1(\bar{x}) + \hat L_{P_1}\kappa\d(x, \mathcal{F}))^{\alpha} + \kappa\d(x, \mathcal{F})^{\alpha} \notag\\
& \overset{\rm (e)}\le\hat c \big[(P_1(x) - P_1(\bar{x}) + \hat L_{P_1}\kappa\d(x, \mathcal{F}))^{\alpha} + [\kappa^{1/\alpha}\d(x, \mathcal{F})]^{\alpha} \big]\notag\\
&\overset{\rm(f)}\leq \bar{c}(P_1(x) \!-\! P_1(\bar{x}) + \kappa_1\d(x, \mathcal{F}))^{\alpha}
\!\overset{\rm(g)}\leq\! \bar{c}\big(P_1(x) \!-\! P_1(\bar{x}) + \kappa_2\!\max_{i = 1,\cdots,m}\![g_i(x)]_+\big)^{\alpha},
\end{align}
where (a) holds because $\Argmin \hat F = \Argmin \hat F_{\eta}$, (b) holds because of \eqref{FKL}, \eqref{projecx} and the fact that $\|\Pi_{C\cap\mathcal{F}}(x) - \bar{x}\|\leq\|x - \bar{x}\|\leq\epsilon_0\le\epsilon$ (thanks to $\bar{x}\in C\cap\mathcal{F}$ and the projection mapping being nonexpansive), (c) follows from \eqref{LipschP} and the fact that $\epsilon_0 \le \bar\epsilon$, (d) follows from \eqref{eb2} and the facts that $\d(x,\mathcal{F})\leq \|x - \bar{x}\|\leq\epsilon_0\le \epsilon<1$ and $0\leq \alpha \leq1$, (e) holds with $\hat{c} = \max\{c, 1\}$, (f) holds with $\bar c = 2^{1-\alpha}\hat c$ and $\kappa_1 = \hat L_{P_1}\kappa + \kappa^{\frac{1}{\alpha}}$ thanks to the fact that $a^{\alpha} + b^{\alpha}\leq 2^{1 - \alpha}(a + b)^{\alpha}$ for any $a$, $b\ge0$ and $\alpha\in (0,1)$, and (g) holds with $\kappa_2 = \kappa_1\tau/\kappa$ thanks to \eqref{eb2}.

Now, if $\eta\geq\kappa_2$, then, from \eqref{distFeta}, we have that
\[
\d(x, \Argmin \hat F_{\eta})\leq \bar{c}\left(P_1(x) - P_1(\bar{x}) + \eta\max_{i = 1,\cdots,m}[g_i(x)]_+\right)^{\alpha}.
\]
On the other hand, if $\kappa_2 >\eta>\bar{\eta}$, then, from \eqref{distFeta}, we obtain that
\begin{align*}
&\d(x, \Argmin \hat F_{\eta})\leq \bar{c}\left(P_1(x) \!-\! P_1(\bar{x}) \!+\! \bar{\eta}\max_{i = 1,\cdots,m}[g_i(x)]_+ \!+\! (\kappa_2 - \bar{\eta})\max_{i = 1,\cdots,m}[g_i(x)]_+\right)^{\alpha}\\
&\overset{\rm(a)} \leq \bar{c}\left(\frac{\kappa_2 - \bar{\eta}}{\eta - \bar{\eta}}\right)^{\alpha}\left(P_1(x) - P_1(\bar{x}) + \bar{\eta}\max_{i = 1,\cdots,m}[g_i(x)]_+ + (\eta - \bar{\eta}) \max_{i = 1,\cdots,m}[g_i(x)]_+\right)^{\alpha}\\
&= \bar{c}\left(\frac{\kappa_2 - \bar{\eta}}{\eta - \bar{\eta}}\right)^{\alpha}\left(\hat F_{\eta}(x) - \hat F_{\eta}(\bar{x})\right)^{\alpha},
\end{align*}
where (a) holds because $a + b\leq\frac{1}{\epsilon}(a + \epsilon b)$ for any $a\geq 0$, $b \geq 0$ and $0<\epsilon\leq 1$.\footnote{We apply this relation to $\epsilon := (\eta-\bar\eta)/(\kappa_2 - \bar\eta)\in (0,1)$, $a:= P_1(x) - P_1(\bar{x}) + \bar{\eta}\max_{i = 1,\cdots,m}[g_i(x)]_+$, which is nonnegative because $\bar{\eta}$ is an exact penalty parameter, $\bar{x}\in\Argmin F$ and $x\in C$, and $b:= (\kappa_2 - \bar{\eta})\max_{i = 1,\cdots,m} [g_i(x)]_+ \geq 0$.} The desired conclusion now follows immediately.
\end{proof}

In the next remark, we comment on the condition $\eta > \bar\eta$ required in Theorem~\ref{FetaKL}.
\begin{remark}[On the condition $\eta > \bar \eta$ in Theorem~\ref{FetaKL}]\label{rem52}
  We comment on the applicability of Theorem~\ref{FetaKL}, which only infers the KL exponent of $\hat F_\eta$ when $\eta > \bar \eta$ for some exact penalty parameter $\bar\eta$.

  Particularly, we consider \eqref{eq1}. Suppose that Assumptions~\ref{B1} and \ref{B2} hold and let $\{\theta_k\}$ be generated by Algorithm~\ref{alg:Framwork} with $\sup_k\beta_k < 1$. Then we deduce by combining \eqref{conditionunhappy} and Lemma~\ref{lem43} that the set of exact penalty parameters is nonempty; indeed, it contains the interval $(\hat\theta,\infty)$. Hence, if we let $\tilde \eta$ denote the infimum of the set of exact penalty parameters, then $\hat\theta\ge \tilde\eta$.

  Now, note that we have $\theta_k \equiv \hat\theta$ whenever $k\ge N_0$ (where $N_0$ is defined in Theorem~\ref{alpha}) and $\theta_k$ is strictly increasing when $k \le N_0$. Intuitively, it is likely that the update rule of $\theta_k$ will result in $\hat\theta > \tilde\eta$. In this case, \eqref{assumptionunhappy} holds (thanks to Lemma~\ref{lem43}) and Theorem~\ref{FetaKL} asserts that the KL property required in Theorem~\ref{Edecres}(iii) can be inferred from that of $P_1+\delta_C+\delta_{\mathscr{F}}$ in \eqref{eq1}. On the other hand, in the case $\hat\theta = \tilde\eta$, it is unclear whether \eqref{assumptionunhappy} holds and Theorem~\ref{Edecres}(iii) may not be applicable.
\end{remark}

\begin{example}\label{RemarkKL}
Suppose that in \eqref{eq1}, $P_1 = \|\cdot\|_1$, $C$ is a polyhedron, $m = 1$, and $g_1 = q_1\circ A_1$ for some matrix $A_1\in \R^{s_1\times n}$ and $q_1:\mathbb{R}^{s_1} \rightarrow \mathbb{R}$ takes one of the following forms with $b\in \R^{s_1}$ and $\sigma > 0$ chosen so that the origin is not feasible and that Assumption~\ref{B2} holds:
\begin{enumerate}[{\rm (i)}]
   \item (Basis pursuit denoising \cite{Ca18}) $q_1(z) = \frac{1}{2}\|z - b\|^2 - \sigma$.
   \item (Logistic loss \cite{HoLS13}) $q_1(z) = \sum_{i=1}^{s_1}\log(1 + \exp(b_iz_i)) - \sigma$ for some $b\in \{-1,1\}^{s_1}$.
 \end{enumerate}
 Let $\bar{\eta}$ be the exact penalty parameter of \eqref{KLproblem}. We deduce from \cite[Corollary~5.1]{li18}, \cite[Corollary~5.1]{zhang23} and Theorem~\ref{FetaKL} that, for any $\eta>\bar{\eta}$, the KL exponent of the corresponding $\hat F_{\eta}$ is $\frac{1}{2}$.
\end{example}




\section{Numerical experiments}

In this section, we perform numerical experiments to illustrate the performance of Algorithm~\ref{alg:Framwork}. Particularly, we consider the following model for compressed sensing:
\begin{equation}\label{CompSen}
\begin{aligned}
\min_{x\in\R^n}\quad &\|x\|_1 - \mu\|x\| \\
{\rm s.t.} \quad & h(Ax - b)\leq \sigma,
\end{aligned}
\end{equation}
where $\mu\in[0, 1]$, $A\in\R^{q\times n}$ has full row rank, $b\in \R^q$, $h: \R^q\rightarrow\R_+$ is an analytic function whose gradient is Lipschitz continuous with modulus $L_{h}$ and satisfies $h(0) = 0$, and $\sigma\in (0, h(-b))$.

Although the feasible region of \eqref{CompSen} is unbounded and Algorithm~\ref{alg:Framwork} cannot be directly applied to solving \eqref{CompSen}, one can argue as in the discussion following \cite[(6.2)]{zhang23} that \eqref{CompSen} is equivalent to the following model:
\begin{equation}\label{CompSen1}
\begin{aligned}
\min_{x\in\R^n}\quad &\|x\|_1 - \mu\|x\| \\
{\rm s.t.} \quad & h(Ax - b) \leq \sigma,\\
           \quad & \|x\|_{\infty} \leq M,
\end{aligned}
\end{equation}
where $M: = (1 - \mu)^{-1}\Big(\|A^\dagger b\|_1 - \mu\|A^\dagger b\|\Big)$. Notice that the equivalent problem \eqref{CompSen1} is a special case of \eqref{eq1} with $P_1(x): = \|x\|_1$, $P_2(x) := \mu\|x\|$, $g(x) := h(Ax - b)$ and $C: = \{x: \|x\|_{\infty}\leq M\}$. Since $A$ has full row rank and $h(0) = 0< \sigma$, we see that $A^\dagger b \in C\cap \{x: g(x) < 0 \}\not=\emptyset$.

Next, we will focus on \eqref{CompSen1} and consider two specific choices of $h$.

\subsection{$h(\cdot) = \frac{1}{2}\|\cdot\|^2$}
In this subsection, we take $h(\cdot) = \frac{1}{2}\|\cdot\|^2$, then \eqref{CompSen1} becomes
\begin{equation}\label{CompSen1-1}
\begin{aligned}
\min_{x\in\R^n}\quad &\|x\|_1 - \mu\|x\| \\
{\rm s.t.} \quad & 0.5\cdot\|Ax - b\|^2 \leq \sigma,\\
           \quad & \|x\|_{\infty} \leq M.
\end{aligned}
\end{equation}
%where $M: = (1 - \mu)^{-1}\Big(\|A^{\dag}b\|_1 - \mu\|A^{\dag}b\|\Big)$.
Notice that $h$ is convex, the Slater condition holds for the feasible region of \eqref{CompSen1-1}, and the origin is not feasible as $\sigma\in(0, \frac{1}{2}\|b\|^2)$. Then Assumptions~\ref{A1} and \ref{A2} hold. One can then apply Theorem~\ref{th2.2} with $\ell_g = 0$ to deduce the convergence of the (whole) sequence $\{x^k\}$ generated by Algorithm~\ref{alg:Framwork} with $\sup_k\beta_k < 1$ for solving \eqref{CompSen1-1}.\footnote{Though we are not considering $\mu = 0$ in our experiments below, we also point out that when $\mu = 0$ in \eqref{CompSen1-1}, thanks to $\sigma\in(0, \frac{1}{2}\|b\|^2)$ and the assumption that $A$ has full row rank, we can deduce from Example~\ref{RemarkKL} that $x\rightarrow \|x\|_1 + \delta_C(x) + \eta\max_{i = 1,\cdots,m} [g_i(x)]_+$ is a KL function with exponent $\frac{1}{2}$ whenever $\eta$ exceeds some exact penalty parameter. Therefore, according to Remark~\ref{rem52}, for the sequence $\{(x^k,\theta_k)\}$ generated by Algorithm~\ref{alg:Framwork} with $\sup_k\beta_k < 1$, if $\hat\theta$ exceeds some exact penalty parameter, then $\{x^k\}$ converges locally linearly thanks to Theorem~\ref{Edecres}(iii).}

We compare SCP$_{\rm ls}$ in \cite{yu21}, ESQM (i.e., setting $\beta_k \equiv 0$ in Algorithm~\ref{alg:Framwork}) and ESQM$_e$ (i.e., Algorithm~\ref{alg:Framwork}). We use the same parameter settings for SCP$_{\rm ls}$ in \cite{yu21}, and the initial point of SCP$_{\rm ls}$ is chosen as $x^0 = A^\dagger b$. In ESQM and ESQM$_e$, we take $L_g = \|A\|^2$, $\ell_g = 0$, $d = 1$ and $\theta_0 = 1$, and the initial points are chosen as $x^0 = 0$. We terminate all algorithms when $\|x^{k+1} - x^k\|< 10^{-8}\max\{1, \|x^{k+1}\|\}$. The subproblems in these algorithms are solved according to the procedures described in the appendices of \cite{yu21} and \cite{zhang23}.

We use the same strategy of choosing $\beta_k$ as in the FISTA with fixed and adaptive restart described in \cite{DonoghueCandes15} for setting $\{\beta_k\}$. In more detail, we set the initial values $\vartheta_{-1}=\vartheta_{0}=1$ and define, for $k\geq 0$, that
\begin{align}\label{beta}
\beta_k = \frac{\vartheta_{k-1}-1}{\vartheta_{k}} \ \ {\rm with}\ \ \vartheta_{k+1}=\frac{1+\sqrt{1+4\vartheta_{k}^2}}{2},
\end{align}
and we reset $\vartheta_{k-1}=\vartheta_{k}=1$ every $K = 200$ iterations or when $\langle y^{k-1} - x^k, x^k - x^{k-1} \rangle>0$. One can show that $\{\beta_k\}$ generated this way satisfies $\{\beta_k\}\subseteq[0,1)$ and $\sup_{k}\beta_k<1$.

We perform tests on random instances of \eqref{CompSen1-1}. Specifically, we generate an $A\in\R^{q\times n}$ with independent and identically distributed (i.i.d.) standard Gaussian entries, and then normalize this matrix so that each column of it has unit norm. Then we choose a subset $T$ of size $k$ uniformly at random from $\{1, 2, \cdots, n\}$ and a $k$-sparse vector $x_{\rm orig}$ having i.i.d. standard Gaussian entries on $T$ is generated. We let $b = Ax_{\rm orig} + 0.01\cdot\hat{n}$ with $\hat{n}$ being a random vector having i.i.d. standard Gaussian entries, and $\sigma = 0.5\sigma_1^2$ with $\sigma_1 = 1.1\cdot\|0.01\cdot\hat{n}\|$.

In our numerical tests, we let $\mu =0.95$ in \eqref{CompSen1-1} and $(p,n,k) = (720i,2560i,80i)$ with $i\in \{2, 4, 6, 8, 10\}$. For each $i$, we generate 20 random data as described above. We present the computational results in Table~\ref{table1}, averaged over the $20$ random instances. Here, we present the time for computing the QR decomposition of $A^T$ (denoted by $t_{\rm QR}$), the time for computing $\|A\|$ (denoted by $t_{\|A\|}$),  the time for computing $x^0 = A^\dagger b$ given the QR factorization of $A^T$ (denoted by $t_{A^\dagger b}$),\footnote{Note that $A^\dagger b$ is only used by SCP$_{\rm ls}$.} the CPU times of the algorithms (CPU time), the number of iterations (denoted by Iter), the recovery errors $\text{RecErr} := \frac{\|x^* - \xorig\|}{\max\{1, \|\xorig\|\}}$ and the residuals ${\rm Residual} := \frac{\|Ax^* - b\| - \sigma }{\sigma}$, where $x^*$ is the approximate solution returned by the respective algorithm.


\begin{table}[h]
{\color{black}
\caption{Computational results for problem \eqref{CompSen1-1}}\label{table1}
\begin{center}
{\footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|}\hline
\phantom{\diagbox{Date}{$i$}} & \multicolumn{1}{|c|}{Method} & \multicolumn{1}{|c|}{$i = 2$} & \multicolumn{1}{c|}{ $i = 4$ }
& \multicolumn{1}{c|}{ $i = 6$ } & \multicolumn{1}{|c|}{ $i = 8$ } & \multicolumn{1}{c|}{ $i = 10$ }
\\\cline{1-7}\multirow{6}*{CPU time} & \multirow{1}*{$t_{\rm QR}$}
&  0.727 &  4.977 & 17.050 & 41.958 & 97.176        \\\cline{2-2} \multirow{1}*{} & \multirow{1}*{$t_{A^\dagger b}$}
&  0.007 &  0.030 &  0.069 &  0.123 &  0.252        \\\cline{2-2} \multirow{1}*{} & \multirow{1}*{$t_{\|A\|}$}
&  0.734 &  1.693 &  5.871 & 13.707 & 30.315        \\\cline{2-7} \multirow{1}*{} & \multirow{1}*{SCP$_{\rm ls}$}
&  1.784 &  5.817 & 12.781 & 21.565 & 44.421        \\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM}
&  5.552 & 20.651 & 47.041 & 79.073 & 161.768        \\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm e}}$}
&  0.759 &  2.715 &  6.345 & 10.671 & 22.689        \\\cline{1-7} \multirow{3}*{Iter} & \multirow{1}*{SCP$_{\rm ls}$}
&    138 &    140 &    137 &    137 &    140        \\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM}
&    753 &    819 &    805 &    806 &    804        \\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm e}}$}
&    112 &    112 &    114 &    113 &    114        \\\cline{1-7} \multirow{3}*{RecErr} & \multirow{1}*{SCP$_{\rm ls}$}
&  0.017 &  0.017 &  0.017 &  0.017 &  0.018       \\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM}
&  0.017 &  0.017 &  0.017 &  0.017 &  0.018       \\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm e}}$}
&  0.017 &  0.017 &  0.017 &  0.017 &  0.018       \\\cline{1-7} \multirow{3}*{Residual} & \multirow{1}*{SCP$_{\rm ls}$}
& -9.28e-14 & -3.25e-13 & -2.31e-13 & -2.69e-13 & -2.15e-13        \\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM}
& -3.52e-09 & -1.72e-09 & -1.14e-09 & -8.59e-10 & -6.76e-10        \\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm e}}$}
& -8.26e-10 & -1.18e-09 & -6.44e-10 & -7.86e-10 & -4.78e-10        \\\cline{1-7}
\end{tabular}
}
\end{center}
}
\end{table}

From Table~\ref{table1}, one can see that ESQM$_{{\rm e}}$ is the fastest, and the recovery errors of all three methods are comparable.

\subsection{When $h$ is the Lorentzian norm}

In this subsection, we consider $h$ is the Lorentzian norm \cite{CarrBarAys10}, which is defined as follows for any given $\gamma>0$:
\[
\|y\|_{L L_2, \gamma}:=\sum_{i=1}^q \log\left(1 + \frac{y_i^2}{\gamma^2}\right).
\]
Then, problem~\eqref{CompSen1} becomes the following problem:
\begin{equation}\label{CompSen1-2}
\begin{aligned}
\min_{x\in\R^n}\quad & \|x\|_1 - \mu\|x\| \\
{\rm s.t.}\quad & \|Ax - b\|_{L L_2, \gamma}\leq \sigma,\\
\quad & \|x\|_{\infty} \le M.
\end{aligned}
\end{equation}
We first argue that Assumption~\ref{A1} holds for \eqref{CompSen1-2} under our assumptions on $A$ and $\sigma$ in \eqref{CompSen1}. To this end, let $\hat h(y) := \|y\|_{L L_2, \gamma} - \sigma$ for notational simplicity. Then \eqref{CompSen1-2} is an instance of \eqref{eq1} with $g_1(x) = \hat h(Ax - b) - \sigma$ and $C := \{x:\; \|x\|_\infty \le M\}$. Now, recall that $A^\dagger b\in C$ by construction. Moreover, observe that for any $x\in C$, we have
\begin{align}\label{RCQexpression}
&\langle \nabla g_1(x), A^\dagger b - x\rangle = \langle A^T\nabla h(Ax-b), A^\dagger b - x\rangle =  \langle \nabla h(Ax-b), b - Ax\rangle\notag\\
& = 2\sum_{i=1}^q\frac{a_i^Tx - b_i}{\gamma^2 + (a_i^Tx - b_i)^2}\cdot (b_i - a_i^Tx)
= -2\sum_{i=1}^q\frac{(a_i^Tx - b_i)^2}{\gamma^2 + (a_i^Tx - b_i)^2},
\end{align}
where $a_i^T$ is the $i$-th row of $A$. Now, suppose that $I(x)$ is nonempty, i.e., $I(x) = \{1\}$. If there exists $u_1$ such that \eqref{A11} holds, then we see from \eqref{RCQexpression} that $a_i^Tx = b_i$ for all $i$. But then $g_1(x) = \hat h(0) - \sigma = -\sigma < 0$, contradicting to the assumption that $I(x) = \{1\}$. This together with Remark~\ref{remarkRCQ}(i) shows that Assumption~\ref{A1} holds.

Next, observe that the $\hat h$ has Lipschitz continuous gradient with modulus $\frac{2}{\gamma^2}$. The following proposition shows that $\hat h$ can be represented as the difference of two convex functions $\hat h_1$ and $\hat h_2$ with Lipschitz continuous gradients, and the Lipschitz continuity modulus of $\nabla \hat h_1$ is $\frac{2}{\gamma^2}$ while that of $\nabla \hat h_2$ is $\frac{1}{4\gamma^2}$.
\begin{proposition}
Let $\hat h(y): = \|y\|_{L L_2, \gamma} - \sigma$. Then there exist two convex functions $\hat h_1$ and $\hat h_2$ with Lipschitz continuous gradients, such that, $\hat h(y) = \hat h_1(y) - \hat h_2(y)$ and the Lipschitz continuity modulus of $\nabla \hat h_1$ is $\frac{2}{\gamma^2}$ while that of $\nabla \hat h_2$ is $\frac{1}{4\gamma^2}$.
\end{proposition}

\begin{proof}
For any $t\in \R$,
\[
\frac{d^2}{dt^2}\log(1 + t^2) = \frac{2(1 - t^2)}{(1 + t^2)^2} = \left[\frac{2(1 - t^2)}{(1+t^2)^2}\right]_+ - \left[\frac{2(1 - t^2)}{(1+t^2)^2}\right]_-.
\]
Let
\[
r_1(t) =\int_0^t (t-s)\left[\frac{2(1 - s^2)}{(1+s^2)^2}\right]_+ds{\text{~ and ~}} r_2(t) =\int_0^t (t-s)\left[\frac{2(1 - s^2)}{(1+s^2)^2}\right]_-ds.
\]
Then we have
\[
r_1''(t) = \left[\frac{2(1 - t^2)}{(1 + t^2)^2}\right]_+ {\text{~ and  ~}}  r_2''(t) = \left[\frac{2(1 - t^2)}{(1 + t^2)^2}\right]_-.
\]
Taking
\[
\hat h_1(y) = \sum_{i = 1}^m r_1(y_i) - \sigma, \text{~  and  ~} \hat h_2(y) = \sum_{i = 1}^m r_2(y_i),
\]
one can see that $\hat h_1$ and $\hat h_2$ are two convex functions with Lipschitz continuous gradients, and $\hat h(y) = \hat h_1(y) - \hat h_2(y)$. Furthermore, the continuity modulus of Lipschitz continuous gradients of $\hat h_1$ and $\hat h_2$ are $\sup_t |r''(t)| = \frac{2}{\gamma^2}$ and $\sup_{t}|r_2''(t)| = \frac{1}{4\gamma^2}$, respectively.
\end{proof}
In view of the above discussions, one can apply Theorem~\ref{th2.2} with $L_g = \frac{2\|A\|^2}{\gamma^2}$ and $\ell_g = \frac{\|A\|^2}{4\gamma^2}$ to deducing the convergence of the $\{x^k\}$ generated by Algorithm~\ref{alg:Framwork} with $\sup_{k}\beta_k < \sqrt{\frac{L_g}{L_g + \ell_g}} = \sqrt{\frac89}$ for solving \eqref{CompSen1-2}.

As in the previous subsection, we compare SCP$_{\rm ls}$ in \cite{yu21}, ESQM (i.e., setting $\beta_k \equiv 0$ in Algorithm~\ref{alg:Framwork}) and ESQM$_e$ (i.e., Algorithm~\ref{alg:Framwork}). We use the same parameter settings for SCP$_{\rm ls}$ in \cite{yu21}, and the initial point of SCP$_{\rm ls}$ is chosen as $x^0 = A^\dagger b$. In ESQM and ESQM$_e$, we take $L_g = \frac{2\|A\|}{\gamma^2}$, $\ell_g = \frac{\|A\|}{4\gamma^2}$, $d = \frac{\gamma^2}{20}$ and $\theta_0 = 0.05$, and the initial point are chosen as $x^0 = 0$. We terminate all algorithms when $\|x^{k+1} - x^k\|< 10^{-8}\max\{1, \|x^{k+1}\|\}$. Furthermore, the subproblems in these algorithms are solved according to the procedures described in the appendices of \cite{yu21} and \cite{zhang23}.

We also choose $\{\beta_k\}$ as described in \eqref{beta} but we set the fixed restart frequency $K = 49$. This parameter will ensure that $\{\beta_k\}$ satisfies $\{\beta_k\}\subseteq\left[0,\sqrt{\frac{L_g}{L_g + \ell_g}}\right)$ and $ \sup_{k}\beta_k<\sqrt{\frac{L_g}{L_g + \ell_g}}$.

We perform tests on random instances of \eqref{CompSen1-1}. As in the previous section, we generate an $A\in\R^{q\times n}$ with i.i.d. standard Gaussian entries, and then normalize this matrix so that each column of $A$ has unit norm. hen we choose a subset $T$ of size $k$ uniformly at random from $\{1, 2, \cdots, n\}$ and a $k$-sparse vector $x_{\rm orig}$ having i.i.d. standard Gaussian entries on $T$ is generated. We let $b = Ax_{\rm orig} + 0.01\cdot\bar{n}$ with $\bar{n}_i\sim{\rm Cauchy}(0, 1)$, specifically, we generate $\bar{n}_i$ as $\tan(\pi(\tilde{n}_i - \frac{1}{2}))$ with $\tilde{n}$ being a random vector with i.i.d. entries uniformly chosen in $[0, 1]$. We then set $\sigma = 1.1\cdot\|0.01\cdot\tilde{n}\|_{L L_2, \gamma}$ with $\gamma = 0.05$.


In our numerical tests, we let $\mu =0.95$ in \eqref{CompSen1-2} and $(p,n,k) = (720i,2560i,80i)$ with $i\in \{2, 4, 6, 8, 10\}$. For each $i$, we generate 20 random data as described above. The computational results averaged over the $20$ random instances are presented in Table~\ref{table2}. Specifically, we present the time for computing the QR decomposition of $A^T$ (denoted by $t_{\rm QR}$), the time for computing $\|A\|$ (denoted by $t_{\|A\|}$), the time for computing $x^0 = A^\dagger b$ given the QR factorization of $A^T$ (denoted by $t_{A^\dagger b}$), the CPU times of the algorithms (CPU time), the number of iterations (denoted by Iter), the recovery errors $\text{RecErr} := \frac{\|x^* - \xorig\|}{\max\{1, \|\xorig\|\}}$ and the residuals ${\rm Residual} := \frac{\|Ax^* - b\|_{LL_2,\gamma} - \sigma }{\sigma}$, where $x^*$ is the approximate solution returned by the respective algorithm.


\begin{table}[h]
{\color{black}
\caption{Computational results for problem \eqref{CompSen1-2}}\label{table2}
\begin{center}
{\footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|}\hline
\phantom{\diagbox{Date}{$i$}} & \multicolumn{1}{|c|}{Method} & \multicolumn{1}{|c|}{$i = 2$} & \multicolumn{1}{c|}{ $i = 4$ }
& \multicolumn{1}{c|}{ $i = 6$ } & \multicolumn{1}{|c|}{ $i = 8$ } & \multicolumn{1}{c|}{ $i = 10$ }
\\
\cline{1-7}\multirow{6}*{CPU time} & \multirow{1}*{$t_{\rm QR}$}
&  0.689 &  7.391 & 56.262 & 129.634 & 209.607      \\
\cline{2-2} \multirow{1}*{} & \multirow{1}*{$t_{A^\dagger b}$}
&  0.008 &  0.048 &  0.237 &  0.424 &  0.578     \\
\cline{2-2} \multirow{1}*{} & \multirow{1}*{$t_{\|A\|}$}
&  0.721 &  2.571 & 18.111 & 39.223 & 63.022     \\
\cline{2-7} \multirow{1}*{} & \multirow{1}*{SCP$_{\rm ls}$}
&  2.232 & 19.284 & 44.192 & 220.783 & 122.567       \\
\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM}
&  5.639 & 32.396 & 149.598 & 265.260 & 354.146        \\
\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm e}}$}
&  0.997 &  5.705 & 24.665 & 44.876 & 58.754     \\
\cline{1-7} \multirow{3}*{Iter} & \multirow{1}*{SCP$_{\rm ls}$}
&    207 &    332 &    196 &    596 &    266     \\
\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM}
&   1107 &   1122 &   1125 &   1151 &   1177     \\
\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm e}}$}
&    189 &    191 &    185 &    194 &    196     \\
\cline{1-7} \multirow{3}*{RecErr} & \multirow{1}*{SCP$_{\rm ls}$}
&  0.082 &  0.086 &  0.086 &  0.086 &  0.088  \\
\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM}
&  0.082 &  0.086 &  0.086 &  0.086 &  0.088  \\
\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm e}}$}
&  0.082 &  0.086 &  0.086 &  0.086 &  0.088  \\
\cline{1-7} \multirow{3}*{Residual} & \multirow{1}*{SCP$_{\rm ls}$}
& -5.53e-15 & -6.57e-15 & -6.76e-15 & -6.08e-15 & -7.00e-15       \\
\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM}
& -5.96e-14 & -2.21e-15 & -1.44e-14 & -4.53e-15 & -1.37e-15       \\
\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm e}}$}
& -2.73e-15 & 2.18e-15 & 1.35e-14 & 2.79e-15 & 1.31e-14   \\
\cline{1-7}
\end{tabular}
}
\end{center}
}
\end{table}

From Table~\ref{table2}, we observe a similar pattern as shown in Table~\ref{table1}, i.e., ESQM$_{{\rm e}}$ is the fastest algorithm and the recovery errors of all three algorithms are comparable.




\bmhead{Acknowledgments}
The second author is supported in part by the Hong Kong Research Grants Council PolyU153001/22p.





\begin{thebibliography}{99}
\bibitem{attouch10}
H. Attouch, J. Bolte, P. Redont and A. Soubeyran.
\newblock Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the Kurdyka-{\L}ojasiewicz inequality.
\newblock {\em Mathematics of Operations Research} 35, 438--457, 2010.


\bibitem{attouch13}
H. Attouch, J. Bolte and B. F. Svaiter.
\newblock Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward--backward splitting, and regularized Gauss--Seidel methods.
\newblock {\em Mathematical Programming} 137, 91--129, 2013.

\bibitem{Ausleder13}
A. Auslender.
\newblock An extended sequential quadratically constrained quadratic programming algorithm for nonlinear, semidefinite, and second-order cone programming.
\newblock{\em Journal of Optimization Theory and Applications} 156, 183--212, 2013.


%\bibitem{Bauschke96}
%H. H. Bauschke and J. M. Borwein.
%\newblock On projection algorithms for solving convex feasibility problems.
%\newblock {\em SIAM Review} 38(3), 367--426, 1996.

\bibitem{BauschkeBorweinLi99}
H. H. Bauschke, J. M. Borwein and W. Li.
\newblock Strong conical hull intersection property, bounded linear regularity, Jameson's property (G), and error bounds in convex optimization.
\newblock {\em Math. Program.} 86, 135--160, 1999.


\bibitem{beck09}
A. Beck and M. Teboulle.
\newblock Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems.
\newblock {\em IEEE Transactions on Image Processing} 18, 2419--2434, 2009.


\bibitem{bolte07}
J. Bolte, A. Daniilidis and A. Lewis.
\newblock The {\L}ojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems.
\newblock {\em SIAM Journal on Optimization} 17, 1205--1223, 2007.


%\bibitem{bolte16}
%J. Bolte and E. Pauwels.
%\newblock Majorization-minimization procedures and convergence of SQP methods for semi-algebraic and tame programs.
%\newblock {\em Mathematics of Operations Research} 41, 442--465, 2016.


\bibitem{bolte14}
J. Bolte, S. Sabach and M. Teboulle.
\newblock Proximal alternating linearized minimization for nonconvex and nonsmooth problems.
\newblock {\em Mathematical Programming} 146, 459--494, 2014.

%\bibitem{bolte17}
%J. Bolte, T.P. Nguyen, J. Peypouquet, B. W. Suter.
%\newblock From error bounds to the complexity of first-order descent methods for convex functions.
%\newblock {\em Mathematical Programming} 165, 471¨C507, 2017.

\bibitem{Ca18}
E. J. Cand\`{e}s.
\newblock The restricted isometry property and its implications for compressed sensing.
\newblock {\em  Comptes Rendus Mathematique} 346, 589--592, 2008.

\bibitem{CarrBarAys10}
R. E. Carrillo, K. E. Barner and T. C. Aysal.
\newblock Robust sampling and reconstruction methodsfor sparse signals in the presence of impulsive noise.
\newblock {\em IEEE Journal of Selected Topics in Signal Processing} 4, 392--408, 2010.

\bibitem{ChenLuPong16}
X. Chen, Z. Lu and T. K. Pong.
\newblock Penalty methods for a class of non-Lipschitz optimization problems.
\newblock {\em SIAM Journal on Optimization} 26, 1465--1492, 2016.

\bibitem{DonoghueCandes15}
B. O'Donoghue and E. J. Cand\`{e}s.
\newblock Adaptive restart for accelerated gradient schemes.
\newblock {\em Foundations of Computational Mathematics} 15, 715--732, 2015.

\bibitem{HoLS13}
J. D. W. Hosmer, S. Lemeshow and R. X. Sturdivant.
\newblock {\em  Applied Logistic Regression}.
\newblock John Wiley Sons, 3rd edition, 2013.

\bibitem{li18}
G. Li and T. K. Pong.
\newblock Calculus of the exponent of Kurdyka--{\L}ojasiewicz inequality and its applications to linear convergence of first-order methods.
\newblock {\em Foundations of Computational Mathematics} 18, 1199--1232, 2018.

%\bibitem{liu19}
%T. X. Liu, T. K. Pong, and A. Takeda.
%\newblock A refined convergence analysis of pDCA$_e$ with applications to simultaneous sparse recovery and outlier detection.
%\newblock {\em Computational Optimization and Applications} 73, 69--100, 2019.

%\bibitem{nesterov83}
%Y. Nesterov.
%\newblock A method for unconstrained convex minimization problem with the rate of convergence O ($\frac{1}{k^2}$).
%\newblock {\em Doklady AN USSR} 269, 543--547, 1983.
%
%\bibitem{Tseng08}
%P. Tseng.
%\newblock On accelerated proximal gradient methods for convex-concave optimization[J].
%\newblock {\em submitted to SIAM Journal on Optimization} 2, 2008.

%\bibitem{Tseng10}
%P. Tseng.
%\newblock Approximation accuracy, gradient methods, and error bound for structured convex optimization[J].
%\newblock {\em Mathematical Programming} 125, 263-295, 2010.

\bibitem{Ro70}
R. T. Rockafellar.
\newblock {\em Convex Analysis.}
\newblock Princeton University Press, Princeton, 1970.

\bibitem{rock97a}
R. T. Rockafellar and R. J-B. Wets.
\newblock {\em Variational Analysis.}
\newblock Springer, 1997.

\bibitem{Rob75}
S. M. Robinson.
\newblock An application of error bounds for convex programming in a linear space.
\newblock {\em SIAM Journal on Control} 13, 271--273, 1975.

%\bibitem{Tu98}
%H. Tuy.
%\newblock {\em Convex Analysis and Global Optimization}.
%\newblock Springer, 1998.

\bibitem{wen17}
B. Wen, X. Chen and T. K. Pong.
\newblock Linear convergence of proximal gradient algorithm with extrapolation for a class of nonconvex nonsmooth minimization problems.
\newblock {\em SIAM Journal on Optimization} 27, 124¨C145, 2017.

\bibitem{wen18}
B. Wen, X. Chen, and T. K. Pong.
\newblock A proximal difference-of-convex algorithm with extrapolation.
\newblock {\em Computational Optimization and Applications} 69, 297--324, 2018.


\bibitem{yu21}
P. Yu, T. K. Pong and Z. Lu.
\newblock Convergence rate analysis of a sequential convex programming method with line search for a class of constrained difference-of-convex optimization problems.
\newblock {\em SIAM Journal on Optimization} 31, 2024--2054, 2021.

\bibitem{zhang23}
Y. Zhang, G. Li, T. K. Pong and S. Xu.
\newblock Retraction-based first-order feasible methods for difference-of-convex programs with smooth inequality and simple geometric constraints.
\newblock {\em Advances in Computational Mathematics} 49, Article number: 8, 2023.

%\bibitem{zo04}
%G. Zou.
%\newblock A modified Poisson regression approach to prospective studies with binary data.
%\newblock {\em American Journal of Epidemiology} 159, 702--706, 2004.




\end{thebibliography}


\end{document}
